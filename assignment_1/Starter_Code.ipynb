{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e259e7b-d05b-41b8-b596-6ecdd4866c60",
   "metadata": {},
   "source": [
    "# DI 725: Transformers and Attention-Based Deep Networks\n",
    "\n",
    "## An Assignment for Implementing Transformers in PyTorch\n",
    "\n",
    "The purpose of this notebook is to guide you through the usage of sample code.\n",
    "\n",
    "This notebook follows the baseline prepared by Andrej Karpathy, with a custom dataset (Don-Quixote by Cervantes). This version of the code, called [nanoGPT](https://github.com/karpathy/nanoGPT), is a revisit to his famous [minGPT](https://github.com/karpathy/minGPT).\n",
    "### Author:\n",
    "* Ümit Mert Çağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715be989-8426-4406-bd8f-2bcf0e003f09",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Install requirements for your environment, comment out for later uses.\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "- [pytorch](https://pytorch.org)\n",
    "- [numpy](https://numpy.org/install/)\n",
    "-  `transformers` for huggingface transformers (to load GPT-2 checkpoints)\n",
    "-  `datasets` for huggingface datasets (to download + preprocess datasets)\n",
    "-  `tiktoken` for OpenAI's fast BPE code\n",
    "-  `wandb` for optional logging\n",
    "-  `tqdm` for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69136d40-c5ac-4623-899c-3b5ad21f368c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torch numpy transformers datasets tiktoken wandb tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e72d12-9aa6-456f-ae34-2c52aaeee7c3",
   "metadata": {},
   "source": [
    "The fastest way to get started to transformers, apart from following the labs of DI725, is to use a small model and dataset. For this purpose, we will start with training a character-level GPT on the Don-Quixote by Cervantes. The code will download a single file (2MB) and apply some transformations. Examine the code [prepare.py](data/don_char/prepare.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2cade-4742-4b44-bcb2-0ae72c9571ad",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869a547-0ebb-4be6-9b25-f158db64407e",
   "metadata": {},
   "source": [
    "Use the following to prepare the don-quixote novel treated in character level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a08a93-5556-4cd9-ad2d-cc58d0363d52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 2,361,864\n",
      "all the unique characters: \n",
      " !#$%&()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÁÆÑÚàáæèéëíñóùŒœ—‘’“”•™\n",
      "vocab size: 105\n",
      "train has 2,125,677 tokens\n",
      "val has 236,187 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/don_char/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a34276-d844-435d-9e16-12f567969d5f",
   "metadata": {},
   "source": [
    "This creates a `train.bin` and `val.bin` in that data directory. Now it is time to train our own GPT. The size of the GPT model depends on the computational resources. It is advised to have a GPU for heavy works, and to train lightweight and evaluate and infer models with a CPU.\n",
    "\n",
    "Small scale GPT with the settings provided in the [config/train_don_char.py](config/train_don_char.py) config file will be trained with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad3097d-65d8-4a72-a8d0-b5fa463b49c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_don_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-don-char'\n",
      "eval_interval = 250  # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 50  # don't print too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False  # override via command line if you like\n",
      "wandb_project = 'don-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'don_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256  # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3  # with baby networks can afford to go a bit higher\n",
      "max_iters = 2000\n",
      "lr_decay_iters = 2000  # make equal to max_iters usually\n",
      "min_lr = 1e-4  # learning_rate / 10 usually\n",
      "beta2 = 0.99  # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100  # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: compile = False\n",
      "tokens per iteration will be: 16,384\n",
      "found vocab_size = 105 (inside data\\don_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 10.66M\n",
      "num decayed parameter tensors: 26, with 10,755,456 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.6920, val loss 4.6924\n",
      "iter 0: loss 4.6963, time 10923.80ms, mfu -100.00%\n",
      "iter 50: loss 2.4422, time 60.95ms, mfu 6.12%\n",
      "iter 100: loss 2.3947, time 63.86ms, mfu 6.09%\n",
      "iter 150: loss 2.3350, time 69.16ms, mfu 6.02%\n",
      "iter 200: loss 2.1251, time 61.64ms, mfu 6.03%\n",
      "step 250: train loss 1.9058, val loss 1.9617\n",
      "saving checkpoint to out-don-char\n",
      "iter 250: loss 1.9602, time 10453.57ms, mfu 5.43%\n",
      "iter 300: loss 1.8434, time 65.70ms, mfu 5.45%\n",
      "iter 350: loss 1.7410, time 62.04ms, mfu 5.51%\n",
      "iter 400: loss 1.6856, time 70.19ms, mfu 5.49%\n",
      "iter 450: loss 1.6375, time 59.15ms, mfu 5.57%\n",
      "step 500: train loss 1.4970, val loss 1.5770\n",
      "saving checkpoint to out-don-char\n",
      "iter 500: loss 1.5908, time 10357.50ms, mfu 5.02%\n",
      "iter 550: loss 1.5198, time 65.44ms, mfu 5.09%\n",
      "iter 600: loss 1.5139, time 70.16ms, mfu 5.11%\n",
      "iter 650: loss 1.4587, time 80.39ms, mfu 5.06%\n",
      "iter 700: loss 1.4644, time 72.28ms, mfu 5.07%\n",
      "step 750: train loss 1.3171, val loss 1.4024\n",
      "saving checkpoint to out-don-char\n",
      "iter 750: loss 1.3810, time 10441.97ms, mfu 4.57%\n",
      "iter 800: loss 1.4000, time 75.23ms, mfu 4.61%\n",
      "iter 850: loss 1.3412, time 63.02ms, mfu 4.74%\n",
      "iter 900: loss 1.3258, time 62.28ms, mfu 4.86%\n",
      "iter 950: loss 1.3546, time 76.09ms, mfu 4.87%\n",
      "step 1000: train loss 1.2262, val loss 1.3390\n",
      "saving checkpoint to out-don-char\n",
      "iter 1000: loss 1.2617, time 10457.35ms, mfu 4.39%\n",
      "iter 1050: loss 1.3245, time 79.33ms, mfu 4.42%\n",
      "iter 1100: loss 1.2818, time 80.74ms, mfu 4.44%\n",
      "iter 1150: loss 1.2846, time 70.64ms, mfu 4.52%\n",
      "iter 1200: loss 1.2138, time 62.01ms, mfu 4.67%\n",
      "step 1250: train loss 1.1593, val loss 1.2890\n",
      "saving checkpoint to out-don-char\n",
      "iter 1250: loss 1.2531, time 10386.29ms, mfu 4.21%\n",
      "iter 1300: loss 1.1990, time 63.55ms, mfu 4.37%\n",
      "iter 1350: loss 1.1746, time 66.81ms, mfu 4.50%\n",
      "iter 1400: loss 1.2057, time 66.91ms, mfu 4.60%\n",
      "iter 1450: loss 1.1835, time 72.22ms, mfu 4.66%\n",
      "step 1500: train loss 1.1202, val loss 1.2545\n",
      "saving checkpoint to out-don-char\n",
      "iter 1500: loss 1.1743, time 10425.63ms, mfu 4.20%\n",
      "iter 1550: loss 1.1797, time 81.79ms, mfu 4.23%\n",
      "iter 1600: loss 1.1670, time 73.45ms, mfu 4.32%\n",
      "iter 1650: loss 1.1934, time 59.68ms, mfu 4.51%\n",
      "iter 1700: loss 1.1405, time 71.00ms, mfu 4.59%\n",
      "step 1750: train loss 1.0911, val loss 1.2327\n",
      "saving checkpoint to out-don-char\n",
      "iter 1750: loss 1.1435, time 10473.20ms, mfu 4.13%\n",
      "iter 1800: loss 1.1659, time 79.80ms, mfu 4.19%\n",
      "iter 1850: loss 1.1402, time 76.42ms, mfu 4.26%\n",
      "iter 1900: loss 1.1556, time 83.38ms, mfu 4.28%\n",
      "iter 1950: loss 1.1348, time 65.75ms, mfu 4.42%\n",
      "step 2000: train loss 1.0714, val loss 1.2196\n",
      "saving checkpoint to out-don-char\n",
      "iter 2000: loss 1.1043, time 10471.80ms, mfu 3.98%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_don_char.py --compile=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06da9f-a5a1-4621-806c-adad9b2d98d5",
   "metadata": {},
   "source": [
    "We are training a small scaled GPT with a context size of up to 256 characters, 384 feature channels, 6 layers of transformer with 6 attention heads. On one GTX 3070 GPU this training run takes about 10 minutes and the best validation loss is 1.1620. Based on the configuration, the model checkpoints are being written into the `--out_dir` directory `out-don-char`. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3380c5-976e-4dbb-b5dc-08ba56f0d93c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-don-char\n",
      "number of parameters: 10.66M\n",
      "Loading meta from data\\don_char\\meta.pkl...\n",
      "\n",
      "\n",
      "“Right you?” said Sancho, “but I know this minute and learned me; for I\n",
      "\n",
      "you have got prayed, I would thank them to you are a\n",
      "\n",
      "government.”\n",
      "\n",
      "\n",
      "\n",
      "“I will give you that I shall come to you,” said the canon, “that with that the\n",
      "\n",
      "devil, and the heart of this kingdom of Gaiferos were public, and that cross\n",
      "\n",
      "he had that it was to let us in a penance and under again\n",
      "\n",
      "a room of desturbance.”\n",
      "\n",
      "\n",
      "\n",
      "“I can thee say that of all that, señor,” replied Don Quixote, “for thou\n",
      "\n",
      "wouldst anything into them who is a h\n",
      "---------------\n",
      "\n",
      "“A mood of Dulcinea,” replied Don Quixote, “that I know what I\n",
      "\n",
      "shall see thee but the wheels of the whole hundred of this\n",
      "\n",
      "legs or customary, and it is so fell and true, when they were all\n",
      "\n",
      "the famous kingdom they are not only despatched them. To begin\n",
      "\n",
      "this master called Teresa Panza de Valencia and Don Quixote. But he might not\n",
      "\n",
      "remember all the cousin said:\n",
      "\n",
      "\n",
      "\n",
      "“For it is the reason of the duke goodness, my respection this, sirs, is the body of\n",
      "\n",
      "our services with me; I know I can believe to t\n",
      "---------------\n",
      "\n",
      "\n",
      "For the tawn, who was also made all that when the barber\n",
      "\n",
      "Maritornes had so done to the appearance of Don Quixote; and when Sancho sweared\n",
      "\n",
      "his brother desire to the bachelor, and then the gentleman he met with\n",
      "\n",
      "his arms gave somewhat notary a pig man, the thicket of the devil? In the\n",
      "\n",
      "government and finished story, and dismay be without the marster-reason of\n",
      "\n",
      "the door, the threader of knights-errantry or his wife; and this was\n",
      "\n",
      "the same dictive of our desires and invitation, or all the same on\n",
      "---------------\n",
      "\n",
      "\n",
      "three threads of his horse; but to make the farther well astraits of\n",
      "\n",
      "the world, for so be rateful to have made him any of the sight which\n",
      "\n",
      "happened for a shepherd of which made an harm of the world and\n",
      "\n",
      "lose by the table joke, he has seen a bit of this way that is there is\n",
      "\n",
      "a brother than what he may not get us sure. Nor who should everything,\n",
      "\n",
      "but to the consideration, I know that these daints which will be so not to\n",
      "\n",
      "see now the world will be able battle; and I wish that so the patience\n",
      "\n",
      "of \n",
      "---------------\n",
      "\n",
      "\n",
      "the history lay self-well, his master, and moved himself to be charged\n",
      "\n",
      "for the time.\n",
      "\n",
      "\n",
      "\n",
      "In that he had said to himself a long remark in such a pitch as a point\n",
      "\n",
      "of restore and press that it was the spot of a far four and smile and\n",
      "\n",
      "gentleman he had not been borne; and he was about his own own name to\n",
      "\n",
      "his affair and a morning, and moved what he said, “There is not\n",
      "\n",
      "remedience more translation, and captive of the world and carry the\n",
      "\n",
      "soft, and call us, and taste by this island to where I percei\n",
      "---------------\n",
      "\n",
      "\n",
      "so unexpected just now content to the most princess that she does not be\n",
      "\n",
      "well in the sole of Antonomanca.”\n",
      "\n",
      "\n",
      "\n",
      "“That could let him seat that this time,” said the curate, “I should not know\n",
      "\n",
      "Señor, but I have no doubt to get a tenderful and action of them about with\n",
      "\n",
      "any rate; but take our person for the first of my master have gone as well\n",
      "\n",
      "as there was not a Christian or Chapter, and leaving my nieces that it may be\n",
      "\n",
      "been so grant and they changed, too, as they say, may say it was that I\n",
      "\n",
      "mean\n",
      "---------------\n",
      "\n",
      "between them down Rocinante, and the best that were began to\n",
      "\n",
      "be so grandfather that, which lay or, himself not full of all to\n",
      "\n",
      "the sun he had gone with all charge and adorned at the antimal rest\n",
      "\n",
      "extrange of the temperor. He did not entertained the ground said, “I wish to\n",
      "\n",
      "see what I do not go abinding them alone to take up the same days of\n",
      "\n",
      "the errors and governors than the grieves of a dress.”\n",
      "\n",
      "\n",
      "\n",
      "“And what was the ladies of the Moor,” said Don Quixote; “besgot thee;\n",
      "\n",
      "that if they had not igno\n",
      "---------------\n",
      "\n",
      "that who had the duke and man was a mill and Spanish and truth\n",
      "\n",
      "people’s wounds and come to him to his unhappy excellence continued to\n",
      "\n",
      "persuade him he was dead or right, arranged the limitation of his house,\n",
      "\n",
      "the approach water said, “What do you want to see if it be persuaded me to\n",
      "\n",
      "go on the strange in _purchasio doubt, the world have done been some\n",
      "\n",
      "crutchfully; I have one found with the presence of heaven that won’t the\n",
      "\n",
      "other part, and let us give half any strong and little perform to hear\n",
      "---------------\n",
      "\n",
      "\n",
      "his squire, and took their eyes to restrain the greatest water\n",
      "\n",
      "with thee forward, and chance with their own bulls, to them, they\n",
      "\n",
      "begged to say to him, “No, master, if thou art satisfied which\n",
      "\n",
      "is an unfortunate ready in thy life that scartles on my author shall be\n",
      "\n",
      "encountered to see; and the case that is not all the truth; and if thou wilt\n",
      "\n",
      "be that heaven thou shalt give no one to the conditions and pitch of your save\n",
      "\n",
      "weary and other misfortunes.”\n",
      "\n",
      "\n",
      "\n",
      "“I shall lay that his duty,” said Sancho\n",
      "---------------\n",
      "\n",
      "\n",
      "complished by that of the four world, of the folly the\n",
      "\n",
      "   Hope showed them so great green eagle as to the first complish the\n",
      "\n",
      "    thing.\n",
      "\n",
      "\n",
      "\n",
      "While this t hath gratitude that thou wast to see, and then the\n",
      "\n",
      "    young disposition of this wonderful are so long over who can\n",
      "\n",
      "    examined when thou art enough to something thine.\n",
      "\n",
      "\n",
      "\n",
      "On these words of thy verses, or the fact, to present,\n",
      "\n",
      "    This is some very wise by heaven by our life.\n",
      "\n",
      "\n",
      "\n",
      "O more than we meant to be,\n",
      "\n",
      "I promised at the distress,\n",
      "\n",
      "The\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-don-char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43535f2-8e3f-4823-955d-a30c9ed9e0eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "This generates a few samples, for example:\n",
    "\n",
    "```\n",
    "“I grant all that,” said the governor; “it’s not in a low voice\n",
    "\n",
    "but not yet forget that there’s none of it the poor in the world; I’ll\n",
    "\n",
    "like to take special to have been no one to write out the stone of\n",
    "\n",
    "patience to the village.”\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61966bc-558b-4964-8575-1046d0aa6a91",
   "metadata": {},
   "source": [
    "It is pretty nice to have a GPT in a few minutes of character level training! Better results can be achieved possibly by hyperparameter tuning and finetuning (transfer learning) from a pre-trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b323fd-20ba-4e6b-8a5a-e40b9fdd0bca",
   "metadata": {},
   "source": [
    "## Quick start with less resources\n",
    "\n",
    "If we are [low on resources](https://www.youtube.com/watch?v=rcXzn6xXdIc), we can use a simpler version of the training, first we need to set compile to false, this is also a must for Windows OS for now. We also set the device to CPU. The model that is trained in 10 minutes for a starter grade GPU, will be trained in a much longer time, so we can also decrease the dimensions of our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef08093-984e-4fae-a1f9-fdf6bf4b1bd5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_don_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-don-char'\n",
      "eval_interval = 250  # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 50  # don't print too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False  # override via command line if you like\n",
      "wandb_project = 'don-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'don_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256  # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3  # with baby networks can afford to go a bit higher\n",
      "max_iters = 2000\n",
      "lr_decay_iters = 2000  # make equal to max_iters usually\n",
      "min_lr = 1e-4  # learning_rate / 10 usually\n",
      "beta2 = 0.99  # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100  # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cpu\n",
      "Overriding: out_dir = out-don-small-char\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 20\n",
      "Overriding: log_interval = 50\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 4\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 1000\n",
      "Overriding: lr_decay_iters = 1000\n",
      "Overriding: dropout = 0.0\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 105 (inside data\\don_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 0.80M\n",
      "num decayed parameter tensors: 18, with 808,064 parameters\n",
      "num non-decayed parameter tensors: 9, with 1,152 parameters\n",
      "using fused AdamW: False\n",
      "step 0: train loss 4.6589, val loss 4.6589\n",
      "iter 0: loss 4.6709, time 320.27ms, mfu -100.00%\n",
      "iter 50: loss 2.8993, time 26.53ms, mfu 0.05%\n",
      "iter 100: loss 2.5246, time 25.61ms, mfu 0.05%\n",
      "iter 150: loss 2.5116, time 25.02ms, mfu 0.05%\n",
      "iter 200: loss 2.4661, time 25.50ms, mfu 0.05%\n",
      "step 250: train loss 2.3680, val loss 2.4258\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 250: loss 2.4379, time 591.16ms, mfu 0.04%\n",
      "iter 300: loss 2.3602, time 47.04ms, mfu 0.04%\n",
      "iter 350: loss 2.4773, time 45.94ms, mfu 0.04%\n",
      "iter 400: loss 2.2216, time 46.37ms, mfu 0.04%\n",
      "iter 450: loss 2.1838, time 46.68ms, mfu 0.04%\n",
      "step 500: train loss 2.1897, val loss 2.2173\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 500: loss 2.1793, time 588.53ms, mfu 0.03%\n",
      "iter 550: loss 2.1115, time 46.07ms, mfu 0.03%\n",
      "iter 600: loss 2.0256, time 45.55ms, mfu 0.03%\n",
      "iter 650: loss 2.1292, time 45.36ms, mfu 0.03%\n",
      "iter 700: loss 2.0509, time 46.62ms, mfu 0.03%\n",
      "step 750: train loss 2.0156, val loss 2.0554\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 750: loss 2.0472, time 593.98ms, mfu 0.03%\n",
      "iter 800: loss 1.9873, time 46.53ms, mfu 0.03%\n",
      "iter 850: loss 1.9387, time 46.64ms, mfu 0.03%\n",
      "iter 900: loss 1.9944, time 48.17ms, mfu 0.03%\n",
      "iter 950: loss 2.0044, time 46.49ms, mfu 0.03%\n",
      "step 1000: train loss 1.9400, val loss 2.0243\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 1000: loss 1.8996, time 589.85ms, mfu 0.03%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_don_char.py --device=cpu --out_dir=\"out-don-small-char\" --compile=False --eval_iters=20 --log_interval=50 --block_size=64 --batch_size=12 --n_layer=4 --n_head=4 --n_embd=128 --max_iters=1000 --lr_decay_iters=1000 --dropout=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d4052-9674-49d9-91a4-c79c887e4c93",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Here, since we are running on CPU instead of GPU we must set both `--device=cpu` and also turn off PyTorch 2.0 compile with `--compile=False`. Then when we evaluate we get a bit more noisy but faster estimate (`--eval_iters=20`, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with `--lr_decay_iters`). Because our network is so small we also ease down on regularization (`--dropout=0.0`). This still runs in about ~5 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b15c137-c810-495b-8962-573f7c4a7d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-don-small-char\n",
      "Overriding: device = cpu\n",
      "number of parameters: 0.80M\n",
      "Loading meta from data\\don_char\\meta.pkl...\n",
      "\n",
      "STHEGI NLIF F CHE WHAM HE GINARE WE N SA UHABE TE T“I? TAD\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  SANSAN IANo This this and tDule he rist hand the wam the to he\n",
      "\n",
      "by cleand the suel a his it his feloss of on the bappose the the I alf; Ma\n",
      "\n",
      "Buld anctonty whouf the whes had had scrraining werroud that and\n",
      "\n",
      "pance frour. Band that and and werthou his and and the sis the werve wough\n",
      "\n",
      "was thim fall the werck belonen her gettre the groom, I, my bererem and\n",
      "\n",
      "thou, so sode his dople, and notanXe, “Tor him destout on whir \n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "This letlare, this may the ife the agll waing herer, “I che the wernt I as\n",
      "\n",
      "ourse wull the deey the amed to the bat come,”\n",
      "\n",
      "\n",
      "“Ong in I that in of thim rould the, and now we the dough dow mastine of and thaucke ofor\n",
      "\n",
      "the move has the was wrens heioved of his meer that uure of the lis of in\n",
      "\n",
      "I de wasty mand by hourd miood hou that beeand the ceerrt of har what and to the supiet;\n",
      "\n",
      "my, belles and the me him pomen the chind of the a he wrooked she\n",
      "\n",
      "as hir, Save for divecers and that uplandarton the\n",
      "---------------\n",
      "\n",
      "The for all his sling we so liashed, for\n",
      "\n",
      "for wa the lave take the of the was the ham hand fretul he the is he\n",
      "\n",
      "castindpile as to the nof har dot of I dis leesf or with finew reard of sellest\n",
      "\n",
      "oorsens the fappeatsis thim tor Sarve dere by this O ve ming and trich\n",
      "\n",
      "mbing repornor amplitard of esseerspooncaals, it the my comertious with\n",
      "\n",
      "was tray this doown hey his neet onentith thelo, somp ind that sore\n",
      "\n",
      "Sancho the his bladingles tay, the be theresest, and have in it the\n",
      "\n",
      "he wild,”\n",
      "\n",
      "\n",
      "\n",
      "“Donco! HiT\n",
      "---------------\n",
      "\n",
      "\n",
      "fashe fall he that ppumatobe in to the mack this word it imenected it the be as cont to to\n",
      "\n",
      "deelus right to susters age, achout to not tay as thachity that dis\n",
      "\n",
      "goremes or fore buste,”s and be and and, “I sust the mais this to\n",
      "\n",
      "gretoulself the were thous thans he mod the ide want as benie futio he gik ingh the at\n",
      "\n",
      "down it thersed the the cpras ampond gizess aintald to renther onound and thest mand pringot the tor he wish he\n",
      "\n",
      "pecare whous the ron hicht in this himelf oor paseer it to on he be\n",
      "\n",
      "a\n",
      "---------------\n",
      "\n",
      "\n",
      "The cour to histher by many mating he bereress to sing to my the\n",
      "\n",
      "bingstys brarcion on iste hir said my in astely, the thaut say the\n",
      "\n",
      "pore the torstuned of it misquries gast dour of in it amesty thim-is praigh\n",
      "\n",
      "Sanche to who to the saptior on or herme of bether with to is on the\n",
      "\n",
      "if tho, reengubonat coold on thy ive ould ham be of the the is\n",
      "\n",
      "them, wand he the cact knig, thim and salle, I mentented or sright the by\n",
      "\n",
      "a and bout, the havam of goost it or the bunooned of thee peiund,\n",
      "\n",
      "ans muts mor\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " mever, with pret sould, and a my be your the houghcle the pablid of\n",
      "\n",
      "chentant anls herand the uresten, and on the dourn of it thir sobliang\n",
      "\n",
      "t shaind the, a\n",
      "\n",
      "Ther knought quichtas no the ime cand to our the wandyoum to mysto\n",
      "\n",
      "gorsond that trereess, I cay brow ithe of the tread sajuryin of on\n",
      "\n",
      "well thy that he man and leeand of hour knim chad thy bery thour pa foll the haptel poup s the lead our the\n",
      "\n",
      "eyeat cade thantione beancornced refurte? Ast thers hemart, wand she\n",
      "\n",
      "treming the ut the di\n",
      "---------------\n",
      "\n",
      "\n",
      "The herstsed them have who kee out net the velpplation frore will in the e sore\n",
      "\n",
      "sordmener of hart thing on hure for ee, or the ve haw, band the a of\n",
      "\n",
      "the xoe siges of or pon the deadll thibrs out on ber in theey hourd\n",
      "\n",
      "semed thim to to we deesattoy and the was therey have peater, I mand itzeant\n",
      "\n",
      "itour hand welll with feer the sto hee the ecad urublens of with the\n",
      "\n",
      "whout ellfoserent that as is saut bainced of the and, see for and\n",
      "\n",
      "Gown he sthough be on and hes is o wall and the in’s Et said the\n",
      "---------------\n",
      "\n",
      "dell it to of the by to the decarst ound sugre cres ucht bais of\n",
      "\n",
      "scaisted the wath wa yous she to noot goobmised asand of whe is he\n",
      "\n",
      "as to for hem soones rayt the Samancil of or this hadd pristes,\n",
      "\n",
      "illtised and on affriall the wat he knonig in thimsed to sabling the\n",
      "\n",
      "may mony entsentrover! “, preghed bet that bupeail; the he easat\n",
      "\n",
      "to the mrache fathe, thout deastis fimthis it everoud the by allth nought this\n",
      "\n",
      "guillls it this ggenters\n",
      "\n",
      "me sto the all croow burthe youns thou to mand saploing, an\n",
      "---------------\n",
      "\n",
      "OD Sa HPOf Dronconon, I Lancen in I all selis, and for nacoull than of gor beroten it geard my wee\n",
      "\n",
      "ifrgouseses pomiting I as band and that the as and fengerns, ancill\n",
      "\n",
      "thhe arnt the maclang kere is a\n",
      "\n",
      "a such this dest sat trisired as your the of sto ben listuy of thim\n",
      "\n",
      "he the fand he plame\n",
      "\n",
      "sad, with and thir his to comone he gen bince. I Pa burich in the\n",
      "\n",
      "hat siss tho gis hight had bereste of the he the the lead thre myou\n",
      "\n",
      "at plable, this the rastemping and of ure und a that rame the neee\n",
      "\n",
      "war\n",
      "---------------\n",
      "\n",
      "wit his wart the the him waul he sow chand who dad the to ou this.\n",
      "\n",
      "\n",
      "\n",
      "That wel migh the losser thou tand but, I him momonnased hill sto he\n",
      "\n",
      "haplod wor win”\n",
      "\n",
      "\n",
      "“That sin utt and biswatencurs than it is as asts he has and as bre not ther bay of the\n",
      "\n",
      "peaster it that mere in it wath that and the to end wall she in wir this noubet it reas fert\n",
      "\n",
      "will wace this seprith the ramprned the rustat no the belon theim, bourd the what he wno\n",
      "\n",
      "burtou he he alve has sefer berenenss the that his of you his on than\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-don-small-char --device=cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75225da9-4908-4f9a-a1f6-f49ebdeb888c",
   "metadata": {},
   "source": [
    "Generates samples like this:\n",
    "\n",
    "```\n",
    "Sancho nother with this then of everantan has for five he enver any\n",
    "\n",
    "shal were than as in though they and I knight the sther his a jlage,\n",
    "\n",
    "and mad priled and squiel a hist to in feet she took and and sersse to her of\n",
    "\n",
    "Marest and good was pefor rubt some by than lave from his dintat all\n",
    "\n",
    "pack that he remants to goost ever to him arestiance of it the were to who\n",
    "\n",
    "which mom, worly gane for he sporen gort he was roosion, and be that\n",
    "\n",
    "it thou, so so he kniders what the and him of him dest us on shart\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89c51c-d9ee-4206-879e-c39f05aed2cc",
   "metadata": {},
   "source": [
    "*Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (`--block_size`), the length of training, etc.*\n",
    "\n",
    "*Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add `--device=mps` (short for \"Metal Performance Shaders\"); PyTorch then uses the on-chip GPU that can *significantly* accelerate training (2-3X) and allow you to use larger networks. See [Issue 28](https://github.com/karpathy/nanoGPT/issues/28) for more.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6eaff-1ca1-4791-a361-b9cef9b822a0",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Finetuning or transfer learning is a precious method of achieving better models thanks to pre-trained models. Finetuning GPT models is just as simple as training from scratch! We will now download the Don-Quixote (again) but this time we will define it with tokens (using OpenAI's BPE tokenizer) instead of characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84c540c-f048-446b-8cc8-214fffdb102d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train has 592,353 tokens\n",
      "val has 66,303 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/don/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b052b-5c7d-4741-b3d5-217966d5ddf6",
   "metadata": {},
   "source": [
    "Run an example finetuning like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d0ba8e9-3977-43fd-b98d-a37b663d6010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/finetune_don.py:\n",
      "import time\n",
      "\n",
      "out_dir = 'out-don'\n",
      "eval_interval = 5\n",
      "eval_iters = 40\n",
      "wandb_log = False # feel free to turn on\n",
      "wandb_project = 'don'\n",
      "wandb_run_name = 'ft-' + str(time.time())\n",
      "\n",
      "dataset = 'don'\n",
      "init_from = 'gpt2' # this is the GPT-2 model\n",
      "\n",
      "# only save checkpoints if the validation loss improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "# the number of examples per iter:\n",
      "batch_size = 1\n",
      "gradient_accumulation_steps = 32\n",
      "max_iters = 20\n",
      "\n",
      "# finetune at constant LR\n",
      "learning_rate = 3e-5\n",
      "decay_lr = False\n",
      "\n",
      "Overriding: compile = False\n",
      "tokens per iteration will be: 32,768\n",
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "number of parameters: 123.65M\n",
      "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 3.3541, val loss 3.3616\n",
      "iter 0: loss 3.3537, time 4283.39ms, mfu -100.00%\n",
      "iter 1: loss 3.3553, time 1665.65ms, mfu -100.00%\n",
      "iter 2: loss 3.0166, time 1627.01ms, mfu -100.00%\n",
      "iter 3: loss 3.0804, time 1658.45ms, mfu -100.00%\n",
      "iter 4: loss 3.2411, time 1671.46ms, mfu -100.00%\n",
      "step 5: train loss 3.1896, val loss 3.0898\n",
      "saving checkpoint to out-don\n",
      "iter 5: loss 2.9270, time 6360.53ms, mfu 1.41%\n",
      "iter 6: loss 3.0989, time 1661.93ms, mfu 1.81%\n",
      "iter 7: loss 3.3934, time 1657.17ms, mfu 2.17%\n",
      "iter 8: loss 2.8056, time 1669.14ms, mfu 2.49%\n",
      "iter 9: loss 3.1196, time 1648.85ms, mfu 2.79%\n",
      "step 10: train loss 3.1041, val loss 3.0200\n",
      "saving checkpoint to out-don\n",
      "iter 10: loss 3.1351, time 5963.51ms, mfu 2.66%\n",
      "iter 11: loss 3.1568, time 1658.26ms, mfu 2.94%\n",
      "iter 12: loss 3.0418, time 1654.39ms, mfu 3.19%\n",
      "iter 13: loss 3.3787, time 1660.57ms, mfu 3.41%\n",
      "iter 14: loss 3.0971, time 1681.47ms, mfu 3.60%\n",
      "step 15: train loss 3.0677, val loss 3.0510\n",
      "iter 15: loss 2.6755, time 3055.28ms, mfu 3.53%\n",
      "iter 16: loss 3.4083, time 1672.20ms, mfu 3.72%\n",
      "iter 17: loss 2.9909, time 1659.64ms, mfu 3.89%\n",
      "iter 18: loss 2.9087, time 1662.82ms, mfu 4.04%\n",
      "iter 19: loss 3.1812, time 1659.55ms, mfu 4.18%\n",
      "step 20: train loss 3.0000, val loss 3.0810\n",
      "iter 20: loss 3.1975, time 3088.11ms, mfu 4.05%\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/finetune_don.py --compile=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040fa6b-64a7-4964-afd5-3c5c545aeaba",
   "metadata": {},
   "source": [
    "This will load the config parameter overrides in `config/finetune_don.py`. Basically, we initialize from a GPT2 checkpoint with `init_from` and train as normal, except shorter and with a small learning rate. Model architecture is changable to `{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}`) and can be decreased in size by the `block_size` (context length). The best checkpoint (lowest validation loss) will be in the `out_dir` directory, e.g. in `out-don` by default, per the config file. You can then run the code in `sample.py --out_dir=out-don`:\n",
    "```\n",
    "* * All creatures that enter the world below may so far as want to observe the rules of their own land, and may obey them under the hand of their lord, and may not follow others below.\n",
    "\n",
    "* * *\n",
    "\n",
    "THE PORT COLLIDATES,\n",
    "\n",
    "- * *\n",
    "\n",
    "ON the light, and the light to the dark, and the darkness to the light, and the darkness to the darkness, were the present-day laws of monarchy, whose lordship they approved in their faces and hearts. From this moment on, however, they had no other representation to give than that of their master, who, for all that was said or heard, had reached the height of his power.\n",
    "\n",
    "The king's hand, though at times little more than a finger of his, required no more than a finger of his, and that power was, that of holding his eye, and the other of his, in his own, body.\n",
    "\n",
    "When this was spoken of, it was a simple and noble quibble, and the subject of this was so as to admit of the few who had any forsemination, and the few who had the most to go on.\n",
    "\n",
    "The time did not come for a thought of this, and for a moment the very thought of it seemed to fall to the ground.\n",
    "\n",
    "But that thought did not come to pass; though the king was not speaking of the king, it came to pass that the king, with all his might, and all his cunning, and no other sense, and without any understanding, and without any desire for the utmost of his services, and without any desire to put an end to his own glory, and without any desire to hide his triumph, had found the time to say that this was what he thought on the subject of religion; that it was what he thought, and according as it seemed to him to be as good or better to him than to the other kings, and he was in no sense a king, for it seemed to him he could never have any more power than he had to be; that it was a matter of his will and power; and that it was all a matter of his will, for he was determined that this look and that to which he might have been given to hold it was the best in himself.\n",
    "\n",
    "And so it was that the king, who was all around him, and all around him; and so\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037197e-f7e3-40bc-80fd-e2626cf4dc4a",
   "metadata": {},
   "source": [
    "# Inference and Sampling\n",
    "Use the script `sample.py` to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available `gpt2-xl` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83d42305-ff4f-4153-be5c-3067e22ccf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-don\n",
      "Overriding: start = Explain the relationship between Don Quixote and Sancho Panza\n",
      "Overriding: num_samples = 5\n",
      "Overriding: max_new_tokens = 100\n",
      "number of parameters: 123.65M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "Explain the relationship between Don Quixote and Sancho Panza. The story, according to the newspapers, is that the two men want to take over the city, and that Sancho has accepted that.\n",
      "\n",
      "I have been told that Don Quixote was not the first man to arrive to the city, and that Sancho was not the first. I have no reason to doubt that, for Don Quixote is an itinerant man, who is always crossing the country, and with a lot of money. I have been told that many other\n",
      "---------------\n",
      "Explain the relationship between Don Quixote and Sancho Panza, the magician who is almost always mistaken for the Countess of Sancho Panza. This is a trick used by the Countess of Sancho Panza.\n",
      "\n",
      "The Countess of Sancho Panza is made the subject of many rumors, but the truth is that since she is the same as the Countess of Sancho Panza, Sancho Panza's presence on earth is not disguised. No, she is not disguised, for she is the person who will become the subject\n",
      "---------------\n",
      "Explain the relationship between Don Quixote and Sancho Panza\n",
      "\n",
      "\n",
      "Don's obsession with his own career was not just a matter of money but of ambition. He was the son of the Sancho Panza family, the family which had been founded in a humble village in the San Diego mountains during the time that it was the 'Calavera of the San Diego Basin' and had lived there for nearly half a century.\n",
      "\n",
      "\n",
      "But it was the family that had gone missing on the night of May 9, 1833, leaving Don Quix\n",
      "---------------\n",
      "Explain the relationship between Don Quixote and Sancho Panza! The first time I saw Sancho, and I was the first to introduce him to some of his most notable friends, I had a good time, full of interest and interest. He is a very successful editor, owing to his good taste, with a good wit and fair, lively story. I came to know him over several years ago on the island of Puerto Rico, where I was to visit, and, as I am to say, we have enough copies to give to you and your\n",
      "---------------\n",
      "Explain the relationship between Don Quixote and Sancho Panza. The latter's speech has in it a request for his aid against the misfortunes of the Negros, who are seeking to be buried, and in a later line has the subject of the misfortunes of the Santa Ana people, who are wanting to be buried, where they are to be buried, and the latter having been buried, the people in want of money, and the Santa Ana amiss, want some of the money to be used for the building of a chamber\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-don --start=\"Explain the relationship between Don Quixote and Sancho Panza\" --num_samples=5 --max_new_tokens=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543a34c-4311-481c-8754-e338afbd46de",
   "metadata": {},
   "source": [
    "If you'd like to sample from a model you trained, use the `--out_dir` to point the code appropriately. You can also prompt the model with some text from a file, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f20da7-fc90-4dbf-b860-8385a4323b79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: start = FILE:prompt/fictional.txt\n",
      "Overriding: out_dir = out-don\n",
      "Overriding: num_samples = 1\n",
      "Overriding: max_new_tokens = 100\n",
      "number of parameters: 123.65M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "Dain charges head on with his warhammer and full plate clad armor. Determined to topple anything in front of him.\n",
      "\n",
      "One of the last bastions of the Dawn so far held in the voids held a massive wattle and rotted horse that had been drenched in sweat and had been stripped of its armour. Its bones were cast into the ground and then buried in the snow.\n",
      "\n",
      "The dragon charged and made a wry noise, and turned back to face the Dawn. He glanced at the dragon, and then at Dain with a smile that was almost full of fear, but that made\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --start=FILE:\"prompt/fictional.txt\" --out_dir=\"out-don\" --num_samples=1 --max_new_tokens=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b1f055-0b1e-4f76-8311-2dd0df84eda5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: start = FILE:prompt/positive_review.txt\n",
      "Overriding: out_dir = out-don\n",
      "Overriding: num_samples = 1\n",
      "Overriding: max_new_tokens = 500\n",
      "number of parameters: 123.65M\n",
      "No meta.pkl found, assuming GPT-2 encodings...\n",
      "This place was DELICIOUS!! My parents saw a recommendation to visit this place from Rick Sebak's \\\"25 Things I Like About Pittsburgh\\\" and he's usually pretty accurate. His recommendations were to try the Reuben, Fish Sandwich and Open-Faced Steak Sandwich. We went early afternoon for a late lunch today (a Saturday) and were seated right away. The staff is extremely friendly. My Mom & I each had the fish sandwich, while my Dad & Brother had a Reuben sandwich. The fish was very good, but the Reuben was to die for! Both dishes were massive, and could very easily be shared between two people. On top of being extremely large portions, it was incredibly affordable. The giant fish sandwich was $8 and the giant Reuben was $7.50. Our drinks were always filled and we were checked on several times during the meal. We will definitely be back!!! Oh and a bit of advice ahead of time - they take CASH ONLY. So come prepared, but I'm pretty sure I saw an ATM there as well. And I do believe they are closed on Sundays & Mondays. Have fun and see you around!\n",
      "\n",
      "I've so far loved this spot and am going to be returning soon. I was coming up here looking to try some of the good things they have on offer. I was really happy to visit the steak sandwich and it had a definite hit of flavor. I was going to visit the steak man and the salad and the beer as well. And I don't know what else I want to see here, but I cannot wait.\n",
      "\n",
      "Honey, we don't like steak here and I know there's not a lot to say more. This was a great way to spend this afternoon at an amazing spot for a nice and iced meal. I was looking to find a place to eat for a quick meal. I had a quick breakfast and I tried the steak man's salad which was a nice side dish and was pretty good. I ordered the beer and the wine, and the steak sandwich was my favorite. I don't go to this place because it's a bit overpriced, but I can say it has a good name and this was a time to try it out. I had this for dinner last night, and I can't say I don't go there sometimes, especially with that kind of food. I thought it was a great spot to be staying for a while, considering the portions.\n",
      "\n",
      "I haven't had a steak sandwich in awhile but I have to say that when I came up through the restaurant it was a little gross. This restaurant has all kinds of great and juicy things at it's prices. I mean they were so good I tried everything. But I go first thing in the morning and I get sandwiches, fish, and salad. I got the fish sandwich with a salad. I got the steak sandwich with a salad. And finally the steak sandwich. I am so happy I found this place because I am such a sucker for deliciousness and fresh ingredients. Because as a chef I am so thankful and I know the restaurant is worthy of a nice restaurant. And my family and friends are so excited to go and see this place as well, so I'm convinced it would be a dream come true and would be a good place to eat a nice steak sandwich if you are pretty new to the dining look out for. That's what I mean. I don't know if I will come back. I know I just may come back, but I don't know what will happen to me or my family.\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --start=FILE:\"prompt/positive_review.txt\" --out_dir=\"out-don\" --num_samples=1 --max_new_tokens=500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600147b-8b88-4de0-bc2c-ee2fb7e781d1",
   "metadata": {},
   "source": [
    "I hope you will enjoy with the GPT as much as I did!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c0355-3e89-4ea0-9976-411dc15d76e5",
   "metadata": {},
   "source": [
    "## Efficiency notes\n",
    "\n",
    "*For simple model benchmarking and profiling, `bench.py` might be useful. It's identical to what happens in the meat of the training loop of `train.py`, but omits much of the other complexities.*\n",
    "\n",
    "*Note that the code by default uses [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/). At the time of writing (Dec 29, 2022) this makes `torch.compile()` available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!*\n",
    "\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "*Note that by default this repo uses PyTorch 2.0 (i.e. `torch.compile`). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding `--compile=False` flag. This will slow down the code but at least it will run.*\n",
    "\n",
    "*For some context on this repository, GPT, and language modeling it might be helpful to watch [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). Specifically, the [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is popular if you have some prior language modeling context.*\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This code is a fork from Andrej Karpathy's introductory [NanoGPT repository](https://github.com/karpathy/nanoGPT), which is an updated form of minGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4e757-8a74-40bf-a36c-9b5f59135121",
   "metadata": {},
   "source": [
    "# Further Experiments\n",
    "\n",
    "(Optional)\n",
    "\n",
    "For further experiments, you can, for example, reproduce the GPT-2, which is still powerful, by following the link to the Andrej Karpathy's repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ce869-04d3-4278-a449-a0c8edb1807b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
