{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Transformers\n",
    "\n",
    "**Author: Ümit Mert Çağlar**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will be demonstrating how to use the torch libraries to build transformers. We will be building transformers following the original architecture from the paper [Attention Is All You Need](https://arxiv.org/pdf/1706.03762). This notebook is the solution notebook where every code is working as intended. The counterpart of this notebook is the exercise notebook which should be completed prior to this one.\n",
    "\n",
    "<img src=\"transformer.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We will begin with importing necessary libraries.\n",
    "\n",
    "This part might be required to be operational on the Colab servers for later parts. It is advised to check the associated python requirements.txt, that is frozen at the time of preparation of this notebook, in case of any library or version error occurs while running this notebook. Mind that installing everything locally via pip install -r \"requirements.txt\" is not advised though, mainly because of the discrepancies between Colab and locally available machine.\n",
    "\n",
    "The [torch](https://pytorch.org/) is a popular and diverse machine learning framework, enabling low level implementation (as low as it gets with Python anyway). The Neural Networks (nn) is a library within PyTorch that enables operations with neural network structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from torch import Tensor\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, Callable, Iterator\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datasets import load_dataset\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordLevelTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "#### Instructions\n",
    "* Specify the PyTorch class that the positional encoder should subclass from.\n",
    "* Initialize a positional encoding matrix for token positions in sequences up to max_length.\n",
    "* Assign unique position encodings to the matrix pe by alternating the use of sine and cosine functions.\n",
    "* Update the input embeddings tensor x to add position information about the sequence using the positional encodings matrix.\n",
    "\n",
    "\n",
    "Remember we require embeddings to translate human words into machine readable format **and** add position information for the transformer architecture.\n",
    "\n",
    "<img src=\"embeddings.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Embeddings\n",
    "\n",
    "The input tokens passed through the Transformer model are first converted to vectors of dimension $d_{model}$ through a fixed **Input Embeddings**.\n",
    "\n",
    "The output of the embedding layer is scaled with $\\sqrt{d_{model}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "        Embedding layer for input tokens\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Hidden dimension of the model. The size of the vector\n",
    "                representations (embeddings / hidden states) used throughout the\n",
    "                Transformer model.\n",
    "            vocab_size (int): Size of the vocabulary. Number of unique tokens in the\n",
    "                input data.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # Input to embedding layer: (*)\n",
    "        # Output from embedding layer: (*, H), where H is the hidden dim of the model.\n",
    "        \n",
    "        # TODO: Create an embedding layer of size (vocab_size, d_model)\n",
    "        self.embedding = ...\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Embed input tokens.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tokens of shape `(bs, seq_len)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Embedded input of shape `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "        # seq_len dimension contains token ids that can be mapped back to unique word\n",
    "        \n",
    "        # TODO: Return the result of the embedded `x` tensor\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/PE_highlight.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "To be able to use the position information, transformers use **Positional Encoding**.\n",
    "\n",
    "We will be using sine and cosine waves for even and odd positions:\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{PE}_{(pos, 2i + 1)} = \\cos(pos / 10000^{2i/d_{model}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    pe: Tensor\n",
    "\n",
    "    def __init__(self, d_model: int, max_seq_len: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Positional encoding / embeddings for input tokens\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Hidden dimension of the model. The size of the vector\n",
    "                representations (embeddings / hidden states) used throughout the\n",
    "                Transformer model.\n",
    "            max_seq_len (int, optional): Maximum sequence length.\n",
    "            dropout (float, optional): Dropout rate. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # TODO: apply dropout\n",
    "        self.dropout = ...\n",
    "\n",
    "        # Create positional encodings of shape (max_seq_len, d_model)\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "\n",
    "        # Create tensor of shape (max_seq_len, 1)\n",
    "        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # PE division term => 10000^(2 * i / d_model)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # TODO: Use sine and cosine functions for even and odd positions\n",
    "        pe[:, 0::2] = ...  # Even positions\n",
    "        pe[:, 1::2] = ...  # Odd positions\n",
    "\n",
    "        # Add batch dimension to positional encodings\n",
    "        pe = pe.unsqueeze(0)  # (1, max_seq_len, d_model)\n",
    "\n",
    "        # Tensor is saved in file when model is saved.\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply positional encoding to input embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input embeddings of shape `(bs, seq_len, d_model)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Positional encodings of shape `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Add positional encodings to input embeddings\n",
    "        seq_len = x.size(1)\n",
    "        # Shorten positional encodings if seq_len is greater than max_seq_len\n",
    "        pe_out = (self.pe[:, :seq_len, :]).requires_grad_(False)\n",
    "        \n",
    "        # TODO: Add the positional information onto the input tensor `x`\n",
    "        x = ...\n",
    "        \n",
    "        # TODO: Apply dropout\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the __init__ method, we first initialize the superclass nn.Module and then define the model's dimension d_model and the maximum sequence length max_length. We then create a zero matrix pe of size max_length by d_model to store the positional encodings.\n",
    "\n",
    "Next, we calculate the positional encodings. We create a tensor position that contains the sequence positions and a tensor div_term that contains the division terms. The division terms are calculated using a formula that involves the natural logarithm of 10000 and the model's dimension. We then calculate the positional encodings by applying the sine function to the product of position and div_term for even indices and the cosine function for odd indices. The calculated positional encodings are then stored in the pe matrix.\n",
    "\n",
    "In the forward method, we add the positional encodings to the input embeddings tensor x. We slice the pe matrix to match the size of x before adding. The updated tensor x is then returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layers\n",
    "\n",
    "The **Multi-Head Attention block** is where the attention mechanism exists. It is computed fundamentally with scaled dot product attention.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/multi_headed_attention_highlighted.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "There are 2 types of attention in the Transformer: **Self-Attention** and **Cross-Attention**. They both use the same multi-head attention mechanism.\n",
    "\n",
    "The primary differences are:\n",
    "\n",
    "- **Self-Attention:** Queries, keys, and values come from the same input sequence.\n",
    "\n",
    "- **Cross Attention:** Queries come from the decoder’s hidden state and keys and values come from the encoder’s outputs.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"attention.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "We will be using $d_{\\text{model}} = 512$ with $h=8$ parallel attention layers (heads).\n",
    "\n",
    "Therefore, the dimension of queries, keys, and values will be $d_q=d_k=d_v=d_{model}/h = 64$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention\n",
    "\n",
    "Each head of attention is computed with the above scaled dot-product attention and then concatenated.\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\cdots, \\text{head}_h) W^O\n",
    "$$\n",
    "$$\n",
    "\\text{where head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "* Split the sequence embeddings x across the multiple attention heads.\n",
    "* Compute dot-product based attention scores between the project query and key.\n",
    "* Normalize the attention scores to obtain attention weights.\n",
    "* Multiply the attention weights by the values and linearly transform the concatenated outputs per head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads  # aka. `h`\n",
    "\n",
    "        assert d_model % num_heads == 0  # Ensure d_model is divisible by num_heads\n",
    "\n",
    "        # TODO: Calculate the dimension of d_k, d_v\n",
    "        \n",
    "        self.d_k = ...\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(\n",
    "        q: Tensor, k: Tensor, v: Tensor, mask: Tensor | None, dropout: nn.Dropout | None\n",
    "    ) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Compute Scaled Dot Product Attention.\"\"\"\n",
    "\n",
    "        d_k = q.shape[-1]\n",
    "\n",
    "        # TODO: Compute attention scores (not applying softmax) with\n",
    "        # @ operation (matrix multiply) and .transpose()\n",
    "\n",
    "        \n",
    "        # (bs, num_heads, seq_len, d_k) -> (bs, num_heads, seq_len, seq_len)\n",
    "        scores = ...\n",
    "\n",
    "        if mask is not None:\n",
    "            # For all values in mask == 0 replace with -inf\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Each row is a query, each column is a key. You want to convert raw scores over keys\n",
    "        # into a probability distribution. In other words, you want each row / query to have\n",
    "        # weights that sum to 1.\n",
    "        \n",
    "        # TODO: Apply softmax to last dim\n",
    "        scores = ...  # (bs, num_heads, seq_len, seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "\n",
    "        # TODO: Multiply by values    \n",
    "        weights = ...  # (bs, num_heads, seq_len, d_k)\n",
    "\n",
    "        # We return the scores for visualization\n",
    "        return weights, scores\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Tensor | None) -> Tensor:\n",
    "        \"\"\"Compute Multi-Headed Attention.\"\"\"\n",
    "\n",
    "        query = self.W_q(q)  # (bs, seq_len, d_model) -> (bs, seq_len, d_model)\n",
    "        key = self.W_k(k)  # (bs, seq_len, d_model) -> (bs, seq_len, d_model)\n",
    "        value = self.W_v(v)  # (bs, seq_len, d_model) -> (bs, seq_len, d_model)\n",
    "\n",
    "        # (bs, seq_len, d_model) -> (bs, seq_len, num_heads, d_k)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k)\n",
    "        # (bs, seq_len, num_heads, d_k) -> (bs, num_heads, seq_len, d_k)\n",
    "        query = query.transpose(1, 2)\n",
    "\n",
    "        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k)\n",
    "        key = key.transpose(1, 2)\n",
    "\n",
    "        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        \n",
    "        # TODO: apply MultiHeadAttention.scaled_dot_product_attention\n",
    "        weights, scores = ...\n",
    "\n",
    "        ### Perform concatenation of the heads ###\n",
    "\n",
    "        # (bs, num_heads, seq_len, d_k) -> (bs, seq_len, num_heads, d_k)\n",
    "        weights = weights.transpose(1, 2)\n",
    "\n",
    "        # (bs, seq_len, num_heads, d_k) -> (bs, seq_len, d_model)\n",
    "        concat = weights.contiguous().view(\n",
    "            weights.shape[0], weights.shape[1], self.d_model\n",
    "        )\n",
    "\n",
    "        # (bs, seq_len, d_model) -> (bs, seq_len, d_model)\n",
    "        return self.W_o(concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Network\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/feed_forward_highlighted.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "The Feed-Forward Network (FFN) in the Transformer architecture is applied independently to each token position after the Multi-Head Self-Attention Mechanism. It consists of two linear layers with a non-linearity:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "The dimensionality of input and output is $d_{model} = 512$, and the inner-layer has dimensionality $d_{ff} = 2048$.\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "* Specify in the __init__() method the sizes of the two linear fully connected layers.\n",
    "* Apply a forward pass through the two linear layers, using the ReLU() activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int = 512, d_ff: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 'The dimensionality of input and output is d_model = 512, and the inner-layer\n",
    "        # has dimensionality d_ff = 2048.'\n",
    "        \n",
    "        # TODO: Create the two linear transformations between\n",
    "        self.linear1 = ...\n",
    "        self.dropout = ...\n",
    "        self.linear2 = ...\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Applies linear transformations with ReLU activation function between.\n",
    "            1. (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff)\n",
    "            2. (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): `(bs, seq_len, d_model)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO: Create forward pass. Apply dropout after ReLU.\n",
    "        x = ...\n",
    "        x = ...\n",
    "        x = ...\n",
    "        x = ...\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LayerNorm operates independently on each sample within a batch, unlike BatchNorm, which normalizes across the batch dimension. It normalizes the inputs across the feature dimension.\n",
    "\n",
    "**Purpose:** Mitigate internal covariate shift thus improving training speed, stability, and convergence of the model. Also, improves generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        LayerNorm operates independently on each sample within a batch, unlike\n",
    "        BatchNorm, which normalizes across the batch dimension. It normalizes the\n",
    "        inputs across the feature dimension.\n",
    "\n",
    "        Purpose: Mitigate internal covariate shift thus improving training speed,\n",
    "        stability, and convergence of the model. Also, improves generalization.\n",
    "\n",
    "        Args:\n",
    "            eps (float, optional): Epsilon value to avoid division by zero.\n",
    "                Defaults to 1e-6.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # Two learnable parameters\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # Scale parameter (Multiplicative)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # Shift parameter (Additive)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply layer norm to last dimension of the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): `(bs, seq_len, d_model)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "        # Apply mean & std to last dimension\n",
    "\n",
    "        # TODO: Apply mean & std to last dimension\n",
    "        mean = ...  # (bs, seq_len, 1)\n",
    "        std = ... # (bs, seq_len, 1)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connections\n",
    "\n",
    "The paper defines the residual connection implementation as\n",
    "$$\n",
    "\\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
    "$$\n",
    "\n",
    "However, we will follow [The Annotated Transformer's](https://nlp.seas.harvard.edu/2018/04/03/attention.html) implementation by applying dropout to the output of each normalized sub-layer, before adding it to the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x: Tensor, sublayer: nn.Module) -> Tensor:\n",
    "        \"\"\"\n",
    "        Residual connection with layer normalization.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): `(bs, seq_len, d_model)`.\n",
    "            sublayer (nn.Module): The intermediate layer to wrap w/ residual connection.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "        # TODO: Apply dropout to sublayer \n",
    "        \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Layer\n",
    "\n",
    "This layer is a projection from $d_{model}$ into log probabilities across the entire vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "        Linear Layer is a projection layer that converts the embedding into the\n",
    "        vocabulary.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The size of the model's hidden dimension.\n",
    "            vocab_size (int): The size of the vocabulary.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Create a linear layer of size (d_model, vocab_size)\n",
    "        self.linear = ...\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply projection on embeddings.\n",
    "        Output will be a log probability distribution over the vocabulary.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): `(bs, seq_len, d_model)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, vocab_size)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # (bs, seq_len, d_model) -> (bs, seq_len, vocab_size)\n",
    "        # TODO: return log probabilities\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Structure\n",
    "We can finally put everything together\n",
    "\n",
    "<img src=\"transformer.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENCODER ###\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        self_attention_block: MultiHeadAttention,\n",
    "        feed_forward_block: FeedForwardBlock,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList(\n",
    "            [ResidualConnection(dropout) for _ in range(2)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder block.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): `(bs, seq_len, d_model)`.\n",
    "            src_mask (Tensor): The mask for the source language `(bs, 1, 1, seq_len)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "        # TODO: build self attention blocks and residual connections\n",
    "        x = self.residual_connections[0](\n",
    "            x, lambda x: ...\n",
    "        )\n",
    "        x = ...\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Foward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input to the encoder.\n",
    "            src_mask (Tensor): The mask for the source language.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: A tensor of `(batch_size, seq_len, d_model)` represents a sequence\n",
    "                of context-rich embeddings that encode the input sequence's semantic and\n",
    "                positional information.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        # TODO: Apply a final layer normalization after all encoder blocks\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/Cross_attention.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "For **Cross Attention**, queries come from the decoder and keys and values come from the encoder’s outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DECODER ###\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        self_attention_block: MultiHeadAttention,\n",
    "        cross_attention_block: MultiHeadAttention,\n",
    "        feed_forward_block: FeedForwardBlock,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Decoder block contains:\n",
    "            1. (Masked Multi-Head Attention) A self-attention block where `qkv` come\n",
    "                from decoder's input embedding.\n",
    "            2. (Multi-Head Attention) A cross-attention block where `q` come from\n",
    "                decoder and `k`,`v` come from encoder outputs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList(\n",
    "            [ResidualConnection(dropout) for _ in range(3)]\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        encoder_output: Tensor,\n",
    "        src_mask: Tensor,\n",
    "        tgt_mask: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder block.\n",
    "        Decoder block ussed for machine-translation to go from source to target lang.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The decoder input `(bs, seq_len, d_model)`.\n",
    "            encoder_output (Tensor): `(bs, seq_len, d_model)`.\n",
    "            src_mask (Tensor): `(bs, 1, 1, seq_len)`.\n",
    "            tgt_mask (Tensor): `(bs, 1, seq_len, seq_len)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "        x = self.residual_connections[0](\n",
    "            x, lambda x: self.self_attention_block(x, x, x, tgt_mask)\n",
    "        )\n",
    "\n",
    "        # TODO: use encoder output here in cross-attention block\n",
    "        x = self.residual_connections[1](\n",
    "            x,\n",
    "            lambda x: self.cross_attention_block(\n",
    "                ...\n",
    "            ),\n",
    "        )\n",
    "        # TODO: finish feed forward network and residual connections\n",
    "        x = ...\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        encoder_output: Tensor,\n",
    "        src_mask: Tensor,\n",
    "        tgt_mask: Tensor,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input to the decoder block.\n",
    "            encoder_output (Tensor): The output from the encoder.\n",
    "            src_mask (Tensor): The mask used for the source language (e.g. English).\n",
    "            tgt_mask (Tensor): The mask used for the target language (e.g. Turkish).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(size: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Causal mask used only in decoder to ensure that future is masked.\n",
    "    https://discuss.huggingface.co/t/difference-between-attention-mask-and-causal-mask/104922\n",
    "    \"\"\"\n",
    "\n",
    "    # Diagonal=1 to get a mask that does not include the main diagonal and only the\n",
    "    # upper triangular part of the matrix excluding the main diagonal\n",
    "    ones_matrix = torch.ones(1, size, size)\n",
    "    # TODO: create a causal mask\n",
    "    mask = ...\n",
    "\n",
    "    # The above returns the mask for the upper diagonal which we DON'T want to include\n",
    "    # in the causal mask. We want it False. Therefore, we use `mask == 0`.\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRANSFORMER ###\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder: Encoder,\n",
    "        decoder: Decoder,\n",
    "        src_embed: InputEmbeddings,\n",
    "        tgt_embed: InputEmbeddings,\n",
    "        src_pos: PositionalEncoding,\n",
    "        tgt_pos: PositionalEncoding,\n",
    "        projection_layer: LinearLayer,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"Forward pass through the encoder with input tokens of type int64.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): `(bs, seq_len)`.\n",
    "            src_mask (Tensor): `(bs, 1, 1, seq_len)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Embedding maps token ids to dense vectors of type float32\n",
    "        # TODO: apply positional embedding and encoder layers to encoder input\n",
    "        src = ...  # (bs, seq_len) -> (bs, seq_len, d_model)\n",
    "        src = ...\n",
    "        return ...\n",
    "\n",
    "    def decode(\n",
    "        self, encoder_output: Tensor, src_mask: Tensor, tgt: Tensor, tgt_mask: Tensor\n",
    "    ) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "        - Encoder output is used in the cross-attention block and is of type float32.\n",
    "        - Target tokens are still of type int64 and need to be embedded with input\n",
    "        embeddings + positional encoding.\n",
    "\n",
    "        Args:\n",
    "            encoder_output (Tensor): `(bs, seq_len, d_model)`.\n",
    "            src_mask (Tensor): `(bs, 1, 1, seq_len)`.\n",
    "            tgt (Tensor): `(bs, seq_len)`.\n",
    "            tgt_mask (Tensor): `(bs, 1, seq_len, seq_len)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, d_model)`.\n",
    "        \"\"\"\n",
    "        # TODO: Apply positional embedding and decoder layers to decoder input\n",
    "        tgt = ...  # (bs, seq_len) -> (bs, seq_len, d_model)\n",
    "        tgt = ...\n",
    "        return ...\n",
    "\n",
    "    def project(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Project the output of the decoder to the target vocabulary size.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The output of the decoder `(bs, seq_len, d_model)`.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: `(bs, seq_len, vocab_size)`.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.projection_layer(x)\n",
    "\n",
    "\n",
    "def build_transformer(\n",
    "    src_vocab_size: int,\n",
    "    tgt_vocab_size: int,\n",
    "    src_seq_len: int,\n",
    "    tgt_seq_len: int,\n",
    "    d_model: int = 512,  # hidden dimension of the model\n",
    "    num_blocks: int = 6,  # number of encoder and decoder blocks\n",
    "    num_heads: int = 8,  # number of attention heads\n",
    "    d_ff: int = 2048,  # size of the feed-forward layer\n",
    "    dropout: float = 0.1,\n",
    ") -> Transformer:\n",
    "    \"\"\"Build and return Transformer.\"\"\"\n",
    "\n",
    "    # TODO: Create embedding layers\n",
    "    src_embed = ...\n",
    "    tgt_embed = ...\n",
    "\n",
    "    # TODO: Create positional encoding layers\n",
    "    src_pos = ...\n",
    "    tgt_pos = ...\n",
    "\n",
    "    # TODO: Create encoder blocks\n",
    "    encoder_layers = nn.ModuleList(\n",
    "        [\n",
    "            EncoderBlock(\n",
    "                ...\n",
    "                ...\n",
    "                ...\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # TODO: Create decoder blocks\n",
    "    decoder_layers = nn.ModuleList(\n",
    "        [\n",
    "            DecoderBlock(\n",
    "                ...\n",
    "                ...\n",
    "                ...\n",
    "                ...\n",
    "            )\n",
    "            for _ in range(num_blocks)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # TODO: Create encoder and decoder\n",
    "    encoder = ...\n",
    "    decoder = ...\n",
    "\n",
    "    # TODO: Create projection layer\n",
    "    projection_layer = ...\n",
    "\n",
    "    # TODO: Create transformer\n",
    "    transformer = Transformer(\n",
    "        ...\n",
    "    )\n",
    "\n",
    "    # Initialize parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights file path for epoch epoch_1: weights\\transformer_epoch_1.pt\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    \"\"\"\n",
    "    Transformer training configuration\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size: int = 16\n",
    "    num_epochs: int = 10\n",
    "    lr: float = 1e-4\n",
    "    seq_len: int = 350  # Max sequence length\n",
    "    d_model: int = 512\n",
    "    lang_src: str = \"en\"  # Source language: English\n",
    "    lang_tgt: str = \"it\"  # Target language: Italian\n",
    "    model_folder: str = \"weights\"\n",
    "    model_filename: str = \"transformer_\"  # Base filename for saved weights\n",
    "    load_from: Optional[str] = None  # Load from this epoch (e.g.)\n",
    "    tokenizer_file: str = \"tokenizer_0.json\"\n",
    "    experiment_name: str = \"runs/transformer\"\n",
    "\n",
    "    def get_weights_file_path(self, epoch_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the file path for the model weights corresponding to a given epoch.\n",
    "\n",
    "        Parameters:\n",
    "            epoch_name (str): The epoch name to get weights file path for.\n",
    "\n",
    "        Returns:\n",
    "            str: The complete file path for the weights file.\n",
    "        \"\"\"\n",
    "        return str(\n",
    "            Path(\".\") / self.model_folder / f\"{self.model_filename}{epoch_name}.pt\"\n",
    "        )\n",
    "\n",
    "\n",
    "def get_config(overrides: Optional[Dict[str, Any]] = None) -> TransformerConfig:\n",
    "    \"\"\"\n",
    "    Retrieve the default configuration for the Transformer model training.\n",
    "    Optionally override configuration values by passing in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "        overrides (Optional[Dict[str, Any]]): Dictionary with keys corresponding to the\n",
    "            TransformerConfig fields that should be overridden.\n",
    "\n",
    "    Returns:\n",
    "        TransformerConfig: An instance of TransformerConfig with default values.\n",
    "    \"\"\"\n",
    "    config = TransformerConfig()\n",
    "    if overrides:\n",
    "        for key, value in overrides.items():\n",
    "            if hasattr(config, key):\n",
    "                setattr(config, key, value)\n",
    "            else:\n",
    "                raise KeyError(f\"Invalid config key: {key}\")\n",
    "    return config\n",
    "\n",
    "\n",
    "\n",
    "# Test get config\n",
    "config = get_config({\"batch_size\": 32, \"num_epochs\": 5})\n",
    "epoch = \"epoch_1\"\n",
    "weights_path = config.get_weights_file_path(epoch)\n",
    "print(f\"Weights file path for epoch {epoch}: {weights_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Subset,\n",
    "        tokenizer_src: Tokenizer,\n",
    "        tokenizer_tgt: Tokenizer,\n",
    "        lang_src: str,\n",
    "        lang_tgt: str,\n",
    "        seq_len: int,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.lang_src = lang_src\n",
    "        self.lang_tgt = lang_tgt\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Vocab can be longer than 32 bits so we use int64\n",
    "        self.sos_token = torch.tensor(\n",
    "            [tokenizer_src.token_to_id(\"[SOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.eos_token = torch.tensor(\n",
    "            [tokenizer_src.token_to_id(\"[EOS]\")], dtype=torch.int64\n",
    "        )\n",
    "        self.pad_token = torch.tensor(\n",
    "            [tokenizer_src.token_to_id(\"[PAD]\")], dtype=torch.int64\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: Any) -> Dict[str, Any]:\n",
    "        # Get the source and target text pair\n",
    "        src_tgt_pair = self.dataset[index]\n",
    "\n",
    "        # Extract the individual source and target text\n",
    "        src_text = src_tgt_pair[\"translation\"][self.lang_src]\n",
    "        tgt_text = src_tgt_pair[\"translation\"][self.lang_tgt]\n",
    "\n",
    "        # Convert tokens to ids\n",
    "        enc_input = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # -2 to account for the start and end tokens\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input) - 2\n",
    "\n",
    "        # -1 to account for the end token\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input) - 1\n",
    "\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long.\")\n",
    "\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                self.pad_token.repeat(enc_num_padding_tokens),  # Pad to reach seq_len\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input, dtype=torch.int64),\n",
    "                self.pad_token.repeat(dec_num_padding_tokens),  # Pad to reach seq_len\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                self.pad_token.repeat(dec_num_padding_tokens),  # Pad to reach seq_len\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        encoder_mask = (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int()\n",
    "\n",
    "        # Causal mask to prevent attending to future\n",
    "        casual_mask = create_causal_mask(decoder_input.size(0))\n",
    "        decoder_mask = (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int()\n",
    "        decoder_mask = decoder_mask & casual_mask\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,  # (seq_len)\n",
    "            \"decoder_input\": decoder_input,  # (seq_len)\n",
    "            \"encoder_mask\": encoder_mask,  # (1, 1, seq_len)\n",
    "            \"decoder_mask\": decoder_mask,  # (1, seq_len, seq_len)\n",
    "            \"label\": label,  # (seq_len)\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Transformer model training and validation\"\"\"\n",
    "\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "        # Define the device\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Create model folder\n",
    "        Path(self.config.model_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        train_dl, val_dl, tokenizer_src, tokenizer_tgt = self._get_dataset()\n",
    "        self.model = self._get_model(\n",
    "            tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.train_dl, self.val_dl = train_dl, val_dl\n",
    "        self.tokenizer_src, self.tokenizer_tgt = tokenizer_src, tokenizer_tgt\n",
    "\n",
    "        # 1. Ignore the padding ([PAD]) tokens\n",
    "        # 2. Apply label smoothing - distributes X% of highest probability tokens to other tokens\n",
    "        self.loss_fn = nn.CrossEntropyLoss(\n",
    "            ignore_index=self.tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.writer = SummaryWriter(self.config.experiment_name)  # Tensorboard\n",
    "\n",
    "        # Create adam optimizer\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(), lr=self.config.lr, eps=1e-9\n",
    "        )\n",
    "\n",
    "        # Load existing model if it is specified / exists\n",
    "        self._load_existing_model()\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.initial_epoch = 0\n",
    "        self.max_len = self.config.seq_len\n",
    "\n",
    "    # Use word level tokenization\n",
    "    def _get_all_sentences(self, dataset, lang) -> Iterator[str]:\n",
    "        \"\"\"Get all sentences in a given language from the dataset as a generator.\"\"\"\n",
    "\n",
    "        for item in dataset:\n",
    "            yield item[\"translation\"][lang]\n",
    "\n",
    "    def _build_tokenizer(self, dataset, lang) -> Tokenizer:\n",
    "        \"\"\"Build / load a word-level tokenizer.\"\"\"\n",
    "\n",
    "        tokenizer_path = Path(self.config.tokenizer_file.format(lang))\n",
    "\n",
    "        if not Path.exists(tokenizer_path):\n",
    "            tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "            tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "            # Train tokenizer with special tokens:\n",
    "            # Unknown, padding, start of sentence, end of sentence\n",
    "            trainer = WordLevelTrainer(\n",
    "                special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n",
    "            )\n",
    "\n",
    "            tokenizer.train_from_iterator(\n",
    "                self._get_all_sentences(dataset, lang), trainer=trainer\n",
    "            )\n",
    "        else:\n",
    "            tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def _get_dataset(self) -> tuple[DataLoader, DataLoader, Tokenizer, Tokenizer]:\n",
    "        \"\"\"Load the dataset to create dataloaders and tokenizers.\"\"\"\n",
    "\n",
    "        ds_raw = load_dataset(\n",
    "            \"opus_books\",\n",
    "            f\"{self.config.lang_src}-{self.config.lang_tgt}\",\n",
    "            split=\"train\",\n",
    "        )\n",
    "\n",
    "        # Build tokenizers\n",
    "        tokenizer_src = self._build_tokenizer(ds_raw, self.config.lang_src)\n",
    "        tokenizer_tgt = self._build_tokenizer(ds_raw, self.config.lang_tgt)\n",
    "\n",
    "        # Keep 90% for training and 10% for validation\n",
    "        train_size = int(0.9 * len(ds_raw))\n",
    "        val_size = len(ds_raw) - train_size\n",
    "        train_ds_raw, val_ds_raw = random_split(ds_raw, [train_size, val_size])\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = BilingualDataset(\n",
    "            train_ds_raw,\n",
    "            tokenizer_src,\n",
    "            tokenizer_tgt,\n",
    "            self.config.lang_src,\n",
    "            self.config.lang_tgt,\n",
    "            self.config.seq_len,\n",
    "        )\n",
    "        val_dataset = BilingualDataset(\n",
    "            val_ds_raw,\n",
    "            tokenizer_src,\n",
    "            tokenizer_tgt,\n",
    "            self.config.lang_src,\n",
    "            self.config.lang_tgt,\n",
    "            self.config.seq_len,\n",
    "        )\n",
    "\n",
    "        # Find max sentence in src and tgt languages\n",
    "        max_len_src = 0\n",
    "        max_len_tgt = 0\n",
    "\n",
    "        for item in ds_raw:\n",
    "            src_text = item[\"translation\"][self.config.lang_src]\n",
    "            tgt_text = item[\"translation\"][self.config.lang_tgt]\n",
    "\n",
    "            src_ids = tokenizer_src.encode(src_text).ids\n",
    "            tgt_ids = tokenizer_src.encode(tgt_text).ids\n",
    "            max_len_src = max(max_len_src, len(src_ids))\n",
    "            max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "        print(f\"Max length of source sentence: {max_len_src}\")\n",
    "        print(f\"Max length of source sentence: {max_len_tgt}\")\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, batch_size=self.config.batch_size, shuffle=True\n",
    "        )\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "        return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "    def _get_model(self, vocab_src_len, vocab_tgt_len) -> Transformer:\n",
    "        \"\"\"Get the transformer model.\"\"\"\n",
    "\n",
    "        model = build_transformer(\n",
    "            vocab_src_len,\n",
    "            vocab_tgt_len,\n",
    "            self.config.seq_len,\n",
    "            self.config.seq_len,\n",
    "            self.config.d_model,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def _load_existing_model(self) -> None:\n",
    "        \"\"\"Load an existing model from a given epoch.\"\"\"\n",
    "\n",
    "        if self.config.load_from:\n",
    "            epoch_name = self.config.load_from\n",
    "            model_filename = self.config.get_weights_file_path(epoch_name)\n",
    "            print(f\"Preloading model {model_filename}\")\n",
    "            state = torch.load(model_filename)\n",
    "\n",
    "            self.initial_epoch = state[\"epoch\"] + 1\n",
    "            self.model.load_state_dict(state[\"model_state_dict\"])\n",
    "            self.optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "            self.global_step = state[\"global_step\"]\n",
    "\n",
    "    ### TRAINING CODE ###\n",
    "    def train(self) -> None:\n",
    "        \"\"\"Train the transformer model.\"\"\"\n",
    "\n",
    "        for epoch in range(self.initial_epoch, self.config.num_epochs):\n",
    "            batch_iterator = tqdm(self.train_dl, desc=f\"Processing epoch {epoch:02d}\")\n",
    "\n",
    "            for batch in batch_iterator:\n",
    "                self.model.train()\n",
    "\n",
    "                encoder_input = batch[\"encoder_input\"].to(self.device)  # (bs, seq_len)\n",
    "                decoder_input = batch[\"decoder_input\"].to(self.device)  # (bs, seq_len)\n",
    "\n",
    "                # Attention mask (hide padding tokens)\n",
    "                encoder_mask = batch[\"encoder_mask\"].to(\n",
    "                    self.device\n",
    "                )  # (bs, 1, 1, seq_len)\n",
    "\n",
    "                # Casual mask (hide padding tokens and future)\n",
    "                decoder_mask = batch[\"decoder_mask\"].to(\n",
    "                    self.device\n",
    "                )  # (bs, 1, seq_len, seq_len)\n",
    "\n",
    "                # Input passthrough\n",
    "                encoder_output = self.model.encode(\n",
    "                    encoder_input, encoder_mask\n",
    "                )  # (bs, seq_len, d_model)\n",
    "                decoder_output = self.model.decode(\n",
    "                    encoder_output, encoder_mask, decoder_input, decoder_mask\n",
    "                )  # (bs, seq_len, d_model)\n",
    "                proj_output = self.model.project(\n",
    "                    decoder_output\n",
    "                )  # (bs, seq_len, tgt_vocab_size)\n",
    "\n",
    "                label = batch[\"label\"].to(self.device)  # (bs, seq_len)\n",
    "\n",
    "                # (bs, seq_len, tgt_vocab_size) -> (bs * seq_len, tgt_vocab_size)\n",
    "                pred = proj_output.view(-1, self.tokenizer_tgt.get_vocab_size())\n",
    "                gt = label.view(-1)  # Ground truth\n",
    "                loss = self.loss_fn(pred, gt)\n",
    "                batch_iterator.set_postfix({\"Loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "                # Log the loss in tensorboard\n",
    "                self.writer.add_scalar(\"Train Loss\", loss.item(), self.global_step)\n",
    "                self.writer.flush()\n",
    "\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "\n",
    "                # Update weights\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                self.global_step += 1\n",
    "\n",
    "            model_filename = self.config.get_weights_file_path(f\"{epoch:02d}\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": self.model.state_dict(),\n",
    "                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                    \"global_step\": self.global_step,\n",
    "                },\n",
    "                model_filename,\n",
    "            )\n",
    "\n",
    "            # Run validation at every 5th epoch\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.validate(lambda msg: batch_iterator.write(msg), self.writer)\n",
    "\n",
    "    ### VALIDATION CODE ###\n",
    "    def greedy_decode(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Greedy decode for efficient validation.\n",
    "        Highest probability token is selected at each step as the next word.\n",
    "        \"\"\"\n",
    "\n",
    "        sos_idx = self.tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "        eos_idx = self.tokenizer_tgt.token_to_id(\"[EOS]\")\n",
    "\n",
    "        # Precompute the encoder output and reuse it for every token we get from decoder\n",
    "        encoder_output = self.model.encode(src, src_mask)\n",
    "\n",
    "        # Initialize the decoder input with the SOS token\n",
    "        decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(src).to(self.device)\n",
    "\n",
    "        # Keep predicting until we reach EOS or max_len\n",
    "        while True:\n",
    "            if decoder_input.size(1) == self.config.seq_len:\n",
    "                break\n",
    "\n",
    "            # Build the mask\n",
    "            decoder_mask = create_causal_mask(decoder_input.size(1))\n",
    "            decoder_mask = decoder_mask.type_as(src_mask).to(self.device)\n",
    "\n",
    "            # Calculate the output of the decoder\n",
    "            out = self.model.decode(\n",
    "                encoder_output, src_mask, decoder_input, decoder_mask\n",
    "            )\n",
    "\n",
    "            # Get the next token\n",
    "            prob = self.model.project(out[:, -1])  # the project of the last token\n",
    "\n",
    "            # Select the token with the highest probability (because it is greedy search)\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "\n",
    "            decoder_input = torch.cat(\n",
    "                [\n",
    "                    decoder_input,\n",
    "                    torch.empty(1, 1)\n",
    "                    .type_as(src)\n",
    "                    .fill_(next_word.item())\n",
    "                    .to(self.device),\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            if next_word == eos_idx:\n",
    "                break\n",
    "\n",
    "        return decoder_input.squeeze(0)  # remove batch dimension\n",
    "\n",
    "    def validate(self, print_msg: Callable, writer: SummaryWriter) -> None:\n",
    "        \"\"\"Run transformer model on the validation dataset.\"\"\"\n",
    "\n",
    "        self.model.eval()  # Put in eval mode\n",
    "        count = 0\n",
    "\n",
    "        source_texts = []\n",
    "        expected = []\n",
    "        predicted = []\n",
    "\n",
    "        CONSOLE_WIDTH = 80\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_dl:\n",
    "                # Stop validation after 5 examples\n",
    "                if count >= 5:\n",
    "                    break\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                # (bs, seq_len)\n",
    "                encoder_input = batch[\"encoder_input\"].to(self.device)\n",
    "\n",
    "                # (bs, 1, 1, seq_len)\n",
    "                encoder_mask = batch[\"encoder_mask\"].to(self.device)\n",
    "\n",
    "                assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "                # Get generation\n",
    "                model_out = self.greedy_decode(encoder_input, encoder_mask)\n",
    "\n",
    "                source_text = batch[\"src_text\"][0]\n",
    "                target_text = batch[\"tgt_text\"][0]\n",
    "\n",
    "                # Detach model_out from computational graph\n",
    "                model_out_tensor = model_out.detach().cpu()\n",
    "                model_out_array = model_out_tensor.numpy()\n",
    "                model_out_text = self.tokenizer_tgt.decode(model_out_array)\n",
    "\n",
    "                source_texts.append(source_text)\n",
    "                expected.append(target_text)\n",
    "                predicted.append(model_out_text)\n",
    "\n",
    "                # Print to the console\n",
    "                print_msg(\"-\" * CONSOLE_WIDTH)\n",
    "                print_msg(f\"SOURCE: {source_text}\")\n",
    "                print_msg(f\"TARGET: {target_text}\")\n",
    "                print_msg(f\"PREDICTED: {model_out_text}\")\n",
    "\n",
    "        if writer:\n",
    "            # Compute the char error rate\n",
    "            metric = torchmetrics.CharErrorRate()\n",
    "            cer = metric(predicted, expected)\n",
    "            writer.add_scalar(\"Validation CER\", cer, self.global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Compute the word error rate\n",
    "            metric = torchmetrics.WordErrorRate()\n",
    "            wer = metric(predicted, expected)\n",
    "            writer.add_scalar(\"Validation WER\", wer, self.global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Compute the BLEU metric\n",
    "            metric = torchmetrics.BLEUScore()\n",
    "            bleu = metric(predicted, expected)\n",
    "            writer.add_scalar(\"Validation BLEU\", bleu, self.global_step)\n",
    "            writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Max length of source sentence: 309\n",
      "Max length of source sentence: 274\n"
     ]
    }
   ],
   "source": [
    "config = get_config()\n",
    "trainer = Trainer(config)\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this notebook we have observed and practiced how to build transformers from zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "References and further tutorials to check out:\n",
    "1. This notebook is built on top of the works of [Transformers From Scratch](https://github.com/aandyw/TransformerFromScratch) \n",
    "1. The paper, Attention is all you need, the transformer paper that introduced a whole new way of approaching deep learning: https://arxiv.org/abs/1706.03762\n",
    "1. A solid blog that explains the Transformer architecture and its sub components: https://buomsoo-kim.github.io/attention/2020/04/19/Attention-mechanism-17.md/\n",
    "1. Training a Transformer model, with a real dataset: https://buomsoo-kim.github.io/attention/2020/04/20/Attention-mechanism-18.md/\n",
    "1. Further explanations on top of previous posts https://buomsoo-kim.github.io/attention/2020/04/21/Attention-mechanism-19.md/\n",
    "1. Another Transformer tutorial: https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "1. The annotated Transformer, updated for newer version of PyTorch, an excellent guide that uses the original authors sentences and implement everything (just like we did in this tutorial): https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "1. PyTorch's documentation for Transformers and other helper libraries: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "1. Dive into Deep Learning tutorial that uses their version of libraries, a different take on Transformer model implementation: https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
