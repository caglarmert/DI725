{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPdgYB4BgiNw+1opFXftQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caglarmert/DI725/blob/main/DI725_Lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DI 725: Transformers and Attention-Based Deep Networks\n",
        "\n",
        "## A Tutorial for Retrieval Augmented Generation\n",
        "<a id='intro_tutorial'></a>\n",
        "The purpose of this notebook is to introduce Retrieval Augmented Generation, RAG for short. RAG is a concept of in-context learning. In context learning is a way of transfering information from a context to a large language model that has never seen that information before!\n",
        "\n",
        "RAG is a tool that can enhance the question-answering abilities of already trained models by incorporating new information. To achieve this, we will import the fresh data into a vector database, which will essentially act as an external memory for the model. The retrieval model, which in our case is llama-index, will then utilize this database to create a specific prompt for the given task and retrieve the relevant document, which will be passed along with the prompt to the language model.\n",
        "\n",
        "We will have two examples, first we will start with Quantized Low Rank Adapters (QLoRA) from the work [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314). This is an interesting apporach, one that is particularly useful for the researchers with limited resources. It basically combines quantization and finetuning to achieve similar performance of the baseline models, which require tremendeous amount of resources compared to QLoRA. The second example is [Class Distance Weighted Cross Entropy](https://arxiv.org/abs/2202.05167) loss function, a special loss function that is proven to be useful for ordinal classes, that is common in medical domain. None of these papers are present in the training of the LLM we are going to use in this lab! We will make sure of it by asking questions about these terms, and will observe either the model failing to answer, or hallucinate about it outright!\n",
        "\n",
        "We will start with an LLM from Antropic, see the details in [Computational Requirements section](#comp_req). This model does not known anything about QloRA, and we will make sure of it by asking the term. The LLM will respond with a phrase that it doesn't know the term.\n",
        "\n",
        "After downloading the paper that describes QLoRA work, we will then use Retrieval Augmented Generation (RAG) to provide the context to the LLM. This is how in-context learning is applied to an already existing and powerful enough model (a foundational model).\n",
        "\n",
        "In our second example, we will use a work conducted in METU. This time, when asked about the term \"Class Distance Weighted Cross Entropy\" the LLM will provide an answer, although it doesnt know anything about it, it will hallucinate about the term. I have conducted a brief research and concluded that this LLM hallucinates about CDW-CE, comes up with \"Contrastive Divergence with Data Augmentation and Consistency Encoding\" or other nonsensical contents. Furthermore, it explains this hallucinated content and explains it like it is a real phenomenon. This is totall unacceptable!\n",
        "\n",
        "We will again use RAG (and thus in-context learning) to enhance the capability of our model, and provide a context for it to work on! We will upload the work conducted in our Institute. The answers provided by the model will resemble the original work, and now has a context, thus no hallucination will occur!\n",
        "\n",
        "Up to this point, we have demonstrated the capability of RAG with a single context document. How would the models fare with multiple documents, like an encyclopedia with many different topics to choose from? This is a pretty valid and important use-case scenario for companies. They would have in-house documentation, or very specific documents, and would want their LLM (their own chatbots) to answer without hallucinating.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Author\n",
        "[Ümit Mert Çağlar](https://avesis.metu.edu.tr/mecaglar)\n",
        "\n"
      ],
      "metadata": {
        "id": "edFqlTF1qt4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computational Requirements\n",
        "<a id='comp_req'></a>\n",
        "In this notebook, we will be using lightweight computational resources, you do not need to employ any GPU or training of any sort to follow or complete this lab.\n",
        "\n",
        "Although we will be employing Large Language Models (LLM), we will be using API granted by [anthropic](https://www.anthropic.com/), specifically API for their foundational AI Model [Claude-3](https://www.anthropic.com/claude). If you want to try this yourself you can follow the steps from [here](https://console.anthropic.com/login?returnTo=%2F%3F), register with your e-mail, enter your phone number and earn your introductory 5 dollars for 14 days. Whole tutorial will cost like 10 cents so you can do whatever you like with the remaining credit balance."
      ],
      "metadata": {
        "id": "-vfqTc3HTCxI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-1: Single Document about QloRA to Provide Context"
      ],
      "metadata": {
        "id": "67RgxuUddaXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) Double Quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) Paged Optimizers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. We release all of our models and code, including CUDA kernels for 4-bit training."
      ],
      "metadata": {
        "id": "zX9t7k_TrRKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context"
      ],
      "metadata": {
        "id": "OlugR4KQdYBA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFftNqQxljS2",
        "outputId": "2c59629f-bb4d-4ec7-cc7a-5629382366cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-09 18:18:16--  https://arxiv.org/pdf/2305.14314.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.67.42, 151.101.131.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2305.14314 [following]\n",
            "--2024-05-09 18:18:16--  http://arxiv.org/pdf/2305.14314\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1065470 (1.0M) [application/pdf]\n",
            "Saving to: ‘/content/QLORA.pdf’\n",
            "\n",
            "\r/content/QLORA.pdf    0%[                    ]       0  --.-KB/s               \r/content/QLORA.pdf  100%[===================>]   1.02M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-05-09 18:18:16 (15.8 MB/s) - ‘/content/QLORA.pdf’ saved [1065470/1065470]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://arxiv.org/pdf/2305.14314.pdf\" -O /content/QLORA.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements\n",
        "Install the requirements with pip install."
      ],
      "metadata": {
        "id": "2urFsVsIhjNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch llama-index==0.10.20 transformers accelerate bitsandbytes pypdf chromadb==0.4.24 sentence-transformers pydantic==1.10.11 llama-index-embeddings-huggingface llama-index-llms-huggingface llama-index-readers-file llama-index-vector-stores-chroma llama-index-llms-anthropic --quiet"
      ],
      "metadata": {
        "id": "aquoJrS3lm9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "We will import the necessary imports here. Basically, we have the [torch](https://pytorch.org/) library, [llama index](https://www.llamaindex.ai/), [transformers](https://huggingface.co/docs/transformers/index) from Huggingface. You can read more about llama index and RAG from [here](https://docs.llamaindex.ai/en/stable/)."
      ],
      "metadata": {
        "id": "cM12n8AVZtr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "import chromadb\n",
        "from llama_index.core import VectorStoreIndex, download_loader, ServiceContext, Settings\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core.storage.storage_context import StorageContext\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.llms.anthropic import Anthropic\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
        "from llama_index.core import PromptTemplate\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from IPython.display import Markdown, display, HTML\n",
        "from pathlib import Path\n",
        "import os"
      ],
      "metadata": {
        "id": "PvSiiiJslrFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API Key\n",
        "Provide your API key here."
      ],
      "metadata": {
        "id": "al2Hz--ihqjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR API KEY HERE\""
      ],
      "metadata": {
        "id": "1WDGTUPKmMhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document\n",
        "Provide the document here, load the PDF for now"
      ],
      "metadata": {
        "id": "ThKrKtT5hutC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PDFReader()\n",
        "documents = loader.load_data(file=Path(\"/content/QLORA.pdf\"))"
      ],
      "metadata": {
        "id": "yjvNgi6NmS_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Large Language Model\n",
        "Here we setup the LLM, a claude-3 model, as described in the [introduction section](#intro_tutorial)."
      ],
      "metadata": {
        "id": "F9Et1Ocvh1lT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = Anthropic(\n",
        "    model=\"claude-3-sonnet-20240229\",\n",
        ")\n",
        "\n",
        "tokenizer = Anthropic().tokenizer\n",
        "Settings.tokenizer = tokenizer\n",
        "Settings.llm = llm\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "4BRJDVRcmUeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First response\n",
        "Here we will get our first response, when asked (queried) about QLORA, the LLM agent responds that it doesn't know the term. Good, but how can we \"teach\" this term? Do we require a training from scracth? Thankfully no, we can use this foundational model and RAG!"
      ],
      "metadata": {
        "id": "7H7FNC-oiPiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# resp contains the response\n",
        "resp = llm.complete(\"What is QLORA?\")\n",
        "\n",
        "# Using HTML with inline CSS for styling (gray color, smaller font size)\n",
        "html_text = f'<p style=\"color: #1f77b4; font-size: 14px;\"><b>{resp}</b></p>'\n",
        "display(HTML(html_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "79ThM4kOmXmF",
        "outputId": "b807685d-f737-4118-dd8a-af742e6e6f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: #1f77b4; font-size: 14px;\"><b>QLORA is not a commonly recognized acronym or term that I'm familiar with. Without more context, it's difficult for me to provide a definitive explanation of what QLORA means or refers to. Acronyms can have multiple meanings across different fields or contexts. Could you provide some additional details about where you encountered this term or what domain it relates to? That would help me try to determine the intended meaning of QLORA.</b></p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response: QLORA is not a commonly recognized acronym or term that I'm familiar with. Without more context, it's difficult for me to provide a definitive explanation of what QLORA means or refers to. Acronyms can have multiple meanings across different fields or contexts. Could you provide some additional details about where you encountered this term or what domain it relates to? That would help me try to determine the intended meaning of QLORA."
      ],
      "metadata": {
        "id": "RwtK-awZn7Wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Database\n",
        "\n",
        "We can observe from the following output that the model has no data on this topic, which is beneficial for us. This provides us with an opportunity to enhance its knowledge on the subject using RAG. We will now configure ChromaDB as our vector database and load the data from the paper we downloaded into it. Chroma is a vector embedding database that is open-source. When a query is made, it computes the feature vector of our prompt and retrieves the most relevant documents from the one we loaded into it using similarity search. This document can then be passed to the language model as context. ChromaDB can run within our Jupyter Notebook and has been installed, so there is no need to attach any external servers. Since we only use one document, Chroma will not have any difficulty determining which document to return, but you can experiment with loading additional documents to see how it affects the result."
      ],
      "metadata": {
        "id": "4Xaur17di74u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create client and a new collection\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "chroma_collection = chroma_client.create_collection(\"firstcollection\")\n",
        "\n",
        "# Load the embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Set up ChromaVectorStore and load in data\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "  documents, storage_context=storage_context, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWFeh4TmbOJ",
        "outputId": "fd105f88-556a-484e-b07a-03559f375e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2bf5a458efb0>:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define query\n",
        "query=\"what is QLORA?\"\n",
        "\n",
        "query_engine =index.as_query_engine(response_mode=\"compact\")\n",
        "response = query_engine.query(query)\n",
        "\n",
        "# Using HTML with inline CSS for styling (blue color)\n",
        "html_text = f'<p style=\"color: #1f77b4; font-size: 14px;\"><b>{response}</b></p>'\n",
        "display(HTML(html_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "8ynpuhpOmsuR",
        "outputId": "7db946e9-af8e-49c6-b399-1fc0fb8af27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: #1f77b4; font-size: 14px;\"><b>Based on the context provided, QLORA appears to be a technique or method used for finetuning large language models like OPT on various datasets and tasks. The context mentions using QLORA for finetuning OPT models of different sizes (7B, 13B, 33B, 65B) on datasets like Self-Instruct, Alpaca, Unnatural Instructions, Longform, and Chip2. It also provides hyperparameter details used for QLORA finetuning across different model sizes and datasets. However, the context does not explicitly define or describe what QLORA is in detail.</b></p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response: Based on the context provided, QLORA appears to be a technique or method used for finetuning large language models like OPT on various datasets and tasks. The context mentions using QLORA for finetuning OPT models of different sizes (7B, 13B, 33B, 65B) on datasets like Self-Instruct, Alpaca, Unnatural Instructions, Longform, and Chip2. It also provides hyperparameter details used for QLORA finetuning across different model sizes and datasets. However, the context does not explicitly define or describe what QLORA is in detail."
      ],
      "metadata": {
        "id": "7Uj4_85Un94y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Results\n",
        "We have successfully implemented RAG with our LLM based foundational model! We can use a powerful tool, and a context to answer a question, while staying within the context! Here are some additional questions and answers. Note that we can use this approach to extract many useful information from documents."
      ],
      "metadata": {
        "id": "SxdPDbvPjIKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What would be potential real world use cases for QLoRA?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kS246u-nWM-",
        "outputId": "11e0fa3f-977c-46f5-c2e2-fa147a1cd738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What would be potential real world use cases for QLoRA?\n",
            "Based on the context information provided, some potential real-world use cases for QLoRA (Quantized Low-Rank Adaptation) could be:\n",
            "\n",
            "1. Fine-tuning large language models for specific tasks or domains while requiring significantly less compute resources and memory compared to full model fine-tuning. This makes it feasible to personalize large models on consumer hardware like laptops or edge devices.\n",
            "\n",
            "2. Enabling on-device fine-tuning and personalization of language models for virtual assistants, chatbots, and other conversational AI applications on mobile devices or IoT products with limited compute capabilities.\n",
            "\n",
            "3. Allowing efficient model adaptation and knowledge injection for large language models in cloud/server environments, reducing costs associated with full fine-tuning at scale.\n",
            "\n",
            "4. Facilitating research and experimentation with large language model customization by making the process more accessible and resource-efficient.\n",
            "\n",
            "5. Enabling real-time or low-latency fine-tuning of language models for time-sensitive applications like live captioning, simultaneous translation, or dialogue systems that require rapid adaptation.\n",
            "\n",
            "The low-rank, quantized nature of QLoRA makes it particularly well-suited for scenarios where model size, memory footprint, and computational efficiency are important considerations alongside the need for task-specific fine-tuning of large language models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response:\n",
        "\n",
        "Querying with: What would be potential real world use cases for QLoRA?\n",
        "Based on the context information provided, some potential real-world use cases for QLoRA (Quantized Low-Rank Adaptation) could be:\n",
        "\n",
        "1. Fine-tuning large language models for specific tasks or domains while requiring significantly less compute resources and memory compared to full model fine-tuning. This makes it feasible to personalize large models on consumer hardware like laptops or edge devices.\n",
        "\n",
        "2. Enabling on-device fine-tuning and personalization of language models for virtual assistants, chatbots, and other conversational AI applications on mobile devices or IoT products with limited compute capabilities.\n",
        "\n",
        "3. Allowing efficient model adaptation and continuous learning for large language models in scenarios where data is arriving in a stream, such as for customer service chatbots or language translation systems.\n",
        "\n",
        "4. Facilitating privacy-preserving fine-tuning, where the base model remains unchanged, and only small LoRA weights need to be transmitted for personalization on the client-side.\n",
        "\n",
        "5. Enabling more efficient and scalable fine-tuning pipelines for large language model providers, reducing the computational costs associated with serving fine-tuned models for various downstream tasks.\n",
        "\n",
        "The low-rank, quantized nature of QLoRA makes it particularly well-suited for resource-constrained environments and applications requiring efficient model customization or continuous adaptation."
      ],
      "metadata": {
        "id": "1oSxeysmn2qq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find those key points from the text ourselves.\n",
        "\n",
        "\n",
        "1. Our QLORA finetuning method is the first method that enables the finetuning of 33B parameter\n",
        "models on a single consumer GPU and 65B parameter models on a single professional GPU, while\n",
        "not degrading performance relative to a full finetuning baseline.\n",
        "1. Another potential source of impact is deployment to mobile phones.\n",
        "1. Since instruction finetuning is an essential tool to transform raw pretrained LLMs into ChatGPT-like\n",
        "chatbots, we believe that our method will make finetuning widespread and common in particular for\n",
        "the researchers that have the least resources, a big win for the accessibility of state of the art NLP\n",
        "technology.\n",
        "1. QLORA\n",
        "can help enable privacy-preserving usage of LLMs, where users can own and manage their own data\n",
        "and models, while simultaneously making LLMs easier to deploy.\n",
        "\n",
        "The results are convincing!\n"
      ],
      "metadata": {
        "id": "hmbJjC0Jnzqy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gneHM6-Cnzw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"Explain QLoRA vs. Standard Finetuning\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8zrRXuvnfnV",
        "outputId": "a90740ca-caac-41fb-d4c5-1d9ba7441cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: Explain QLoRA vs. Standard Finetuning\n",
            "QLoRA (Quantized Low-Rank Adaptation) is a method that enables efficient finetuning of large language models by combining quantization and low-rank adaptation techniques. Unlike standard full finetuning, which updates all the parameters of the pre-trained model, QLoRA first quantizes the pre-trained model to a lower precision (e.g., 4-bit) and then only finetunes a small set of additional parameters, called adapters, during the finetuning process.\n",
            "\n",
            "The key advantages of QLoRA over standard finetuning are:\n",
            "\n",
            "1. Memory efficiency: By quantizing the pre-trained model to lower precision, QLoRA significantly reduces the memory footprint, allowing finetuning of much larger models on hardware with limited memory.\n",
            "\n",
            "2. Computational efficiency: Since QLoRA only updates the small adapter parameters during finetuning, it requires significantly fewer computational resources compared to full finetuning, which updates all model parameters.\n",
            "\n",
            "3. Matching performance: Despite the quantization and low-rank adaptation, QLoRA can match the performance of standard full-precision finetuning, as demonstrated in the provided results on various benchmarks.\n",
            "\n",
            "In summary, QLoRA is a memory and compute-efficient finetuning method that enables finetuning of massive language models on consumer-grade hardware, while maintaining the performance of standard full finetuning methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"Explain QLoRA Finetuning steps\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTZqPjR9p6Z7",
        "outputId": "85ee608c-dfd8-4420-9722-7b5bd9faffe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: Explain QLoRA Finetuning steps\n",
            "QLoRA (Quantized Low-Rank Adaptation) is a method for efficient finetuning of large language models. Here are the key steps involved:\n",
            "\n",
            "1. Start with a pre-trained large language model that has been quantized to lower precision (e.g. 4-bit) to reduce memory requirements.\n",
            "\n",
            "2. Add a small set of trainable parameters called LoRA (Low-Rank Adaptation) to the quantized model. These are low-rank matrices that can adapt the pre-trained model to a new task or domain.\n",
            "\n",
            "3. Finetune only the LoRA parameters on the target dataset, while keeping the quantized pre-trained model fixed. This requires much less memory and compute compared to full finetuning.\n",
            "\n",
            "4. During inference, combine the LoRA parameters with the quantized pre-trained model to obtain the finetuned model outputs.\n",
            "\n",
            "The key advantages of QLoRA are its high efficiency, enabling finetuning of very large models like 33B or 65B parameters on a single GPU, and its performance being on par with full precision finetuning despite using quantized models. It makes state-of-the-art language model finetuning much more accessible to researchers with limited resources.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"Explain Qualitative Analysis on QLoRA\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo7m30nsl-b0",
        "outputId": "dac124da-36ff-4bfb-ec59-31a89a25600d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: Explain Qualitative Analysis on QLoRA\n",
            "Based on the provided context, QLoRA (Quantized Low-Rank Adaptation) is a technique used for efficient finetuning of large language models. It involves adding low-rank matrices to the existing weights of the model during finetuning, rather than updating all weights. This allows for faster and more memory-efficient finetuning compared to standard finetuning methods.\n",
            "\n",
            "The context provides details on the hyperparameters used for QLoRA finetuning experiments across different model sizes (7B, 13B, 33B, 65B) and datasets like OASST1, HH-RLHF, Longform, and others. It mentions using techniques like LoRA dropout, tuning the LoRA rank (r), and other settings like learning rates and training steps for different model-dataset combinations.\n",
            "\n",
            "The qualitative analysis likely refers to evaluating the performance and behavior of models finetuned with QLoRA on various tasks and datasets, compared to standard finetuning or other baselines. This could involve metrics like perplexity, accuracy on benchmarks, qualitative human evaluations, and analyzing the outputs and capabilities of the finetuned models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What are the benchmark results for Guanaco\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AodmBPGqKzj",
        "outputId": "714afffe-5559-4893-8118-0ac3e58f5f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What are the benchmark results for Guanaco\n",
            "According to the context information provided, the Guanaco models achieved impressive results on various benchmarks:\n",
            "\n",
            "1. On the Vicuna benchmark, which favors open-source models, the Guanaco 33B and 65B models outperformed all other models except GPT-4. They performed comparably to ChatGPT.\n",
            "\n",
            "2. On the larger OA (Open Assistant) benchmark, the Guanaco 33B and 65B models also outperformed all models besides GPT-4 and performed similarly to ChatGPT.\n",
            "\n",
            "3. In the Elo rating competition, where GPT-4 judged the quality of responses on the Vicuna benchmark, the Guanaco 65B model achieved an Elo rating of 1022, and the Guanaco 33B model achieved an Elo rating of 992, ranking them as the second and third best models after GPT-4.\n",
            "\n",
            "4. The Guanaco 33B model achieved 97.8% of the performance level of ChatGPT on the Vicuna benchmark, while the Guanaco 65B model essentially closed the gap, reaching 99.3% of ChatGPT's performance.\n",
            "\n",
            "5. Even the smaller Guanaco 7B model, requiring only 5 GB of memory, outperformed the 26 GB Alpaca model by more than 20 percentage points on the Vicuna benchmark.\n",
            "\n",
            "Overall, the Guanaco models, especially the 33B and 65B versions, demonstrated state-of-the-art performance on various benchmarks, rivaling the performance of ChatGPT and surpassing other open-source models by a significant margin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response:\n",
        "Querying with: What are the benchmark results for Guanaco\n",
        "According to the context information provided, the Guanaco models achieved impressive results on various benchmarks:\n",
        "\n",
        "1. On the Vicuna benchmark, which favors open-source models, the Guanaco 33B and 65B models outperformed all other models except GPT-4. They performed comparably to ChatGPT.\n",
        "\n",
        "2. On the larger OA (Open Assistant) benchmark, the Guanaco 33B and 65B models also outperformed all models besides GPT-4 and performed similarly to ChatGPT.\n",
        "\n",
        "3. In the Elo rating competition, where GPT-4 judged the responses, the Guanaco 65B model achieved an Elo rating of 1022, and the Guanaco 33B model achieved an Elo rating of 992, ranking them as the second and third best models, respectively, after GPT-4.\n",
        "\n",
        "4. The Guanaco 13B model outperformed the Bard model in the Elo rating competition, with an Elo rating of 916 compared to Bard's 902.\n",
        "\n",
        "5. On the Vicuna benchmark, the smallest Guanaco model (7B parameters) outperformed the 26GB Alpaca model by more than 20 percentage points.\n",
        "\n",
        "Overall, the Guanaco models, especially the 33B and 65B versions, demonstrated state-of-the-art performance, rivaling ChatGPT on various benchmarks and outperforming other open-source models by a significant margin."
      ],
      "metadata": {
        "id": "-Jzmd1O5qqfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-2: Single Document about CDW-CE to Prevent Hallucination\n",
        "\n",
        "In this example, we will start with the following document, with the following abstract:\n",
        "\n",
        "In scoring systems used to measure the endoscopic activity of ulcerative colitis, such as Mayo endoscopic score or Ulcerative Colitis Endoscopic Index Severity, levels increase with severity of the disease activity. Such relative ranking among the scores makes it an ordinal regression problem. On the other hand, most studies use categorical cross-entropy loss function to train deep learning models, which is not optimal for the ordinal regression problem. In this study, we propose a novel loss function, class distance weighted cross-entropy (CDW-CE), that respects the order of the classes and takes the distance of the classes into account in calculation of the cost. Experimental evaluations show that models trained with CDW-CE outperform the models trained with conventional categorical cross-entropy and other commonly used loss functions which are designed for the ordinal regression problems. In addition, the class activation maps of models trained with CDW-CE loss are more class-discriminative and they are found to be more reasonable by the domain experts."
      ],
      "metadata": {
        "id": "5GIItA5jdngi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://arxiv.org/pdf/2202.05167\" -O /content/cdwce.pdf"
      ],
      "metadata": {
        "id": "XDuxCSPkqK2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf0e4d9-de15-460f-d3f0-0e3be3be0f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-09 18:24:39--  https://arxiv.org/pdf/2202.05167\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.195.42, 151.101.67.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.195.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5256318 (5.0M) [application/pdf]\n",
            "Saving to: ‘/content/cdwce.pdf’\n",
            "\n",
            "\r/content/cdwce.pdf    0%[                    ]       0  --.-KB/s               \r/content/cdwce.pdf   87%[================>   ]   4.39M  22.0MB/s               \r/content/cdwce.pdf  100%[===================>]   5.01M  24.6MB/s    in 0.2s    \n",
            "\n",
            "2024-05-09 18:24:39 (24.6 MB/s) - ‘/content/cdwce.pdf’ saved [5256318/5256318]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again we will load our PDF reader, initiate our LLM, and ask potential use case scenarios for CDW-CE."
      ],
      "metadata": {
        "id": "z0k_mi0VjvrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PDFReader()\n",
        "documents_cdw_ce = loader.load_data(file=Path(\"/content/cdwce.pdf\"))"
      ],
      "metadata": {
        "id": "iB0A3CR_tKRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm2 = Anthropic(\n",
        "    model=\"claude-3-sonnet-20240229\",\n",
        ")\n",
        "\n",
        "tokenizer = Anthropic().tokenizer\n",
        "Settings.tokenizer = tokenizer\n",
        "Settings.llm = llm2\n",
        "Settings.chunk_size = 1024"
      ],
      "metadata": {
        "id": "DGnMMIQHtKUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resp contains the response\n",
        "resp = llm2.complete(\"What would be potential real world use cases for CDW-CE?\")\n",
        "print(resp)"
      ],
      "metadata": {
        "id": "HkZQyvT_qK5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b06564-9771-4629-b29a-b82033b3749a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CDW-CE, or Contrastive Divergence with Data Augmentation and Consistency Encoding, is a machine learning technique used for training generative models, particularly energy-based models (EBMs) and implicit generative models. Some potential real-world use cases for CDW-CE include:\n",
            "\n",
            "1. Image generation and manipulation: CDW-CE can be used to train generative models for creating realistic synthetic images or manipulating existing images. This has applications in computer vision, graphics, and multimedia domains, such as image editing, style transfer, and data augmentation for training computer vision models.\n",
            "\n",
            "2. Anomaly detection: Generative models trained with CDW-CE can learn the underlying distribution of normal data, making them useful for detecting anomalies or outliers in various domains, such as fraud detection, manufacturing defect detection, and medical imaging analysis.\n",
            "\n",
            "3. Data imputation and denoising: CDW-CE can be used to train generative models for imputing missing data or denoising corrupted data. This can be useful in domains where data is incomplete or noisy, such as sensor data analysis, image restoration, and speech enhancement.\n",
            "\n",
            "4. Representation learning: Generative models trained with CDW-CE can learn rich representations of data, which can be useful for downstream tasks such as classification, clustering, or transfer learning.\n",
            "\n",
            "5. Simulation and data augmentation: Generative models trained with CDW-CE can be used to generate synthetic data for simulations or data augmentation, which can be particularly useful in domains where real-world data is scarce or expensive to collect, such as robotics, autonomous systems, and medical applications.\n",
            "\n",
            "6. Unsupervised learning and density estimation: CDW-CE can be used to train generative models for unsupervised learning tasks, such as density estimation, clustering, and dimensionality reduction, which can provide insights into the underlying structure of data in various domains.\n",
            "\n",
            "It's important to note that the specific applications of CDW-CE may depend on the type of generative model being trained and the domain in which it is being applied. Additionally, the performance and effectiveness of CDW-CE may vary depending on the complexity of the data and the specific problem being addressed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is hallucination!\n",
        "All of this information, is based upon hallucination so there is no possible way of determining where it originated, how the LLM answered or such. How can we prevent this from happening? We can use RAG!"
      ],
      "metadata": {
        "id": "nehau0yjFlpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create client and a new collection\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "chroma_collection = chroma_client.create_collection(\"secondcollection\")\n",
        "\n",
        "# Load the embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Set up ChromaVectorStore and load in data\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
        "index_cdw_ce = VectorStoreIndex.from_documents(\n",
        "  documents_cdw_ce, storage_context=storage_context, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "id": "jTl7Xx-ntlAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e256ee-9907-4c52-a31a-f5463ebdfd97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-13fe4f387dc5>:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We again use the ChromaDB, to create a vector DB of our document (CDW-CE paper). and as"
      ],
      "metadata": {
        "id": "Jogl60uOkPrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_cdw_ce.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What would be potential real world use cases for CDW-CE?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "aKs5AWHPtlGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2757e2f-ac85-4830-f4ec-3b5ea34fa2ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What would be potential real world use cases for CDW-CE?\n",
            "Potential real-world use cases for the Class Distance Weighted Cross-Entropy Loss (CDW-CE) method could include:\n",
            "\n",
            "1. Medical image analysis and disease diagnosis - As demonstrated in the context, CDW-CE can improve the performance of convolutional neural networks in classifying medical images like endoscopy images for assessing ulcerative colitis severity. It provides better explainability through class activation maps highlighting more relevant and discriminative regions related to the disease.\n",
            "\n",
            "2. Any ordinal classification task where there is a natural ordering between classes and misclassifications to distant classes should be penalized more heavily. Examples could include age estimation, product rating prediction, document grading etc.\n",
            "\n",
            "3. Applications requiring better interpretability of model decisions, as CDW-CE encourages the model to focus on more relevant features/regions compared to standard cross-entropy loss.\n",
            "\n",
            "4. Scenarios where there is an imbalance in the cost/impact of different types of misclassification errors based on how far they are from the true class. CDW-CE can help mitigate this by applying a distance-based penalty.\n",
            "\n",
            "The key advantages of CDW-CE are its ability to improve performance on ordinal classification tasks while providing better explainability, without enforcing fixed distances between classes or any parametric distribution assumptions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_cdw_ce.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"How does CDW-CE perform compared to other loss functions?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Br0_B0m5i8",
        "outputId": "5be362cf-d29f-4673-db72-08815d237591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: How does CDW-CE perform compared to other loss functions?\n",
            "According to the experimental results presented in the context, the proposed Class Distance Weighted Cross-Entropy (CDW-CE) loss function outperforms other ordinal loss approaches like Cross-Entropy (CE), Squared Ordinal Regression (CORN), and Hierarchical Ordinal Regression (HO2). \n",
            "\n",
            "Specifically, CDW-CE achieves the highest performance scores across different evaluation metrics and CNN models for the remission classification task. It significantly reduces mispredictions that are farther away (two or more class distances) from the true class compared to CE. While the sensitivity for edge classes may remain similar, CDW-CE notably increases the sensitivity for intermediate classes by centering wrong estimates closer to the true class due to the higher penalty given to distant mispredictions.\n",
            "\n",
            "The context also mentions that CDW-CE provides better explainability through Class Activation Map (CAM) visualizations, highlighting more relevant and discriminative areas related to the disease compared to CE. Overall, the proposed CDW-CE loss function demonstrates improved optimization and performance over previous ordinal loss approaches in the experiments described.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_cdw_ce.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"Which loss function is the best performing loss function in terms of QWK and F1 Score?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsxK7PcHm5mH",
        "outputId": "debb0c3d-0153-4e56-f21a-8fd11402348b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: Which loss function is the best performing loss function in terms of QWK and F1 Score?\n",
            "Based on the results presented, the Class Distance Weighted Cross-Entropy (CDW-CE) loss function performs the best in terms of Quadratic Weighted Kappa (QWK) and F1 scores across the different models evaluated - ResNet18, Inception-v3, and MobileNet-v3-Large. The CDW-CE loss outperforms the standard cross-entropy (CE) loss as well as other ordinal loss functions like CORN, HO2, and CO2, achieving the highest QWK and F1 scores for all three model architectures. The visualizations in Figure 2 also show that CDW-CE provides more stable performance across different values of the hyperparameter α compared to CE loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_cdw_ce.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"Which loss function has best explainability?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsaox0sMm5pC",
        "outputId": "e428f0aa-80b0-49d0-bcd0-790780724d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: Which loss function has best explainability?\n",
            "Based on the experimental results described in the context, the proposed Class Distance Weighted Cross-Entropy (CDW-CE) loss function provides better explainability compared to the standard cross-entropy loss. Specifically, the context mentions that training models with the CDW-CE loss results in Class Activation Map (CAM) visualizations that highlight more relevant and discriminative regions for the decision-making process. This improved explainability is attributed to the way CDW-CE penalizes mispredictions according to their distance from the true class, encouraging the model to learn more meaningful representations. Therefore, the CDW-CE loss function has the best explainability according to the information provided.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html_text = f'<p style=\"color: #1f77b4; font-size: 14px;\"><b>{response}</b></p>'\n",
        "display(HTML(html_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "4jlJUSnMn0Do",
        "outputId": "6d90aa39-4e2c-434e-8512-537b0ba32807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: #1f77b4; font-size: 14px;\"><b>Based on the experimental results described in the context, the proposed Class Distance Weighted Cross-Entropy (CDW-CE) loss function provides better explainability compared to the standard cross-entropy loss. Specifically, the context mentions that training models with the CDW-CE loss results in Class Activation Map (CAM) visualizations that highlight more relevant and discriminative regions for the decision-making process. This improved explainability is attributed to the way CDW-CE penalizes mispredictions according to their distance from the true class, encouraging the model to learn more meaningful representations. Therefore, the CDW-CE loss function has the best explainability according to the information provided.</b></p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_cdw_ce.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What is QLORA?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUsc22LOn44J",
        "outputId": "0e1a1086-abc4-4c81-dab7-8048e8d74b1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What is QLORA?\n",
            "Unfortunately, there is no mention of \"QLORA\" in the given context information. The context appears to be discussing deep learning models, loss functions, and evaluation metrics for analyzing medical images related to ulcerative colitis. Without any relevant information provided, I cannot determine what \"QLORA\" refers to.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html_text = f'<p style=\"color: #1f77b4; font-size: 14px;\"><b>{response}</b></p>'\n",
        "display(HTML(html_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "WYduVcb0oAE7",
        "outputId": "42e91315-4566-47c9-ecc5-33a69b97402f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: #1f77b4; font-size: 14px;\"><b>Unfortunately, there is no mention of \"QLORA\" in the given context information. The context appears to be discussing deep learning models, loss functions, and evaluation metrics for analyzing medical images related to ulcerative colitis. Without any relevant information provided, I cannot determine what \"QLORA\" refers to.</b></p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-3: Multiple Documents and Letting LLM to Choose"
      ],
      "metadata": {
        "id": "xny5HQz6nzMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/caglarmert/DI725/blob/b056f9f05e612942689031655d92dc21f802f28c/METU_Regulations.pdf?raw=true\" -O /content/gda.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVYWxSFDoxE3",
        "outputId": "005e8261-70f4-4057-bf74-257f763de3d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-09 18:55:56--  https://github.com/caglarmert/DI725/blob/b056f9f05e612942689031655d92dc21f802f28c/METU_Regulations.pdf?raw=true\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/caglarmert/DI725/raw/b056f9f05e612942689031655d92dc21f802f28c/METU_Regulations.pdf [following]\n",
            "--2024-05-09 18:55:57--  https://github.com/caglarmert/DI725/raw/b056f9f05e612942689031655d92dc21f802f28c/METU_Regulations.pdf\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/caglarmert/DI725/b056f9f05e612942689031655d92dc21f802f28c/METU_Regulations.pdf [following]\n",
            "--2024-05-09 18:55:57--  https://raw.githubusercontent.com/caglarmert/DI725/b056f9f05e612942689031655d92dc21f802f28c/METU_Regulations.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 343362 (335K) [application/octet-stream]\n",
            "Saving to: ‘/content/gda.pdf’\n",
            "\n",
            "/content/gda.pdf    100%[===================>] 335.31K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-05-09 18:55:57 (2.68 MB/s) - ‘/content/gda.pdf’ saved [343362/343362]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PDFReader()\n",
        "documents_gda = loader.load_data(file=Path(\"/content/gda.pdf\"))\n",
        "\n",
        "llm3 = Anthropic(\n",
        "    model=\"claude-3-sonnet-20240229\",\n",
        ")\n",
        "\n",
        "tokenizer = Anthropic().tokenizer\n",
        "Settings.tokenizer = tokenizer\n",
        "Settings.llm = llm3\n",
        "Settings.chunk_size = 1024\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PB55VFyLdyjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resp contains the response\n",
        "resp = llm3.complete(\"What is the date of Higher Education Act 2547?\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJC5PlINdyuZ",
        "outputId": "77414f28-0059-487f-d428-5e8d5fd56275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfortunately, I could not find any specific information about a \"Higher Education Act 2547\". The year 2547 seems to refer to the Buddhist Era calendar, which is widely used in several Southeast Asian countries like Thailand.\n",
            "\n",
            "Without more context about which country this act is from, it's difficult to pinpoint the exact date corresponding to the year 2547 BE on the Gregorian calendar that is commonly used internationally.\n",
            "\n",
            "If you could provide some more details about which country's legislation this refers to, I may be able to determine the equivalent Gregorian calendar year for 2547 BE. Thailand, for example, is currently in the year 2566 BE, which corresponds to 2023 CE on the Gregorian calendar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resp contains the response\n",
        "resp = llm3.complete(\"I want to improve my grade from a course I took 5 semesters ago, can I take the course now?\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyMVdSJKtwEx",
        "outputId": "143fa24e-e13b-4a0c-bccc-0c88d97af1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ability to retake a course to improve your grade usually depends on the policies of the specific college or university. Here are a few common scenarios:\n",
            "\n",
            "- Many schools allow students to retake courses in which they received a low grade (C or below). The new grade replaces the old grade in calculating your GPA, though the original grade may still appear on your transcript.\n",
            "\n",
            "- Some schools have time limits on retaking courses for grade replacement, such as within 1-2 years after originally taking the course.\n",
            "\n",
            "- Other schools may average the new grade with the old grade instead of replacing it entirely.\n",
            "\n",
            "- There are usually restrictions on how many courses or credits can be retaken for grade replacement purposes.\n",
            "\n",
            "- Retaking a course after graduating is typically not allowed for the purpose of improving your final degree GPA.\n",
            "\n",
            "The best thing to do is to check with your school's registrar or academic advising office about their specific course repeat/grade replacement policy. They can let you know if retaking a course from 5 semesters ago is permitted to improve your grade based on the university's rules. Getting approval ahead of time is recommended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resp contains the response\n",
        "resp = llm3.complete(\"I want to apply to a graduate school in METU, which exam shall I take?\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JSDgP5couKv",
        "outputId": "e76a4545-4240-46b8-af61-0fc4a2d83034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To apply for graduate programs at Middle East Technical University (METU) in Turkey, you will typically need to take one of the following standardized exams, depending on the program you are applying to:\n",
            "\n",
            "1. GRE (Graduate Record Examination): The GRE is a widely accepted exam for admission to many graduate programs, including those at METU. It tests your verbal reasoning, quantitative reasoning, and analytical writing skills.\n",
            "\n",
            "2. ALES (Academic Personnel and Postgraduate Education Entrance Exam): ALES is a national exam administered by the Student Selection and Placement Center (ÖSYM) in Turkey. It is required for admission to many graduate programs in Turkish universities, including METU.\n",
            "\n",
            "3. Subject-specific exams: Some graduate programs at METU may require you to take a subject-specific exam, such as the GMAT for business-related programs or the GRE Subject Tests for specific fields of study.\n",
            "\n",
            "It's important to check the specific requirements of the graduate program you are interested in at METU, as they may have different exam preferences or additional requirements. You can find this information on the METU website or by contacting the relevant department or graduate school office.\n",
            "\n",
            "Additionally, if you are an international applicant, you may also need to provide proof of English language proficiency, such as TOEFL or IELTS scores, unless you have completed your previous degree in an English-medium program.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resp contains the response\n",
        "resp = llm3.complete(\"What is the minimum CGPA requirement to take DCA in METU?\")\n",
        "print(resp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgJx2uEzuih7",
        "outputId": "491b1f2f-2053-4052-de8e-519e853bde30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unfortunately, I do not have specific information about the minimum CGPA (Cumulative Grade Point Average) requirement to take the DCA (Department of Computer Applications) program at METU (Middle East Technical University) in Turkey.\n",
            "\n",
            "The CGPA requirements can vary between universities and even between different programs within the same university. They are usually set by the individual institution based on factors such as program competitiveness, available seats, and their own academic standards.\n",
            "\n",
            "To get the most accurate and up-to-date information about the CGPA cutoff for the DCA program at METU, I would recommend checking the university's official website, contacting their admissions office directly, or speaking with an academic advisor there. They would have the authoritative details on the specific CGPA criteria needed to be eligible for that particular program.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create client and a new collection\n",
        "chroma_client = chromadb.EphemeralClient()\n",
        "chroma_collection = chroma_client.create_collection(\"thirdcollection\")\n",
        "\n",
        "# Load the embedding model\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Set up ChromaVectorStore and load in data\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
        "index_gan = VectorStoreIndex.from_documents(\n",
        "  documents_gda, storage_context=storage_context, service_context=service_context\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iE8KpMzdypQ",
        "outputId": "44bfc345-15a7-4f9d-ea47-ed8b4505a40d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-24012c7798a7>:11: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What is the date of Higher Education Act 2547?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvqTsz2vdyr_",
        "outputId": "8a389d62-1be9-4361-d337-32aeec1cfa8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What is the date of Higher Education Act 2547?\n",
            "The Higher Education Act 2547 is dated November 4, 1981.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What is the maximum duration of education for an undergraduate student at METU?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxbVfPEpdyw9",
        "outputId": "6ba147fe-e297-4dc2-ddd9-07ec40151865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What is the maximum duration of education for an undergraduate student at METU?\n",
            "According to Article 6, the maximum duration of an undergraduate program at METU is seven years (fourteen semesters). For programs offering a master's degree along with an undergraduate degree, the maximum duration is nine years (sixteen semesters).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What is the maximum duration of education for a master's student at METU?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNPPzlLcwAlL",
        "outputId": "4d6f93a4-3ea7-493e-b8ae-09254dc747df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What is the maximum duration of education for a master's student at METU?\n",
            "According to Article 31, the maximum duration for a Master's program with a thesis at METU is six semesters. This does not include any time spent in the Academic Deficiency Program.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What is the maximum duration of education for a doctoral student at METU?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBOxQNnRwOsg",
        "outputId": "b8eb0642-9e36-49e3-938f-fcd76d1096bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What is the maximum duration of education for a doctoral student at METU?\n",
            "According to Article 39, the maximum duration of a Ph.D. program at METU is twelve academic semesters. For Ph.D. on Bachelor's degree programs, the maximum duration is fourteen academic semesters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"Can I withrdaw from a course as an undergraduate student?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vStDaySJwUKU",
        "outputId": "d0b8f62b-1b2d-4ad3-9d9b-8021ef492df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: Can I withrdaw from a course as an undergraduate student?\n",
            "Yes, as an undergraduate student at METU, you can withdraw from a course. The context information does not explicitly mention course withdrawal, but it does outline the registration process and rules around taking courses with an \"NI\" (not included) status. Specifically, Article 19(d) states that \"The status of courses falling into the NI status cannot be altered after the registration process of the concerned semester is completed.\" This implies that during the registration period, you can likely withdraw from or drop a course before it enters the \"NI\" status, after which the status cannot be changed for that semester.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What is the regulation regarding course withdrawal for graduate students?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKqmabB3wUNK",
        "outputId": "48401de6-8e68-4fef-d5cb-9bfca24e9a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What is the regulation regarding course withdrawal for graduate students?\n",
            "According to the provided context information, the regulations regarding course withdrawal for graduate students are as follows:\n",
            "\n",
            "b) Course withdrawals are processed and advisor approvals are given online.\n",
            "c) An advisor approval is required following a one-on-one meeting with the advisor for course withdrawal.  \n",
            "e) Course withdrawal may be processed for only one course in a semester.\n",
            "f) Course withdrawal may be processed for a maximum of six courses throughout the duration of education.\n",
            "g) Course withdrawal is not possible in the first two semesters of the curriculum.\n",
            "h) Course withdrawal is not possible for repeated courses, previously withdrawn courses, courses in NI (Not Included) status, or non-credit courses.\n",
            "i) Course withdrawal is not allowed for students taking the minimum course load or below in a semester.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"What are scores and coefficents corresponding to letter grade AA and CC?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Qb9QA_wUP5",
        "outputId": "11d06329-5919-4d2b-b5f4-731a093adf66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: What are scores and coefficents corresponding to letter grade AA and CC?\n",
            "According to the context information provided:\n",
            "\n",
            "For the letter grade AA:\n",
            "- The score interval is 90-100\n",
            "- The coefficient is 4.00\n",
            "\n",
            "For the letter grade CC: \n",
            "- The score interval is 70-74\n",
            "- The coefficient is 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"I am a graduate student, I have a CGPA of 2.6 will I be dismissed?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYj2mhhmwUR6",
        "outputId": "47dfd985-1ad7-4c70-a8be-48274d6e5e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: I am a graduate student, I have a CGPA of 2.6 will I be dismissed?\n",
            "Yes, based on the information provided in the context, if you are a graduate student with a Cumulative Grade Point Average (CGPA) below 3.00, you will be dismissed from your graduate program.\n",
            "\n",
            "Specifically, for Master's programs, Article 33 (5) states: \"Students who cannot successfully complete the courses (credit courses and seminar course) specified by the concerned GSD in a maximum of two academic years (four semesters), and/or students whose Cumulative Grade Point Average is below 3.00 are dismissed from their graduate program.\"\n",
            "\n",
            "Similarly, for Ph.D. programs, Article 41 (6) mentions: \"Students who cannot successfully complete the courses (credit courses and seminar course) specified by the concerned GSD within four academic semesters in Ph.D. programs, and in six academic semesters in Ph.D. on Bachelor's degree programs, and/or students whose Cumulative Grade Point Average is below 3.00 may not sit the doctoral comprehensive examination, and are dismissed from the programs they are enrolled in.\"\n",
            "\n",
            "Since your CGPA is 2.6, which is below the required 3.00, you will be dismissed from your graduate program according to the regulations outlined in the provided context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_engine = index_gan.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
        "response = chat_engine.chat(\"I am an undergraduate student, I have a CGPA of 2.6 will I be considered successful?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTfnhqfJxnIw",
        "outputId": "07f52ae2-2104-42f6-f88b-d1e0275f6691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Querying with: I am an undergraduate student, I have a CGPA of 2.6 will I be considered successful?\n",
            "Yes, based on the information provided in the context, if you are an undergraduate student with a Cumulative Grade Point Average (CGPA) of 2.6, you will be considered successful. According to Article 31, one of the requirements for graduation from an undergraduate program is to have a CGPA of at least 2.00. Since your CGPA of 2.6 exceeds this minimum requirement, you will be considered a successful student.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to these questions and answers, with multiple documents, our RAG approach can choose between them according to the relevance of the vectors of the document and the query. This enables us to combine larger knowledge databases, obtain the vectorized database version and retrieve the useful information. We have asked the same question for undergraduate and graduate students, with different answers from the regulations, and the LLM that is not specifically trained on this topic successfully answered these questions!"
      ],
      "metadata": {
        "id": "xruhRjYsx4la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "In this lab we have seen how RAG operates, we have observed how we can enable a foundational model (LLM), that is trained on closed datasets, with enormous amount of data, time and computation power, to adapt to our needs. Specifically, we have observed how we can let the LLM answer in-context provided, prevented hallucination and let RAG choose from different documents."
      ],
      "metadata": {
        "id": "p7B4OfbHx3FW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "1. [A documentation about RAG](https://docs.llamaindex.ai/en/stable/getting_started/concepts/)\n",
        "1. [A blog about in-context learning](https://www.lakera.ai/blog/what-is-in-context-learning)\n",
        "1. [A about LLM](https://app.datacamp.com/learn/courses/introduction-to-llms-in-python)\n",
        "1. [Another course about LLM applications with LangChain](https://app.datacamp.com/learn/courses/developing-llm-applications-with-langchain)\n",
        "1. [A video tutorial about RAG](https://www.youtube.com/watch?v=sVcwVQRHIc8)\n",
        "1. [A tutorial about RAG implementation with open source LLM](https://learnbybuilding.ai/tutorials/rag-from-scratch)\n",
        "1. [Another tutorial about RAG implementation](https://blog.risingstack.com/retrieval-augmented-generation-tutorial-google-colab/)"
      ],
      "metadata": {
        "id": "860812gKx3H2"
      }
    }
  ]
}