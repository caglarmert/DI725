{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36bca1a2",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/caglarmert/DI725/blob/main/DI725_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745686a-405f-4729-a239-ffd17f6c5087",
   "metadata": {
    "id": "3745686a-405f-4729-a239-ffd17f6c5087"
   },
   "source": [
    "# DI 725: Transformers and Attention-Based Deep Networks\n",
    "\n",
    "## A Tutorial for Implementing Transformers in PyTorch\n",
    "\n",
    "The purpose of this notebook is to introduce the transformers architecture, building different types of transformers and its adaptations to various tasks.\n",
    "\n",
    "In this notebook, there will be three different tasks, suitable to demonstrate Encoder-Transformer, Decoder-Transformer and Encoder-Decoder Transformer architectures.\n",
    "\n",
    "### Author:\n",
    "* Ümit Mert Çağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f81a53-355d-456d-90d1-9b5432dd1ce7",
   "metadata": {
    "id": "c0f81a53-355d-456d-90d1-9b5432dd1ce7"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5125f78-dea0-4c0e-ab42-5978b7b2633e",
   "metadata": {
    "id": "c5125f78-dea0-4c0e-ab42-5978b7b2633e"
   },
   "source": [
    "## Attention Is All You Need\n",
    "\n",
    "The paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), by Vaswani et al. changed the deep learning scene, starting with the NLP tasks, spanning almost all other aspects of deep learning studies, including vision and time series tasks. The emphasis of this paper is on attention and the carefully designed well-known and proven components. We will begin by examining the figure (provided by the author) and understand every component in the so-called transformer architecture.\n",
    "\n",
    "---\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/attention_research_1.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "Just by observing the architecture we can spot some important aspects of the Transformer architecture.\n",
    "* First, we have [embeddings](https://arxiv.org/abs/1608.05859) at the input and\n",
    "output, these are required in many tasks, to transform data into high dimensional vectors.\n",
    "* Second, we have [Positional Encoding](https://arxiv.org/pdf/1705.03122.pdf), which we require for the Transformer model to understand and relate the relative position of input and output tokens or embeddings.\n",
    "* On the left-hand side, we have the Encoder structure, which is a stacked network (depicted with Nx), that has the subcomponents of a Multi-Head Attention and a Feed Forward Network.\n",
    "* The attention mechanism, has Query, Key and Value (Q K V) as inputs, and all three of them are fed into the Multi-Head Attention block. This mechanism is the building pillar of Transformers, also highlighted by the authors:\n",
    " * *In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.*\n",
    " * *An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.*\n",
    "* We can also observe the Residual Connections and Layer Normalization block applied right after the attention and feed forward blocks. [Residual Connections](https://arxiv.org/abs/1512.03385) is an important factor that enables gradient flow in deeper networks. And [Layer Normalization](https://arxiv.org/abs/1607.06450) helps with the model training. Also the [Dropout](https://jmlr.org/papers/v15/srivastava14a.html) mechanism is applied for all sub-layers and it helps with the training.\n",
    "* Similarly on the right-hand side, we have another stacked architecture, but this time it is in the form of the Decoder structure.\n",
    "* The first attention mechanism we observe is ***Masked*** Multi-Head Self-Attention. It is masked to make sure that model only observes and attends to the previous tokens or embeddings.\n",
    "* The second attention mechanism in the Decoder architecture is Encoder-Decoder attention, or cross-attention layer. The keys and values come from the output of the Encoder stack while queries come from the first self-attention layer of the Decoder stack. With this cross-attention, decoder can attend over all positions in the input.\n",
    "* The Feed Forward layers, that are present in both Encoder and Decoder stacks, are defined by two linear layers and a ReLU activation function.\n",
    "* Finally, after all of the mechanisms and calculations, the generator that generates output probabilities is modeled by a linear layer and a softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bx_lEEFJI5N",
   "metadata": {
    "id": "9bx_lEEFJI5N"
   },
   "source": [
    "## Imports\n",
    "In this part we import the required libraries. This part might be required to be operational on the Colab servers for later parts. It is advised to check the associated python requirements.txt, that is frozen at the time of preparation of this notebook, in case of any library or version error occurs while running this notebook. Mind that installing everything locally via pip install -r \"requirements.txt\" is not advised though, mainly because of the discrepancies between Colab and locally available machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70a9b2e0-fdb5-4697-a48f-44d1a08503f8",
   "metadata": {
    "id": "70a9b2e0-fdb5-4697-a48f-44d1a08503f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QMDfs6xsJwj3",
   "metadata": {
    "id": "QMDfs6xsJwj3"
   },
   "source": [
    "The [torch](https://pytorch.org/) is a popular and diverse machine learning framework, enabling low level implementation (as low as it gets with Python anyway). The Neural Networks (nn) is a library within PyTorch that enables operations with neural network structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cce3f17-cf16-4b8e-be4c-769adba52eb2",
   "metadata": {
    "id": "1cce3f17-cf16-4b8e-be4c-769adba52eb2"
   },
   "source": [
    "# Building a Transformer Architecture in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UXVW1LslAu9y",
   "metadata": {
    "id": "UXVW1LslAu9y"
   },
   "source": [
    "## Building Blocks\n",
    "Here we will try to build some building blocks that we will use in further parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cCKhGMTXx-ss",
   "metadata": {
    "id": "cCKhGMTXx-ss"
   },
   "source": [
    "### Part 1: Positional Encoding\n",
    "\n",
    "Building the positional encoding can be observed from the implementation provided below.\n",
    "\n",
    "#### Instructions\n",
    "* Specify the PyTorch class that the positional encoder should subclass from.\n",
    "* Initialize a positional encoding matrix for token positions in sequences up to max_length.\n",
    "* Assign unique position encodings to the matrix pe by alternating the use of sine and cosine functions.\n",
    "* Update the input embeddings tensor x to add position information about the sequence using the positional encodings matrix.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The Positional Encoding component is highlighted in the figure below, indicating its use, both in the Encoder, and the Decoder.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/PE_highlight.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Xl4O79RK_bW",
   "metadata": {
    "id": "6Xl4O79RK_bW"
   },
   "source": [
    "In the __init__ method, we first initialize the superclass nn.Module and then define the model's dimension d_model and the maximum sequence length max_length. We then create a zero matrix pe of size max_length by d_model to store the positional encodings.\n",
    "\n",
    "Next, we calculate the positional encodings. We create a tensor position that contains the sequence positions and a tensor div_term that contains the division terms. The division terms are calculated using a formula that involves the natural logarithm of 10000 and the model's dimension. We then calculate the positional encodings by applying the sine function to the product of position and div_term for even indices and the cosine function for odd indices. The calculated positional encodings are then stored in the pe matrix.\n",
    "\n",
    "In the forward method, we add the positional encodings to the input embeddings tensor x. We slice the pe matrix to match the size of x before adding. The updated tensor x is then returned.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/positional_encoding.PNG?raw=true\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZFzC4clPGsP_",
   "metadata": {
    "id": "ZFzC4clPGsP_"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding module. Inject some information about the relative or\n",
    "    absolute position of the tokens in the sequence.The positional encodings\n",
    "    have the same dimension as the embeddings, so that the two can be summed.\n",
    "    Here, we use sine and cosine functions of different frequencies.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        dropout (float, optional): Dropout probability (default: 0.1).\n",
    "        max_len (int, optional): Maximum sequence length (default: 5000).\n",
    "\n",
    "    Attributes:\n",
    "        dropout (torch.nn.Dropout): Dropout layer.\n",
    "        pe (torch.Tensor): Positional encoding tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = ## TODO the formula is given above, fill the code\n",
    "        pe[:, 1::2] = ## TODO\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the positional encoding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after adding positional encoding and applying dropout.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uKcRi-gdE8wf",
   "metadata": {
    "id": "uKcRi-gdE8wf"
   },
   "source": [
    "To demonstrate the operation of Positional Encoding, we can use the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZKulvnac6xRb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "ZKulvnac6xRb",
    "outputId": "8765eb19-f17f-4484-f5c5-c72c84417dea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-19e007d77bab43208e5b4c7b2783a128\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-19e007d77bab43208e5b4c7b2783a128\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-19e007d77bab43208e5b4c7b2783a128\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-eafc635ddfa311c913cbcc2d2bc4477d\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"dimension\", \"type\": \"nominal\"}, \"x\": {\"field\": \"position\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"embedding\", \"type\": \"quantitative\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-eafc635ddfa311c913cbcc2d2bc4477d\": [{\"embedding\": 0.0, \"dimension\": 4, \"position\": 0}, {\"embedding\": 0.15782663226127625, \"dimension\": 4, \"position\": 1}, {\"embedding\": 0.3116971552371979, \"dimension\": 4, \"position\": 2}, {\"embedding\": 0.45775455236434937, \"dimension\": 4, \"position\": 3}, {\"embedding\": 0.5923377275466919, \"dimension\": 4, \"position\": 4}, {\"embedding\": 0.7120732069015503, \"dimension\": 4, \"position\": 5}, {\"embedding\": 0.813959538936615, \"dimension\": 4, \"position\": 6}, {\"embedding\": 0.8954429626464844, \"dimension\": 4, \"position\": 7}, {\"embedding\": 0.9544808864593506, \"dimension\": 4, \"position\": 8}, {\"embedding\": 0.989593505859375, \"dimension\": 4, \"position\": 9}, {\"embedding\": 0.9999006390571594, \"dimension\": 4, \"position\": 10}, {\"embedding\": 0.9851439595222473, \"dimension\": 4, \"position\": 11}, {\"embedding\": 0.94569331407547, \"dimension\": 4, \"position\": 12}, {\"embedding\": 0.8825376033782959, \"dimension\": 4, \"position\": 13}, {\"embedding\": 0.7972599267959595, \"dimension\": 4, \"position\": 14}, {\"embedding\": 0.6919978260993958, \"dimension\": 4, \"position\": 15}, {\"embedding\": 0.5693899393081665, \"dimension\": 4, \"position\": 16}, {\"embedding\": 0.4325096309185028, \"dimension\": 4, \"position\": 17}, {\"embedding\": 0.284787654876709, \"dimension\": 4, \"position\": 18}, {\"embedding\": 0.12992730736732483, \"dimension\": 4, \"position\": 19}, {\"embedding\": -0.028190065175294876, \"dimension\": 4, \"position\": 20}, {\"embedding\": -0.18560057878494263, \"dimension\": 4, \"position\": 21}, {\"embedding\": -0.3383587896823883, \"dimension\": 4, \"position\": 22}, {\"embedding\": -0.4826357960700989, \"dimension\": 4, \"position\": 23}, {\"embedding\": -0.6148146390914917, \"dimension\": 4, \"position\": 24}, {\"embedding\": -0.7315824031829834, \"dimension\": 4, \"position\": 25}, {\"embedding\": -0.8300122618675232, \"dimension\": 4, \"position\": 26}, {\"embedding\": -0.9076365828514099, \"dimension\": 4, \"position\": 27}, {\"embedding\": -0.96250981092453, \"dimension\": 4, \"position\": 28}, {\"embedding\": -0.9932565093040466, \"dimension\": 4, \"position\": 29}, {\"embedding\": -0.9991058707237244, \"dimension\": 4, \"position\": 30}, {\"embedding\": -0.9799113273620605, \"dimension\": 4, \"position\": 31}, {\"embedding\": -0.9361540079116821, \"dimension\": 4, \"position\": 32}, {\"embedding\": -0.8689308166503906, \"dimension\": 4, \"position\": 33}, {\"embedding\": -0.7799267172813416, \"dimension\": 4, \"position\": 34}, {\"embedding\": -0.6713724136352539, \"dimension\": 4, \"position\": 35}, {\"embedding\": -0.5459895133972168, \"dimension\": 4, \"position\": 36}, {\"embedding\": -0.40692076086997986, \"dimension\": 4, \"position\": 37}, {\"embedding\": -0.2576519548892975, \"dimension\": 4, \"position\": 38}, {\"embedding\": -0.10192479938268661, \"dimension\": 4, \"position\": 39}, {\"embedding\": 0.056357722729444504, \"dimension\": 4, \"position\": 40}, {\"embedding\": 0.21322709321975708, \"dimension\": 4, \"position\": 41}, {\"embedding\": 0.3647516369819641, \"dimension\": 4, \"position\": 42}, {\"embedding\": 0.5071332454681396, \"dimension\": 4, \"position\": 43}, {\"embedding\": 0.6368028521537781, \"dimension\": 4, \"position\": 44}, {\"embedding\": 0.7505101561546326, \"dimension\": 4, \"position\": 45}, {\"embedding\": 0.8454052805900574, \"dimension\": 4, \"position\": 46}, {\"embedding\": 0.9191088080406189, \"dimension\": 4, \"position\": 47}, {\"embedding\": 0.9697737693786621, \"dimension\": 4, \"position\": 48}, {\"embedding\": 0.9961300492286682, \"dimension\": 4, \"position\": 49}, {\"embedding\": 0.9975170493125916, \"dimension\": 4, \"position\": 50}, {\"embedding\": 0.9738998413085938, \"dimension\": 4, \"position\": 51}, {\"embedding\": 0.9258706569671631, \"dimension\": 4, \"position\": 52}, {\"embedding\": 0.8546332716941833, \"dimension\": 4, \"position\": 53}, {\"embedding\": 0.7619734406471252, \"dimension\": 4, \"position\": 54}, {\"embedding\": 0.6502137184143066, \"dimension\": 4, \"position\": 55}, {\"embedding\": 0.5221555233001709, \"dimension\": 4, \"position\": 56}, {\"embedding\": 0.38100889325141907, \"dimension\": 4, \"position\": 57}, {\"embedding\": 0.23031172156333923, \"dimension\": 4, \"position\": 58}, {\"embedding\": 0.07384055852890015, \"dimension\": 4, \"position\": 59}, {\"embedding\": -0.08448058366775513, \"dimension\": 4, \"position\": 60}, {\"embedding\": -0.2406841218471527, \"dimension\": 4, \"position\": 61}, {\"embedding\": -0.3908545970916748, \"dimension\": 4, \"position\": 62}, {\"embedding\": -0.5312277674674988, \"dimension\": 4, \"position\": 63}, {\"embedding\": -0.6582850813865662, \"dimension\": 4, \"position\": 64}, {\"embedding\": -0.768841564655304, \"dimension\": 4, \"position\": 65}, {\"embedding\": -0.8601260781288147, \"dimension\": 4, \"position\": 66}, {\"embedding\": -0.9298503994941711, \"dimension\": 4, \"position\": 67}, {\"embedding\": -0.9762668013572693, \"dimension\": 4, \"position\": 68}, {\"embedding\": -0.9982118010520935, \"dimension\": 4, \"position\": 69}, {\"embedding\": -0.9951352477073669, \"dimension\": 4, \"position\": 70}, {\"embedding\": -0.967114269733429, \"dimension\": 4, \"position\": 71}, {\"embedding\": -0.9148513078689575, \"dimension\": 4, \"position\": 72}, {\"embedding\": -0.839656412601471, \"dimension\": 4, \"position\": 73}, {\"embedding\": -0.7434144616127014, \"dimension\": 4, \"position\": 74}, {\"embedding\": -0.6285378336906433, \"dimension\": 4, \"position\": 75}, {\"embedding\": -0.4979061186313629, \"dimension\": 4, \"position\": 76}, {\"embedding\": -0.3547937273979187, \"dimension\": 4, \"position\": 77}, {\"embedding\": -0.20278796553611755, \"dimension\": 4, \"position\": 78}, {\"embedding\": -0.04569905996322632, \"dimension\": 4, \"position\": 79}, {\"embedding\": 0.11253630369901657, \"dimension\": 4, \"position\": 80}, {\"embedding\": 0.2679498493671417, \"dimension\": 4, \"position\": 81}, {\"embedding\": 0.4166468679904938, \"dimension\": 4, \"position\": 82}, {\"embedding\": 0.5549001097679138, \"dimension\": 4, \"position\": 83}, {\"embedding\": 0.6792440414428711, \"dimension\": 4, \"position\": 84}, {\"embedding\": 0.7865618467330933, \"dimension\": 4, \"position\": 85}, {\"embedding\": 0.8741634488105774, \"dimension\": 4, \"position\": 86}, {\"embedding\": 0.9398530125617981, \"dimension\": 4, \"position\": 87}, {\"embedding\": 0.9819839596748352, \"dimension\": 4, \"position\": 88}, {\"embedding\": 0.9995002150535583, \"dimension\": 4, \"position\": 89}, {\"embedding\": 0.9919626712799072, \"dimension\": 4, \"position\": 90}, {\"embedding\": 0.959559977054596, \"dimension\": 4, \"position\": 91}, {\"embedding\": 0.903104841709137, \"dimension\": 4, \"position\": 92}, {\"embedding\": 0.8240122199058533, \"dimension\": 4, \"position\": 93}, {\"embedding\": 0.7242646217346191, \"dimension\": 4, \"position\": 94}, {\"embedding\": 0.6063624024391174, \"dimension\": 4, \"position\": 95}, {\"embedding\": 0.4732609689235687, \"dimension\": 4, \"position\": 96}, {\"embedding\": 0.32829657196998596, \"dimension\": 4, \"position\": 97}, {\"embedding\": 0.17510302364826202, \"dimension\": 4, \"position\": 98}, {\"embedding\": 0.01752028614282608, \"dimension\": 4, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 5, \"position\": 0}, {\"embedding\": 0.9874668121337891, \"dimension\": 5, \"position\": 1}, {\"embedding\": 0.9501814842224121, \"dimension\": 5, \"position\": 2}, {\"embedding\": 0.8890786170959473, \"dimension\": 5, \"position\": 3}, {\"embedding\": 0.8056897521018982, \"dimension\": 5, \"position\": 4}, {\"embedding\": 0.7021052241325378, \"dimension\": 5, \"position\": 5}, {\"embedding\": 0.5809215903282166, \"dimension\": 5, \"position\": 6}, {\"embedding\": 0.44517630338668823, \"dimension\": 5, \"position\": 7}, {\"embedding\": 0.2982720732688904, \"dimension\": 5, \"position\": 8}, {\"embedding\": 0.14389123022556305, \"dimension\": 5, \"position\": 9}, {\"embedding\": -0.01409643329679966, \"dimension\": 5, \"position\": 10}, {\"embedding\": -0.1717306226491928, \"dimension\": 5, \"position\": 11}, {\"embedding\": -0.32506027817726135, \"dimension\": 5, \"position\": 12}, {\"embedding\": -0.47024187445640564, \"dimension\": 5, \"position\": 13}, {\"embedding\": -0.6036361455917358, \"dimension\": 5, \"position\": 14}, {\"embedding\": -0.7218996286392212, \"dimension\": 5, \"position\": 15}, {\"embedding\": -0.8220675587654114, \"dimension\": 5, \"position\": 16}, {\"embedding\": -0.9016293287277222, \"dimension\": 5, \"position\": 17}, {\"embedding\": -0.9585906267166138, \"dimension\": 5, \"position\": 18}, {\"embedding\": -0.9915235042572021, \"dimension\": 5, \"position\": 19}, {\"embedding\": -0.9996025562286377, \"dimension\": 5, \"position\": 20}, {\"embedding\": -0.9826252460479736, \"dimension\": 5, \"position\": 21}, {\"embedding\": -0.941017210483551, \"dimension\": 5, \"position\": 22}, {\"embedding\": -0.8758211731910706, \"dimension\": 5, \"position\": 23}, {\"embedding\": -0.788671612739563, \"dimension\": 5, \"position\": 24}, {\"embedding\": -0.6817529797554016, \"dimension\": 5, \"position\": 25}, {\"embedding\": -0.5577451586723328, \"dimension\": 5, \"position\": 26}, {\"embedding\": -0.4197568893432617, \"dimension\": 5, \"position\": 27}, {\"embedding\": -0.27124688029289246, \"dimension\": 5, \"position\": 28}, {\"embedding\": -0.11593768745660782, \"dimension\": 5, \"position\": 29}, {\"embedding\": 0.042278096079826355, \"dimension\": 5, \"position\": 30}, {\"embedding\": 0.19943365454673767, \"dimension\": 5, \"position\": 31}, {\"embedding\": 0.3515901565551758, \"dimension\": 5, \"position\": 32}, {\"embedding\": 0.4949335753917694, \"dimension\": 5, \"position\": 33}, {\"embedding\": 0.6258708238601685, \"dimension\": 5, \"position\": 34}, {\"embedding\": 0.7411201596260071, \"dimension\": 5, \"position\": 35}, {\"embedding\": 0.8377919793128967, \"dimension\": 5, \"position\": 36}, {\"embedding\": 0.9134634733200073, \"dimension\": 5, \"position\": 37}, {\"embedding\": 0.9662377834320068, \"dimension\": 5, \"position\": 38}, {\"embedding\": 0.994792103767395, \"dimension\": 5, \"position\": 39}, {\"embedding\": 0.9984106421470642, \"dimension\": 5, \"position\": 40}, {\"embedding\": 0.9770026803016663, \"dimension\": 5, \"position\": 41}, {\"embedding\": 0.931104838848114, \"dimension\": 5, \"position\": 42}, {\"embedding\": 0.8618676662445068, \"dimension\": 5, \"position\": 43}, {\"embedding\": 0.7710266709327698, \"dimension\": 5, \"position\": 44}, {\"embedding\": 0.6608588695526123, \"dimension\": 5, \"position\": 45}, {\"embedding\": 0.5341253876686096, \"dimension\": 5, \"position\": 46}, {\"embedding\": 0.3940037488937378, \"dimension\": 5, \"position\": 47}, {\"embedding\": 0.24400585889816284, \"dimension\": 5, \"position\": 48}, {\"embedding\": 0.08789165318012238, \"dimension\": 5, \"position\": 49}, {\"embedding\": -0.07042567431926727, \"dimension\": 5, \"position\": 50}, {\"embedding\": -0.2269781529903412, \"dimension\": 5, \"position\": 51}, {\"embedding\": -0.37784066796302795, \"dimension\": 5, \"position\": 52}, {\"embedding\": -0.5192320942878723, \"dimension\": 5, \"position\": 53}, {\"embedding\": -0.6476082801818848, \"dimension\": 5, \"position\": 54}, {\"embedding\": -0.7597513794898987, \"dimension\": 5, \"position\": 55}, {\"embedding\": -0.8528502583503723, \"dimension\": 5, \"position\": 56}, {\"embedding\": -0.9245713949203491, \"dimension\": 5, \"position\": 57}, {\"embedding\": -0.973116934299469, \"dimension\": 5, \"position\": 58}, {\"embedding\": -0.9972700476646423, \"dimension\": 5, \"position\": 59}, {\"embedding\": -0.9964250922203064, \"dimension\": 5, \"position\": 60}, {\"embedding\": -0.9706035256385803, \"dimension\": 5, \"position\": 61}, {\"embedding\": -0.9204524159431458, \"dimension\": 5, \"position\": 62}, {\"embedding\": -0.8472290635108948, \"dimension\": 5, \"position\": 63}, {\"embedding\": -0.7527687549591064, \"dimension\": 5, \"position\": 64}, {\"embedding\": -0.6394393444061279, \"dimension\": 5, \"position\": 65}, {\"embedding\": -0.5100815296173096, \"dimension\": 5, \"position\": 66}, {\"embedding\": -0.3679378628730774, \"dimension\": 5, \"position\": 67}, {\"embedding\": -0.2165713608264923, \"dimension\": 5, \"position\": 68}, {\"embedding\": -0.05977622792124748, \"dimension\": 5, \"position\": 69}, {\"embedding\": 0.09851823002099991, \"dimension\": 5, \"position\": 70}, {\"embedding\": 0.254342257976532, \"dimension\": 5, \"position\": 71}, {\"embedding\": 0.4037908613681793, \"dimension\": 5, \"position\": 72}, {\"embedding\": 0.543117880821228, \"dimension\": 5, \"position\": 73}, {\"embedding\": 0.6688309907913208, \"dimension\": 5, \"position\": 74}, {\"embedding\": 0.7777789831161499, \"dimension\": 5, \"position\": 75}, {\"embedding\": 0.8672309517860413, \"dimension\": 5, \"position\": 76}, {\"embedding\": 0.9349446296691895, \"dimension\": 5, \"position\": 77}, {\"embedding\": 0.9792226552963257, \"dimension\": 5, \"position\": 78}, {\"embedding\": 0.998955249786377, \"dimension\": 5, \"position\": 79}, {\"embedding\": 0.9936476349830627, \"dimension\": 5, \"position\": 80}, {\"embedding\": 0.9634328484535217, \"dimension\": 5, \"position\": 81}, {\"embedding\": 0.9090684056282043, \"dimension\": 5, \"position\": 82}, {\"embedding\": 0.8319169878959656, \"dimension\": 5, \"position\": 83}, {\"embedding\": 0.733912467956543, \"dimension\": 5, \"position\": 84}, {\"embedding\": 0.617511510848999, \"dimension\": 5, \"position\": 85}, {\"embedding\": 0.4856317937374115, \"dimension\": 5, \"position\": 86}, {\"embedding\": 0.3415791094303131, \"dimension\": 5, \"position\": 87}, {\"embedding\": 0.18896427750587463, \"dimension\": 5, \"position\": 88}, {\"embedding\": 0.0316128134727478, \"dimension\": 5, \"position\": 89}, {\"embedding\": -0.12653106451034546, \"dimension\": 5, \"position\": 90}, {\"embedding\": -0.28150418400764465, \"dimension\": 5, \"position\": 91}, {\"embedding\": -0.4294200837612152, \"dimension\": 5, \"position\": 92}, {\"embedding\": -0.5665720105171204, \"dimension\": 5, \"position\": 93}, {\"embedding\": -0.6895220875740051, \"dimension\": 5, \"position\": 94}, {\"embedding\": -0.7951884269714355, \"dimension\": 5, \"position\": 95}, {\"embedding\": -0.880922257900238, \"dimension\": 5, \"position\": 96}, {\"embedding\": -0.9445747137069702, \"dimension\": 5, \"position\": 97}, {\"embedding\": -0.9845501184463501, \"dimension\": 5, \"position\": 98}, {\"embedding\": -0.9998465180397034, \"dimension\": 5, \"position\": 99}, {\"embedding\": 0.0, \"dimension\": 6, \"position\": 0}, {\"embedding\": 0.06305388361215591, \"dimension\": 6, \"position\": 1}, {\"embedding\": 0.12585683166980743, \"dimension\": 6, \"position\": 2}, {\"embedding\": 0.18815888464450836, \"dimension\": 6, \"position\": 3}, {\"embedding\": 0.24971213936805725, \"dimension\": 6, \"position\": 4}, {\"embedding\": 0.31027159094810486, \"dimension\": 6, \"position\": 5}, {\"embedding\": 0.3695962131023407, \"dimension\": 6, \"position\": 6}, {\"embedding\": 0.4274499714374542, \"dimension\": 6, \"position\": 7}, {\"embedding\": 0.4836025834083557, \"dimension\": 6, \"position\": 8}, {\"embedding\": 0.5378305912017822, \"dimension\": 6, \"position\": 9}, {\"embedding\": 0.5899181365966797, \"dimension\": 6, \"position\": 10}, {\"embedding\": 0.6396579146385193, \"dimension\": 6, \"position\": 11}, {\"embedding\": 0.6868520379066467, \"dimension\": 6, \"position\": 12}, {\"embedding\": 0.7313126921653748, \"dimension\": 6, \"position\": 13}, {\"embedding\": 0.7728629112243652, \"dimension\": 6, \"position\": 14}, {\"embedding\": 0.8113372921943665, \"dimension\": 6, \"position\": 15}, {\"embedding\": 0.8465827703475952, \"dimension\": 6, \"position\": 16}, {\"embedding\": 0.8784590363502502, \"dimension\": 6, \"position\": 17}, {\"embedding\": 0.9068393111228943, \"dimension\": 6, \"position\": 18}, {\"embedding\": 0.9316105246543884, \"dimension\": 6, \"position\": 19}, {\"embedding\": 0.9526742100715637, \"dimension\": 6, \"position\": 20}, {\"embedding\": 0.9699464440345764, \"dimension\": 6, \"position\": 21}, {\"embedding\": 0.9833585619926453, \"dimension\": 6, \"position\": 22}, {\"embedding\": 0.9928570985794067, \"dimension\": 6, \"position\": 23}, {\"embedding\": 0.9984043836593628, \"dimension\": 6, \"position\": 24}, {\"embedding\": 0.999978244304657, \"dimension\": 6, \"position\": 25}, {\"embedding\": 0.9975724220275879, \"dimension\": 6, \"position\": 26}, {\"embedding\": 0.9911965131759644, \"dimension\": 6, \"position\": 27}, {\"embedding\": 0.9808759093284607, \"dimension\": 6, \"position\": 28}, {\"embedding\": 0.9666516780853271, \"dimension\": 6, \"position\": 29}, {\"embedding\": 0.9485803842544556, \"dimension\": 6, \"position\": 30}, {\"embedding\": 0.9267339110374451, \"dimension\": 6, \"position\": 31}, {\"embedding\": 0.9011994004249573, \"dimension\": 6, \"position\": 32}, {\"embedding\": 0.8720782399177551, \"dimension\": 6, \"position\": 33}, {\"embedding\": 0.8394865393638611, \"dimension\": 6, \"position\": 34}, {\"embedding\": 0.8035537600517273, \"dimension\": 6, \"position\": 35}, {\"embedding\": 0.7644230127334595, \"dimension\": 6, \"position\": 36}, {\"embedding\": 0.7222501039505005, \"dimension\": 6, \"position\": 37}, {\"embedding\": 0.6772029399871826, \"dimension\": 6, \"position\": 38}, {\"embedding\": 0.6294605135917664, \"dimension\": 6, \"position\": 39}, {\"embedding\": 0.57921302318573, \"dimension\": 6, \"position\": 40}, {\"embedding\": 0.5266605615615845, \"dimension\": 6, \"position\": 41}, {\"embedding\": 0.4720119535923004, \"dimension\": 6, \"position\": 42}, {\"embedding\": 0.41548484563827515, \"dimension\": 6, \"position\": 43}, {\"embedding\": 0.3573042154312134, \"dimension\": 6, \"position\": 44}, {\"embedding\": 0.29770180583000183, \"dimension\": 6, \"position\": 45}, {\"embedding\": 0.23691439628601074, \"dimension\": 6, \"position\": 46}, {\"embedding\": 0.17518411576747894, \"dimension\": 6, \"position\": 47}, {\"embedding\": 0.1127568930387497, \"dimension\": 6, \"position\": 48}, {\"embedding\": 0.04988069087266922, \"dimension\": 6, \"position\": 49}, {\"embedding\": -0.013194027356803417, \"dimension\": 6, \"position\": 50}, {\"embedding\": -0.07621623575687408, \"dimension\": 6, \"position\": 51}, {\"embedding\": -0.1389348804950714, \"dimension\": 6, \"position\": 52}, {\"embedding\": -0.20110084116458893, \"dimension\": 6, \"position\": 53}, {\"embedding\": -0.2624664604663849, \"dimension\": 6, \"position\": 54}, {\"embedding\": -0.3227875530719757, \"dimension\": 6, \"position\": 55}, {\"embedding\": -0.3818237781524658, \"dimension\": 6, \"position\": 56}, {\"embedding\": -0.4393406808376312, \"dimension\": 6, \"position\": 57}, {\"embedding\": -0.49510911107063293, \"dimension\": 6, \"position\": 58}, {\"embedding\": -0.5489069223403931, \"dimension\": 6, \"position\": 59}, {\"embedding\": -0.6005204319953918, \"dimension\": 6, \"position\": 60}, {\"embedding\": -0.6497439742088318, \"dimension\": 6, \"position\": 61}, {\"embedding\": -0.6963817477226257, \"dimension\": 6, \"position\": 62}, {\"embedding\": -0.7402478456497192, \"dimension\": 6, \"position\": 63}, {\"embedding\": -0.7811681628227234, \"dimension\": 6, \"position\": 64}, {\"embedding\": -0.8189795017242432, \"dimension\": 6, \"position\": 65}, {\"embedding\": -0.8535317182540894, \"dimension\": 6, \"position\": 66}, {\"embedding\": -0.8846868872642517, \"dimension\": 6, \"position\": 67}, {\"embedding\": -0.9123212099075317, \"dimension\": 6, \"position\": 68}, {\"embedding\": -0.936324954032898, \"dimension\": 6, \"position\": 69}, {\"embedding\": -0.956602156162262, \"dimension\": 6, \"position\": 70}, {\"embedding\": -0.9730724096298218, \"dimension\": 6, \"position\": 71}, {\"embedding\": -0.9856699705123901, \"dimension\": 6, \"position\": 72}, {\"embedding\": -0.9943448305130005, \"dimension\": 6, \"position\": 73}, {\"embedding\": -0.9990625381469727, \"dimension\": 6, \"position\": 74}, {\"embedding\": -0.9998041391372681, \"dimension\": 6, \"position\": 75}, {\"embedding\": -0.9965668320655823, \"dimension\": 6, \"position\": 76}, {\"embedding\": -0.9893633723258972, \"dimension\": 6, \"position\": 77}, {\"embedding\": -0.9782225489616394, \"dimension\": 6, \"position\": 78}, {\"embedding\": -0.963188648223877, \"dimension\": 6, \"position\": 79}, {\"embedding\": -0.9443213939666748, \"dimension\": 6, \"position\": 80}, {\"embedding\": -0.9216960668563843, \"dimension\": 6, \"position\": 81}, {\"embedding\": -0.8954026699066162, \"dimension\": 6, \"position\": 82}, {\"embedding\": -0.8655455708503723, \"dimension\": 6, \"position\": 83}, {\"embedding\": -0.8322440981864929, \"dimension\": 6, \"position\": 84}, {\"embedding\": -0.795630156993866, \"dimension\": 6, \"position\": 85}, {\"embedding\": -0.7558501362800598, \"dimension\": 6, \"position\": 86}, {\"embedding\": -0.7130619883537292, \"dimension\": 6, \"position\": 87}, {\"embedding\": -0.6674357056617737, \"dimension\": 6, \"position\": 88}, {\"embedding\": -0.6191535592079163, \"dimension\": 6, \"position\": 89}, {\"embedding\": -0.5684073567390442, \"dimension\": 6, \"position\": 90}, {\"embedding\": -0.5153986215591431, \"dimension\": 6, \"position\": 91}, {\"embedding\": -0.46033912897109985, \"dimension\": 6, \"position\": 92}, {\"embedding\": -0.40344759821891785, \"dimension\": 6, \"position\": 93}, {\"embedding\": -0.3449500501155853, \"dimension\": 6, \"position\": 94}, {\"embedding\": -0.28508007526397705, \"dimension\": 6, \"position\": 95}, {\"embedding\": -0.22407560050487518, \"dimension\": 6, \"position\": 96}, {\"embedding\": -0.1621788740158081, \"dimension\": 6, \"position\": 97}, {\"embedding\": -0.09963719546794891, \"dimension\": 6, \"position\": 98}, {\"embedding\": -0.03669850528240204, \"dimension\": 6, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 7, \"position\": 0}, {\"embedding\": 0.9980100989341736, \"dimension\": 7, \"position\": 1}, {\"embedding\": 0.9920483827590942, \"dimension\": 7, \"position\": 2}, {\"embedding\": 0.9821385741233826, \"dimension\": 7, \"position\": 3}, {\"embedding\": 0.9683201313018799, \"dimension\": 7, \"position\": 4}, {\"embedding\": 0.9506479501724243, \"dimension\": 7, \"position\": 5}, {\"embedding\": 0.9291924834251404, \"dimension\": 7, \"position\": 6}, {\"embedding\": 0.9040390253067017, \"dimension\": 7, \"position\": 7}, {\"embedding\": 0.8752877116203308, \"dimension\": 7, \"position\": 8}, {\"embedding\": 0.8430529236793518, \"dimension\": 7, \"position\": 9}, {\"embedding\": 0.8074630498886108, \"dimension\": 7, \"position\": 10}, {\"embedding\": 0.7686597108840942, \"dimension\": 7, \"position\": 11}, {\"embedding\": 0.7267972826957703, \"dimension\": 7, \"position\": 12}, {\"embedding\": 0.6820423603057861, \"dimension\": 7, \"position\": 13}, {\"embedding\": 0.6345730423927307, \"dimension\": 7, \"position\": 14}, {\"embedding\": 0.5845783352851868, \"dimension\": 7, \"position\": 15}, {\"embedding\": 0.532257080078125, \"dimension\": 7, \"position\": 16}, {\"embedding\": 0.47781768441200256, \"dimension\": 7, \"position\": 17}, {\"embedding\": 0.4214765727519989, \"dimension\": 7, \"position\": 18}, {\"embedding\": 0.36345821619033813, \"dimension\": 7, \"position\": 19}, {\"embedding\": 0.30399325489997864, \"dimension\": 7, \"position\": 20}, {\"embedding\": 0.243318572640419, \"dimension\": 7, \"position\": 21}, {\"embedding\": 0.18167544901371002, \"dimension\": 7, \"position\": 22}, {\"embedding\": 0.11930941045284271, \"dimension\": 7, \"position\": 23}, {\"embedding\": 0.056468550115823746, \"dimension\": 7, \"position\": 24}, {\"embedding\": -0.006597157102078199, \"dimension\": 7, \"position\": 25}, {\"embedding\": -0.06963648647069931, \"dimension\": 7, \"position\": 26}, {\"embedding\": -0.1323987990617752, \"dimension\": 7, \"position\": 27}, {\"embedding\": -0.1946340948343277, \"dimension\": 7, \"position\": 28}, {\"embedding\": -0.25609490275382996, \"dimension\": 7, \"position\": 29}, {\"embedding\": -0.31653639674186707, \"dimension\": 7, \"position\": 30}, {\"embedding\": -0.37571826577186584, \"dimension\": 7, \"position\": 31}, {\"embedding\": -0.43340474367141724, \"dimension\": 7, \"position\": 32}, {\"embedding\": -0.4893665015697479, \"dimension\": 7, \"position\": 33}, {\"embedding\": -0.5433804988861084, \"dimension\": 7, \"position\": 34}, {\"embedding\": -0.5952321887016296, \"dimension\": 7, \"position\": 35}, {\"embedding\": -0.6447150111198425, \"dimension\": 7, \"position\": 36}, {\"embedding\": -0.6916319727897644, \"dimension\": 7, \"position\": 37}, {\"embedding\": -0.7357962727546692, \"dimension\": 7, \"position\": 38}, {\"embedding\": -0.7770324349403381, \"dimension\": 7, \"position\": 39}, {\"embedding\": -0.8151761889457703, \"dimension\": 7, \"position\": 40}, {\"embedding\": -0.8500756621360779, \"dimension\": 7, \"position\": 41}, {\"embedding\": -0.8815921545028687, \"dimension\": 7, \"position\": 42}, {\"embedding\": -0.9096000790596008, \"dimension\": 7, \"position\": 43}, {\"embedding\": -0.933988094329834, \"dimension\": 7, \"position\": 44}, {\"embedding\": -0.9546589255332947, \"dimension\": 7, \"position\": 45}, {\"embedding\": -0.971530556678772, \"dimension\": 7, \"position\": 46}, {\"embedding\": -0.9845356941223145, \"dimension\": 7, \"position\": 47}, {\"embedding\": -0.9936226010322571, \"dimension\": 7, \"position\": 48}, {\"embedding\": -0.998755156993866, \"dimension\": 7, \"position\": 49}, {\"embedding\": -0.9999129772186279, \"dimension\": 7, \"position\": 50}, {\"embedding\": -0.9970912933349609, \"dimension\": 7, \"position\": 51}, {\"embedding\": -0.9903014898300171, \"dimension\": 7, \"position\": 52}, {\"embedding\": -0.9795705676078796, \"dimension\": 7, \"position\": 53}, {\"embedding\": -0.964941143989563, \"dimension\": 7, \"position\": 54}, {\"embedding\": -0.9464714527130127, \"dimension\": 7, \"position\": 55}, {\"embedding\": -0.9242351651191711, \"dimension\": 7, \"position\": 56}, {\"embedding\": -0.8983205556869507, \"dimension\": 7, \"position\": 57}, {\"embedding\": -0.8688308000564575, \"dimension\": 7, \"position\": 58}, {\"embedding\": -0.8358834981918335, \"dimension\": 7, \"position\": 59}, {\"embedding\": -0.7996094226837158, \"dimension\": 7, \"position\": 60}, {\"embedding\": -0.7601531147956848, \"dimension\": 7, \"position\": 61}, {\"embedding\": -0.7176715731620789, \"dimension\": 7, \"position\": 62}, {\"embedding\": -0.6723340749740601, \"dimension\": 7, \"position\": 63}, {\"embedding\": -0.6243206262588501, \"dimension\": 7, \"position\": 64}, {\"embedding\": -0.5738227963447571, \"dimension\": 7, \"position\": 65}, {\"embedding\": -0.5210408568382263, \"dimension\": 7, \"position\": 66}, {\"embedding\": -0.46618568897247314, \"dimension\": 7, \"position\": 67}, {\"embedding\": -0.4094752371311188, \"dimension\": 7, \"position\": 68}, {\"embedding\": -0.3511347472667694, \"dimension\": 7, \"position\": 69}, {\"embedding\": -0.2913972735404968, \"dimension\": 7, \"position\": 70}, {\"embedding\": -0.23049965500831604, \"dimension\": 7, \"position\": 71}, {\"embedding\": -0.1686851680278778, \"dimension\": 7, \"position\": 72}, {\"embedding\": -0.10619935393333435, \"dimension\": 7, \"position\": 73}, {\"embedding\": -0.04329042136669159, \"dimension\": 7, \"position\": 74}, {\"embedding\": 0.019790323451161385, \"dimension\": 7, \"position\": 75}, {\"embedding\": 0.08279230445623398, \"dimension\": 7, \"position\": 76}, {\"embedding\": 0.14546526968479156, \"dimension\": 7, \"position\": 77}, {\"embedding\": 0.20755885541439056, \"dimension\": 7, \"position\": 78}, {\"embedding\": 0.2688263952732086, \"dimension\": 7, \"position\": 79}, {\"embedding\": 0.3290245532989502, \"dimension\": 7, \"position\": 80}, {\"embedding\": 0.3879128098487854, \"dimension\": 7, \"position\": 81}, {\"embedding\": 0.4452572762966156, \"dimension\": 7, \"position\": 82}, {\"embedding\": 0.5008301138877869, \"dimension\": 7, \"position\": 83}, {\"embedding\": 0.5544094443321228, \"dimension\": 7, \"position\": 84}, {\"embedding\": 0.605782687664032, \"dimension\": 7, \"position\": 85}, {\"embedding\": 0.6547446846961975, \"dimension\": 7, \"position\": 86}, {\"embedding\": 0.7011010050773621, \"dimension\": 7, \"position\": 87}, {\"embedding\": 0.7446674108505249, \"dimension\": 7, \"position\": 88}, {\"embedding\": 0.7852699160575867, \"dimension\": 7, \"position\": 89}, {\"embedding\": 0.8227472901344299, \"dimension\": 7, \"position\": 90}, {\"embedding\": 0.856950581073761, \"dimension\": 7, \"position\": 91}, {\"embedding\": 0.8877431154251099, \"dimension\": 7, \"position\": 92}, {\"embedding\": 0.9150027632713318, \"dimension\": 7, \"position\": 93}, {\"embedding\": 0.9386210441589355, \"dimension\": 7, \"position\": 94}, {\"embedding\": 0.9585037231445312, \"dimension\": 7, \"position\": 95}, {\"embedding\": 0.9745717644691467, \"dimension\": 7, \"position\": 96}, {\"embedding\": 0.9867613911628723, \"dimension\": 7, \"position\": 97}, {\"embedding\": 0.9950238466262817, \"dimension\": 7, \"position\": 98}, {\"embedding\": 0.9993264079093933, \"dimension\": 7, \"position\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_positional():\n",
    "    pe = PositionalEncoding(20, 0, 5000)\n",
    "    y = pe.forward(torch.zeros(1, 100, 20))\n",
    "\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"embedding\": y[0, :, dim],\n",
    "                    \"dimension\": dim,\n",
    "                    \"position\": list(range(100)),\n",
    "                }\n",
    "            )\n",
    "            for dim in [4, 5, 6, 7] ##TODO Change these values to your liking\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "\n",
    "\n",
    "example_positional()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TnfVeVeSzLU3",
   "metadata": {
    "id": "TnfVeVeSzLU3"
   },
   "source": [
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/positional_encoding_sine.png?raw=true\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6FZGjkpZ2zHu",
   "metadata": {
    "id": "6FZGjkpZ2zHu"
   },
   "source": [
    "### Part 2: Implementing multi-headed self-attention\n",
    "\n",
    "The multi-headed attention mechanisms are highlighted in the Figure below:\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/multi_headed_attention_highlighted.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The multi-head attention is a form of scaled dot-product attention, calculated as:\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/scaled_dot_product_attention_formula.PNG?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Scaled dot-product attention demonstrated as:\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/scaled_dot_product_attention.PNG?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Multi-head attention calculated as:\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/multi_head_Attention_formula.PNG?raw=true\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Multi-head attention demonstrated as:\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/multi_head_Attention.PNG?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Building the multi-headed self-attention can be observed from the implementation provided below.\n",
    "\n",
    "#### Instructions\n",
    "* Split the sequence embeddings x across the multiple attention heads.\n",
    "* Compute dot-product based attention scores between the project query and key.\n",
    "* Normalize the attention scores to obtain attention weights.\n",
    "* Multiply the attention weights by the values and linearly transform the concatenated outputs per head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb67b6-5884-40ae-9b54-78192b5506d4",
   "metadata": {
    "id": "b1fb67b6-5884-40ae-9b54-78192b5506d4"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A Multi-Head Attention layer as described in the paper \"Attention is All You Need\"\n",
    "    (Vaswani et al., 2017).\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        num_heads (int): Number of attention heads.\n",
    "\n",
    "    Attributes:\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        head_dim (int): Dimensionality of each attention head.\n",
    "        query_linear (torch.nn.Linear): Linear transformation layer for queries.\n",
    "        key_linear (torch.nn.Linear): Linear transformation layer for keys.\n",
    "        value_linear (torch.nn.Linear): Linear transformation layer for values.\n",
    "        output_linear (torch.nn.Linear): Linear transformation layer for the final output.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // num_heads  # Dimension of each attention head\n",
    "\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"\n",
    "        Splits the sequence embeddings in `x` across the attention heads.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor to be split.\n",
    "            batch_size (int): Batch size of the input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Reshaped tensor with the sequence embeddings split across the attention heads.\n",
    "        \"\"\"\n",
    "        # Split the sequence embeddings in x across the attention heads\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n",
    "\n",
    "    def compute_attention(self, query, key, mask=None):\n",
    "        \"\"\"\n",
    "        Computes dot-product attention scores.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor.\n",
    "            key (torch.Tensor): Key tensor.\n",
    "            mask (torch.Tensor, optional): Mask tensor to mask out certain positions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Attention weights.\n",
    "        \"\"\"\n",
    "        # Compute dot-product attention scores\n",
    "        \n",
    "        ## use the \"query\" and \"key.permute(1, 2, 0)\" (key transpose) to calculate the attention score\n",
    "        scores = # TODO\n",
    "        if mask is not None:\n",
    "            ## Apply mask with torch.masked_fill, don't forget to put an infinitesimally small \"0\"\n",
    "            scores = ## TODO\n",
    "        # Normalize attention scores into attention weights\n",
    "        \n",
    "        ## Apply softmax to the scores\n",
    "        attention_weights = ## TODO\n",
    "        return attention_weights\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the multi-head attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor.\n",
    "            key (torch.Tensor): Key tensor.\n",
    "            value (torch.Tensor): Value tensor.\n",
    "            mask (torch.Tensor, optional): Mask tensor to mask out certain positions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying multi-head attention mechanism.\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Project queries, keys, and values to separate heads\n",
    "        query = self.split_heads(self.query_linear(query), batch_size)\n",
    "        key = self.split_heads(self.key_linear(key), batch_size)\n",
    "        value = self.split_heads(self.value_linear(value), batch_size)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = ##TODO \n",
    "\n",
    "        # Multiply attention weights by values and linearly project concatenated outputs\n",
    "        output = ##TODO\n",
    "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.output_linear(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0ix1uQi3Gkt",
   "metadata": {
    "id": "v0ix1uQi3Gkt"
   },
   "source": [
    "### Part 3: Post-attention feed-forward layer\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/feed_forward_highlighted.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Feed-forward sublayer following multi-head self-attention for every encoder layer is built as an example below:\n",
    "\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "* Specify in the __init__() method the sizes of the two linear fully connected layers.\n",
    "* Apply a forward pass through the two linear layers, using the ReLU() activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a3f9f-f65c-450c-b49d-d849b0cdf617",
   "metadata": {
    "id": "f32a3f9f-f65c-450c-b49d-d849b0cdf617"
   },
   "outputs": [],
   "source": [
    "class FeedForwardSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward sub-layer module.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        d_ff (int): Dimensionality of the intermediate hidden layer.\n",
    "\n",
    "    Attributes:\n",
    "        fc1 (torch.nn.Linear): First fully connected layer.\n",
    "        fc2 (torch.nn.Linear): Second fully connected layer.\n",
    "        relu (torch.nn.ReLU): ReLU activation function.\n",
    "    \"\"\"\n",
    "    # Specify the two linear layers' input and output sizes\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Initialize feed-forward sub-layer with two linear layers and ReLU activation.\n",
    "        \"\"\"\n",
    "        super(FeedForwardSubLayer, self).__init__()\n",
    "        self.fc1 = # TODO\n",
    "        self.fc2 = # TODO\n",
    "        self.relu = # TODO\n",
    "\n",
    "    # Apply a forward pass\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the feed-forward sub-layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying the feed-forward sub-layer.\n",
    "        \"\"\"\n",
    "        return # TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xl3xBFBBA3LN",
   "metadata": {
    "id": "xl3xBFBBA3LN"
   },
   "source": [
    "## Encoder Transformer\n",
    "\n",
    "Useful for tasks like classification, one of the most common application is  BERT (Bidirectional Encoder Representations from Transformers). Applies self-attention to the inputs to determine which parts are more useful for the task.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/Encoder_only_transformer.png?raw=true\" width=\"150\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uFbiS9Jg4Ic9",
   "metadata": {
    "id": "uFbiS9Jg4Ic9"
   },
   "source": [
    "### Part 4: Encoder layer\n",
    "\n",
    "\n",
    "Assembling a full encoder layer containing:\n",
    "\n",
    "* A multi-headed self-attention mechanism.\n",
    "* A feed-forward sublayer.\n",
    "* A combined layer normalization and dropout to be applied after each of the above two stages.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/Encoder_only_transformer_norm_highlighted.png?raw=true\" width=\"150\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XtH2xwFO4OW3",
   "metadata": {
    "id": "XtH2xwFO4OW3"
   },
   "outputs": [],
   "source": [
    "# Complete the initialization of elements in the encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder layer module.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_ff (int): Dimensionality of the intermediate hidden layer in the feed-forward sub-layer.\n",
    "        dropout (float): Dropout probability.\n",
    "\n",
    "    Attributes:\n",
    "        self_attn (MultiHeadAttention): Multi-head self-attention mechanism.\n",
    "        feed_forward (FeedForwardSubLayer): Feed-forward sub-layer.\n",
    "        norm1 (torch.nn.LayerNorm): Layer normalization for the first sub-layer.\n",
    "        norm2 (torch.nn.LayerNorm): Layer normalization for the second sub-layer.\n",
    "        dropout (torch.nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Initialize encoder layer with multi-head self-attention and feed-forward sub-layer.\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = # TODO\n",
    "        # Feedforward neural network\n",
    "        self.feed_forward = # TODO\n",
    "        self.norm1 = # TODO\n",
    "        self.norm2 = # TODO\n",
    "        self.dropout = # TODO\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            mask (torch.Tensor): Mask tensor for attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying multi-head self-attention and feed-forward sub-layer.\n",
    "        \"\"\"\n",
    "        # Multi-head self-attention\n",
    "        attn_output = # TODO\n",
    "        x = self.norm1(# TODO)\n",
    "        # Feedforward neural network\n",
    "        ff_output = # TODO\n",
    "        return self.norm2(# TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_8vDWOqJ4Yfb",
   "metadata": {
    "id": "_8vDWOqJ4Yfb"
   },
   "source": [
    "### Part 5: Encoder transformer body and head\n",
    "\n",
    "Implementing the transformer body, that is consisting of a stack of multiple encoder layers and a task specific transformer head that is used to process the encoder's hidden states.\n",
    "\n",
    "Apart from the highlighted components, we have implemented everything so far. The linear layer followed by a softmax to output probabilities is the final implementation we will do in this section.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/Encoder_only_transformer_head_highlighted.png?raw=true\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "* Define a stack of multiple encoder layers in the __init__() method.\n",
    "* Complete the forward() method. Note that the process starts by converting the original sequence tokens in x into embeddings.\n",
    "* Add final linear layer to project encoder results into raw classification outputs.\n",
    "* Apply the necessary function to map raw classification outputs into log class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7tjYzQG4PBN",
   "metadata": {
    "id": "c7tjYzQG4PBN"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        num_layers (int): Number of encoder layers.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_ff (int): Dimensionality of the intermediate hidden layer in the feed-forward sub-layer.\n",
    "        dropout (float): Dropout probability.\n",
    "        max_sequence_length (int): Maximum sequence length for positional encoding.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (torch.nn.Embedding): Embedding layer.\n",
    "        positional_encoding (PositionalEncoding): Positional encoding layer.\n",
    "        layers (torch.nn.ModuleList): List of encoder layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        \"\"\"\n",
    "        Initialize transformer encoder with embedding, positional encoding, and multiple encoder layers.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(# TODO)\n",
    "        self.positional_encoding = PositionalEncoding(# TODO)\n",
    "        # Define a stack of multiple encoder layers\n",
    "        self.layers = nn.ModuleList([# TODO for _ in range(num_layers)])\n",
    "\n",
    "    # Complete the forward pass method\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            mask (torch.Tensor): Mask tensor for attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after encoding.\n",
    "        \"\"\"\n",
    "        # Apply input embedding    \n",
    "        x = # TODO\n",
    "        # Apply positional encoding\n",
    "        x = # TODO\n",
    "        for layer in self.layers:\n",
    "            # Stack layers\n",
    "            x = # TODO\n",
    "        return x\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Classifier head module.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of the input feature vectors.\n",
    "        num_classes (int): Number of classes.\n",
    "\n",
    "    Attributes:\n",
    "        fc (torch.nn.Linear): Linear layer for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        \"\"\"\n",
    "        Initialize classifier head with linear layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the input feature vectors.\n",
    "            num_classes (int): Number of classes.\n",
    "        \"\"\"\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        # Add linear layer for multiple-class classification\n",
    "        self.fc = # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the classifier head.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Log class probabilities upon raw outputs.\n",
    "        \"\"\"\n",
    "        logits = self.fc(x[:, 0, :])\n",
    "        # Obtain log class probabilities upon raw outputs\n",
    "        return F.log_softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mdusUqHn4vrs",
   "metadata": {
    "id": "mdusUqHn4vrs"
   },
   "source": [
    "### Part 6: Testing the encoder transformer\n",
    "\n",
    "A random and simple sequence will be used as an input to the encoder transformer. Obtaining the output (that is not even human-readable) without any errors is sufficient for this exercise.\n",
    "\n",
    "The following components are adequate to form a full encoder transformer:\n",
    "* PositionalEncoder\n",
    "* MultiHeadAttention\n",
    "* FeedForwardSublayer\n",
    "* EncoderLayer\n",
    "* TransformerEncoder\n",
    "* ClassifierHead\n",
    "\n",
    "Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length.\n",
    "\n",
    "#### Instructions\n",
    "* Instantiate the body and head of the encoder transformer.\n",
    "* Complete the forward pass throughout the entire transformer body and head to obtain and print classification outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vGvWfAbYr5Mv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGvWfAbYr5Mv",
    "outputId": "302a1e0d-73d1-44b4-95e1-91b4f8cf230e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification outputs for a batch of  8 sequences:\n",
      "tensor([[-1.4459, -0.8831, -1.0471],\n",
      "        [-0.8046, -1.7915, -0.9519],\n",
      "        [-0.9212, -1.2561, -1.1482],\n",
      "        [-0.9247, -1.4208, -1.0165],\n",
      "        [-1.4352, -1.1845, -0.7852],\n",
      "        [-1.4846, -0.6909, -1.3009],\n",
      "        [-1.1482, -1.4163, -0.8206],\n",
      "        [-1.4330, -1.2273, -0.7586]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 64\n",
    "dropout = 0.1\n",
    "\n",
    "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "# input_sequence = torch.randint(0, vocab_size, (1, sequence_length))\n",
    "# for i in range(1,batch_size):\n",
    "#   input_sequence = torch.cat((input_sequence,(torch.randint(0, int(i*vocab_size/batch_size), (1, sequence_length)))))\n",
    "\n",
    "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "# Instantiate the encoder transformer's body and head\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "# Complete the forward pass\n",
    "output = encoder(input_sequence, mask)\n",
    "classification_no_train = classifier(output)\n",
    "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
    "print(classification_no_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MWO_4N1coiA8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "MWO_4N1coiA8",
    "outputId": "bcb32a55-b19a-4f70-d524-38e6990e1574"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f51c0fe4850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(input_sequence.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aUXUSF4KkuUW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "aUXUSF4KkuUW",
    "outputId": "c68f4301-7bbf-46a9-b1d1-5bce71319d07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f51bc55c2e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(classification_no_train.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gXwAibBsZWOu",
   "metadata": {
    "id": "gXwAibBsZWOu"
   },
   "source": [
    "The output is random, as expected from the random initialization of the learnable parameters. For each element in the batch, we have a row and columns corresponding to the number of classes we have defined.\n",
    "\n",
    "Now we can prepare a very basic training structure. Note that this training will not be plausible for any real-world example, but it is still important to follow for how to adapt the power of PyTorch. Changing parameters that define the transformer model and the task itself is encouraged here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LFvCVz42kub2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFvCVz42kub2",
    "outputId": "a65a2a4a-50c9-41d4-d264-bc7ca7df21d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.2837\n",
      "Epoch [2/10], Loss: 3.5493\n",
      "Epoch [3/10], Loss: 9.9661\n",
      "Epoch [4/10], Loss: 6.6045\n",
      "Epoch [5/10], Loss: 4.2706\n",
      "Epoch [6/10], Loss: 1.7698\n",
      "Epoch [7/10], Loss: 1.5776\n",
      "Epoch [8/10], Loss: 1.7213\n",
      "Epoch [9/10], Loss: 1.8039\n",
      "Epoch [10/10], Loss: 1.8295\n",
      "Classification outputs for a batch of 8 sequences:\n",
      "tensor([[-2.1471, -2.3153, -0.2428],\n",
      "        [-2.2620, -2.4823, -0.2079],\n",
      "        [-2.1690, -2.4683, -0.2219],\n",
      "        [-2.2322, -2.4408, -0.2162],\n",
      "        [-2.0167, -2.5945, -0.2329],\n",
      "        [-2.2177, -2.4163, -0.2208],\n",
      "        [-2.2602, -2.7448, -0.1846],\n",
      "        [-2.2154, -2.6419, -0.1989]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define your model\n",
    "num_classes = 3\n",
    "vocab_size = 10000\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "sequence_length = 64\n",
    "dropout = 0.1\n",
    "\n",
    "#input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "#mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "classifier = ClassifierHead(d_model, num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = encoder(input_sequence, mask)\n",
    "    classification = classifier(output)\n",
    "\n",
    "    # Assuming you have ground truth labels as 'targets'\n",
    "    targets = torch.randint(0, num_classes, (batch_size,))\n",
    "    loss = criterion(classification, targets)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# After training, you can print the classification results\n",
    "print(\"Classification outputs for a batch of\", batch_size, \"sequences:\")\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dEd-HYWxnpnX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "dEd-HYWxnpnX",
    "outputId": "9f9a9c46-fba0-4fb5-9f4f-19419f65a57a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f51bc3c6620>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(classification.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eigZkj6sYcQd",
   "metadata": {
    "id": "eigZkj6sYcQd"
   },
   "source": [
    "We have not arranged any viable feature for the input to the transformer model, so it is not possible to distinguish any of the inputs. However, we can observe that three classes will (somewhat) align after some training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2XYFLPGATsRN",
   "metadata": {
    "id": "2XYFLPGATsRN"
   },
   "source": [
    "Note: Although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length.\n",
    "\n",
    "We will come back to this mask again later to compare it with a causal mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iC7VVr3PS4zm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "iC7VVr3PS4zm",
    "outputId": "5b9f8a35-9195-4562-ac6c-99ce0d0b76d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f51b76f5600>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g_yPohnTA6i4",
   "metadata": {
    "id": "g_yPohnTA6i4"
   },
   "source": [
    "## Decoder Transformer\n",
    "Used for tasks like text generation, where the model generates text for a given input or prompt. Text generation occurs as a word at a time and conditions on itself (previously generated content) to generate new ones. GPT-3 (Generative Pre-trained Transformer 3) is a well-known Decoder only Transformer.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/decoder_only_transformer.png?raw=true\" width=\"150\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PgiCPIppxepa",
   "metadata": {
    "id": "PgiCPIppxepa"
   },
   "source": [
    "### Part 7: Decoder Layer\n",
    "\n",
    "The encoder layer in part 4 is built similarly; what is the difference between these two structures?\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "* A multi-headed self-attention mechanism.\n",
    "* A feed-forward sublayer.\n",
    "* Normalization and dropout are to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SHslCMRKxe18",
   "metadata": {
    "id": "SHslCMRKxe18"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder layer module.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_ff (int): Dimensionality of the intermediate hidden layer in the feed-forward sub-layer.\n",
    "        dropout (float): Dropout probability.\n",
    "\n",
    "    Attributes:\n",
    "        self_attention (MultiHeadAttention): Multi-head self-attention mechanism.\n",
    "        norm1 (torch.nn.LayerNorm): Layer normalization for the first sub-layer.\n",
    "        feed_forward (FeedForwardSubLayer): Feed-forward sub-layer.\n",
    "        norm2 (torch.nn.LayerNorm): Layer normalization for the second sub-layer.\n",
    "        dropout (torch.nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Initialize decoder layer with multi-head self-attention and feed-forward sub-layer.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Multi-head self-attention\n",
    "        self.self_attention = # TODO\n",
    "        self.norm1 = # TODO\n",
    "        # Feedforward neural network\n",
    "        self.feed_forward = # TODO\n",
    "        self.norm2 = # TODO\n",
    "        self.dropout = # TODO\n",
    "\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            self_mask (torch.Tensor): Mask tensor for self-attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying multi-head self-attention and feed-forward sub-layer.\n",
    "        \"\"\"\n",
    "        # Multi-head self-attention\n",
    "        attention_output = # TODO\n",
    "        x = # TODO\n",
    "        x = # TODO\n",
    "\n",
    "        # Feedforward neural network\n",
    "        ff_output = # TODO\n",
    "        x = # TODO\n",
    "        x = # TODO\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mXXBXK9t5yFa",
   "metadata": {
    "id": "mXXBXK9t5yFa"
   },
   "source": [
    "### Part 8: Building a decoder body and head\n",
    "\n",
    "A high-level structure for a decoder-only transformer will be implemented in this exercise. Unlike the encoder transformer, the model body and head are not separated in the decoder transformer. Instead, the decoder transformer contains the model head and body. The model body is a stack of decoder layers.\n",
    "\n",
    "#### Instructions\n",
    "* Add the linear layer for the model head inside the TransformerDecoder class.\n",
    "* Apply the last stage of the forward pass through the model head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DGiGIeNm4PG3",
   "metadata": {
    "id": "DGiGIeNm4PG3"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderOnly(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder-only module for next-element prediction.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        num_layers (int): Number of decoder layers.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_ff (int): Dimensionality of the intermediate hidden layer in the feed-forward sub-layer.\n",
    "        dropout (float): Dropout probability.\n",
    "        max_sequence_length (int): Maximum sequence length for positional encoding.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (torch.nn.Embedding): Embedding layer.\n",
    "        positional_encoding (PositionalEncoding): Positional encoding layer.\n",
    "        layers (torch.nn.ModuleList): List of decoder layers.\n",
    "        fc (torch.nn.Linear): Linear layer for next-word prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        \"\"\"\n",
    "        Initialize transformer decoder-only with embedding, positional encoding, decoder layers, and linear layer for next-element prediction.\n",
    "        \"\"\"\n",
    "        super(TransformerDecoderOnly, self).__init__()\n",
    "        self.embedding = # TODO\n",
    "        self.positional_encoding = # TODO\n",
    "        self.layers = nn.ModuleList([# TODO for _ in range(num_layers)])\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = # TODO\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer decoder-only for next-word prediction.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            self_mask (torch.Tensor): Mask tensor for self-attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Log probabilities of the next words in the vocabulary.\n",
    "        \"\"\"\n",
    "        x = # TODO\n",
    "        x = # TODO\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask)\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = # TODO\n",
    "        return # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahPqhTJ06BIP",
   "metadata": {
    "id": "ahPqhTJ06BIP"
   },
   "source": [
    "### Part 9: Testing the decoder transformer\n",
    "\n",
    "A random and simple sequence will be used as an input to the decoder transformer. Obtaining the output without any errors is sufficient for this exercise.\n",
    "\n",
    "The following components are adequate to form a full decoder transformer:\n",
    "* PositionalEncoder\n",
    "* MultiHeadAttention\n",
    "* FeedForwardSublayer\n",
    "* DecoderLayer\n",
    "* TransformerDecoder\n",
    "\n",
    "#### Instructions\n",
    "* Implement the decoder transformer with methods and classes defined before.\n",
    "* Complete the forward pass throughout the entire transformer body and head to obtain and print outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HFMAatSbvg5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFMAatSbvg5e",
    "outputId": "39b1e2ab-97c1-42de-c64b-3526a8b6e529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input sequence\n",
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7],\n",
      "        [0, 1, 2, 3, 4, 5, 6, 7]])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8\n",
    "batch_size = 8\n",
    "d_model = 16\n",
    "num_heads = 1\n",
    "num_layers = 6\n",
    "d_ff = 32\n",
    "sequence_length = 8\n",
    "dropout = 0.1\n",
    "\n",
    "input_sequence = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "# Create a triangular attention mask for causal attention\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n",
    "print(\"The input sequence\")\n",
    "print(input_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qZRP8ddDrRhf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZRP8ddDrRhf",
    "outputId": "e1244bf3-3f90-4ad5-fa07-33ab4c066a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our output:\n",
      "torch.Size([8, 8, 8])\n",
      "The first output we get:\n",
      "tensor([[-1.8165, -1.7808, -2.5037, -3.3011, -2.7044, -1.8818, -1.9559, -1.6627],\n",
      "        [-2.4807, -2.3567, -2.2225, -3.0499, -2.5416, -1.3380, -2.0216, -1.6485],\n",
      "        [-1.9305, -2.3686, -2.0409, -3.0417, -2.7439, -1.5541, -2.7420, -1.4127],\n",
      "        [-1.9213, -2.1896, -1.6013, -3.1951, -2.4969, -1.7887, -2.6118, -1.7367],\n",
      "        [-1.6177, -1.8508, -2.3247, -3.0555, -2.6899, -1.8568, -2.0845, -1.8892],\n",
      "        [-2.3183, -2.0023, -2.7549, -3.2174, -2.8442, -1.4006, -2.1964, -1.3982],\n",
      "        [-1.5329, -1.4114, -2.6585, -3.4570, -3.3868, -1.8595, -2.1438, -2.0256],\n",
      "        [-1.6161, -1.8129, -2.5816, -2.8837, -3.2749, -1.6019, -2.1123, -1.9224]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the decoder transformer\n",
    "decoder = TransformerDecoderOnly(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "output = decoder(input_sequence, self_attention_mask)\n",
    "print(\"Shape of our output:\")\n",
    "print(output.shape)\n",
    "print(\"The first output we get:\")\n",
    "print(output[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4_p3PvxyTyyN",
   "metadata": {
    "id": "4_p3PvxyTyyN"
   },
   "source": [
    "The triangular mask defined here is the causal mask that prohibits the decoder from observing the \"future\" or cheating. For the first element in the sequence, the decoder can only observe the first element; for the second, the second and the first; and for the nth element, the decoder can only observe elements (tokens) up to the nth element, basically the last generated element and all previous ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8IqI7WqJT77t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "8IqI7WqJT77t",
    "outputId": "23456280-bb55-428d-aa95-15582944494b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f51a1798310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(self_attention_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuhjOuxetd_G",
   "metadata": {
    "id": "nuhjOuxetd_G"
   },
   "source": [
    "Let's modify the decoder-transformer to be able to ouput integer values for a given sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EJA3u05WnMfD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJA3u05WnMfD",
    "outputId": "19bdd68f-021a-456e-dfae-989fbd3e93f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Loss: 21.65715789794922\n",
      "Epoch [2/30], Loss: 18.709980010986328\n",
      "Epoch [3/30], Loss: 16.24869155883789\n",
      "Epoch [4/30], Loss: 14.108474731445312\n",
      "Epoch [5/30], Loss: 12.15359878540039\n",
      "Epoch [6/30], Loss: 10.517559051513672\n",
      "Epoch [7/30], Loss: 9.112133026123047\n",
      "Epoch [8/30], Loss: 7.572569847106934\n",
      "Epoch [9/30], Loss: 6.415394306182861\n",
      "Epoch [10/30], Loss: 5.534232139587402\n",
      "Epoch [11/30], Loss: 4.8613481521606445\n",
      "Epoch [12/30], Loss: 4.333537578582764\n",
      "Epoch [13/30], Loss: 3.768571138381958\n",
      "Epoch [14/30], Loss: 2.8923428058624268\n",
      "Epoch [15/30], Loss: 2.962461471557617\n",
      "Epoch [16/30], Loss: 2.2327513694763184\n",
      "Epoch [17/30], Loss: 1.966929316520691\n",
      "Epoch [18/30], Loss: 1.8678109645843506\n",
      "Epoch [19/30], Loss: 2.0587801933288574\n",
      "Epoch [20/30], Loss: 2.022169589996338\n",
      "Epoch [21/30], Loss: 2.1177818775177\n",
      "Epoch [22/30], Loss: 1.5497159957885742\n",
      "Epoch [23/30], Loss: 1.0339434146881104\n",
      "Epoch [24/30], Loss: 0.9906540513038635\n",
      "Epoch [25/30], Loss: 0.60573410987854\n",
      "Epoch [26/30], Loss: 0.6985992193222046\n",
      "Epoch [27/30], Loss: 1.1792875528335571\n",
      "Epoch [28/30], Loss: 0.9224138855934143\n",
      "Epoch [29/30], Loss: 0.8013413548469543\n",
      "Epoch [30/30], Loss: 0.5438991785049438\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderOnly(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        super(TransformerDecoderOnly, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model = d_model, max_len = max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Add a linear layer (head) for next-word prediction\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "        self.fc2 = nn.Linear(vocab_size, 1)\n",
    "\n",
    "    def forward(self, x, self_mask):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, self_mask)\n",
    "\n",
    "        # Apply the forward pass through the model head\n",
    "        x = self.fc2(self.fc(x)) # <-----\n",
    "        return x # <-----\n",
    "\n",
    "vocab_size = 8\n",
    "batch_size = 8\n",
    "d_model = 16\n",
    "num_heads = 1\n",
    "num_layers = 1\n",
    "d_ff = 16\n",
    "sequence_length = 8\n",
    "dropout = 0.1\n",
    "input_sequence = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "input_sequence = input_sequence.long()\n",
    "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n",
    "\n",
    "decoder = TransformerDecoderOnly(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = decoder(input_sequence, self_attention_mask)\n",
    "\n",
    "    # Flatten the output and target tensors to have a consistent shape\n",
    "    output_flat = output.view(-1)\n",
    "    target_flat = input_sequence.view(-1)\n",
    "    # Compute the loss\n",
    "    loss = criterion(output_flat, target_flat.float())\n",
    "\n",
    "    # Backpropagation and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0q0tCsqTepwi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0q0tCsqTepwi",
    "outputId": "42807b2d-0fdd-49dd-fbb7-dd73901e19d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 3, 4, 4, 4, 6, 0, 1, 1, 2, 3, 3, 6, 5, 1, 0, 2, 3, 4, 4, 5, 5,\n",
       "        0, 0, 1, 3, 2, 4, 6, 6, 1, 1, 1, 1, 3, 4, 6, 6, 0, 1, 2, 3, 4, 3, 6, 6,\n",
       "        1, 1, 2, 3, 4, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 5], dtype=torch.int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_flat.int()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l3cvzEpmBwRE",
   "metadata": {
    "id": "l3cvzEpmBwRE"
   },
   "source": [
    "This is a very dummy and simple way of adapting a transformer model to requirements. It is recommended to adapt the transformer further, and output a sequence just like the input sequence (0,1,2,3,...) or output the next element for a given sequence (input: 0,1,2, output: **3**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z-M0a32yA8yR",
   "metadata": {
    "id": "Z-M0a32yA8yR"
   },
   "source": [
    "## Encoder-Decoder Transformer\n",
    "It is useful for translation tasks where a given input text is required to be translated into an output in a target language. T5 (Text-to-Text Transfer Transformer) is a well-known encoder-decoder transformer that can also do summarization, question answering, and translation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZBaL5H4lKLlY",
   "metadata": {
    "id": "ZBaL5H4lKLlY"
   },
   "source": [
    "### Part 10: Incorporating cross-attention in a decoder\n",
    "\n",
    "In an encoder-decoder transformer, decoder layers incorporate two attention mechanisms: the causal attention inherent to any transformer decoder and a cross-attention that integrates source sequence information processed by the encoder with the target sequence information being processed through the decoder.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/Cross_attention.png?raw=true\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Modify the DecoderLayer class to incorporate this two-fold attention scheme.\n",
    "\n",
    "#### Instructions\n",
    "* Initialize the two attention mechanisms used in an encoder-decoder transformer's decoder layer: causal (masked) self-attention and cross-attention.\n",
    "* Pass the necessary input arguments (query, key, values, and mask) to the two attention stages in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7drUCHLr6Kb5",
   "metadata": {
    "id": "7drUCHLr6Kb5"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder layer module.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_ff (int): Dimensionality of the intermediate hidden layer in the feed-forward sub-layer.\n",
    "        dropout (float): Dropout probability.\n",
    "\n",
    "    Attributes:\n",
    "        self_attn (MultiHeadAttention): Multi-head self-attention mechanism.\n",
    "        cross_attn (MultiHeadAttention): Multi-head cross-attention mechanism.\n",
    "        feed_forward (FeedForwardSubLayer): Feed-forward sub-layer.\n",
    "        norm1 (torch.nn.LayerNorm): Layer normalization for the first sub-layer.\n",
    "        norm2 (torch.nn.LayerNorm): Layer normalization for the second sub-layer.\n",
    "        norm3 (torch.nn.LayerNorm): Layer normalization for the third sub-layer.\n",
    "        dropout (torch.nn.Dropout): Dropout layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Initialize decoder layer with multi-head self-attention, multi-head cross-attention, and feed-forward sub-layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the input and output feature vectors.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimensionality of the intermediate hidden layer in the feed-forward sub-layer.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Initialize the causal (masked) self-attention and cross-attention\n",
    "        self.self_attn = # TODO\n",
    "        self.cross_attn = # TODO\n",
    "        self.feed_forward = # TODO\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            causal_mask (torch.Tensor): Mask tensor for causal self-attention mechanism.\n",
    "            encoder_output (torch.Tensor): Encoder output tensor.\n",
    "            cross_mask (torch.Tensor): Mask tensor for cross-attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after applying multi-head self-attention, multi-head cross-attention, and feed-forward sub-layer.\n",
    "        \"\"\"\n",
    "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
    "        self_attn_output = # TODO\n",
    "        x = # TODO\n",
    "        cross_attn_output = # TODO\n",
    "        # (query, key, values, and mask)\n",
    "        x = # TODO\n",
    "        ff_output = # TODO\n",
    "        x = # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l_m2lLe35zyd",
   "metadata": {
    "id": "l_m2lLe35zyd"
   },
   "source": [
    "### Part 11: Defining Decoder Layer\n",
    "\n",
    "The decoder layer is to be built in this part. What is the main difference of this part and part 7?\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "* A multi-headed self-attention mechanism.\n",
    "* A feed-forward sublayer.\n",
    "* Normalization and dropout are to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xJgCmGfv5t1h",
   "metadata": {
    "id": "xJgCmGfv5t1h"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder module.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        d_model (int): Dimensionality of the input and output feature vectors.\n",
    "        num_layers (int): Number of decoder layers.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        d_ff (int): Dimensionality of the intermediate hidden layer in the feed-forward sub-layer.\n",
    "        dropout (float): Dropout probability.\n",
    "        max_sequence_length (int): Maximum sequence length for positional encoding.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (torch.nn.Embedding): Embedding layer.\n",
    "        positional_encoding (PositionalEncoding): Positional encoding layer.\n",
    "        layers (torch.nn.ModuleList): List of decoder layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
    "        \"\"\"\n",
    "        Initialize transformer decoder with embedding, positional encoding, and decoder layers.\n",
    "        \"\"\"\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model = d_model, max_len = max_sequence_length)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            causal_mask (torch.Tensor): Mask tensor for causal self-attention mechanism.\n",
    "            encoder_output (torch.Tensor): Encoder output tensor.\n",
    "            cross_mask (torch.Tensor): Mask tensor for cross-attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after decoding.\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, causal_mask, encoder_output, cross_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XEr8387SKV65",
   "metadata": {
    "id": "XEr8387SKV65"
   },
   "source": [
    "### Part 12: Trying out an encoder-decoder transformer\n",
    "\n",
    "Encoder-decoder transformers can be used for translation tasks. Here we are going to demonstrate the ability of adapting a transformer for this specific task. Basically, we are going to feed a sequence and expect to obtain another sequence. We can think of this as sentence in the source language as the input and in the target language as the output. For this task, we are going to have an input sequence, that will be ***translated*** into a target sequence, this translation is effectively used in image domain as well. We can think of translation as a task that maps certain values to target values.\n",
    "\n",
    "Remember that we are only testing a yet-to-be-trained transformer architecture, hence the use of random input sequences.\n",
    "\n",
    "The following components are required to form a full encoder-decoder transformer:\n",
    "* MultiHeadAttention\n",
    "* FeedForwardSubLayer\n",
    "* PositionalEncoding\n",
    "* EncoderLayer\n",
    "* DecoderLayer\n",
    "* TransformerEncoder\n",
    "* TransformerDecoder\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* Create a batch of random input sequences of size batch_size X sequence_length.\n",
    "* Instantiate the two transformer bodies using the appropriate class names.\n",
    "* Pass the necessary masks as arguments to the encoder and the decoder for their underlying attention mechanisms; each mask argument should be added in the same order they are utilized inside the encoder or decoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PF-GaVkHKAQK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PF-GaVkHKAQK",
    "outputId": "815c09e8-bcfb-425c-fced-a73d230ac5a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch's output shape:  torch.Size([4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8\n",
    "batch_size = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "d_ff = 16\n",
    "sequence_length = 8\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "# Create a batch of random input sequences\n",
    "#input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
    "\n",
    "input_sequence = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the two transformer bodies\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "\n",
    "# Pass the necessary masks as arguments to the encoder and the decoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gK_WUwsNU2ZH",
   "metadata": {
    "id": "gK_WUwsNU2ZH"
   },
   "source": [
    "Let's check the causal mask for the Encoder-Decoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ypAizhx3U2hF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "ypAizhx3U2hF",
    "outputId": "cdb8857e-1970-4e85-ed41-6e8f2b004aa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f51a1837340>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(causal_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MScLkKJvLqTH",
   "metadata": {
    "id": "MScLkKJvLqTH"
   },
   "source": [
    "As we have seen in Decoder-Transformer, the causal mask is again a diagonal matrix that disables the capability of the Decoder part to see the future elements in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JiOoHZxXZn24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JiOoHZxXZn24",
    "outputId": "baa1ac6c-ebfe-4bdc-bbaa-66ed62053160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "Target sequence (Translated)\n",
      "tensor([6, 7, 0, 1, 2, 3, 4, 5])\n",
      "Epoch [1/10], Loss: 2.697746992111206\n",
      "Epoch [2/10], Loss: 2.659696578979492\n",
      "Epoch [3/10], Loss: 2.6397345066070557\n",
      "Epoch [4/10], Loss: 2.6122608184814453\n",
      "Epoch [5/10], Loss: 2.5470809936523438\n",
      "Epoch [6/10], Loss: 2.5937390327453613\n",
      "Epoch [7/10], Loss: 2.6101434230804443\n",
      "Epoch [8/10], Loss: 2.595712900161743\n",
      "Epoch [9/10], Loss: 2.3930373191833496\n",
      "Epoch [10/10], Loss: 2.5007448196411133\n",
      "Batch's output shape:  torch.Size([4, 8, 8])\n",
      "Output sequence:\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=0.001)\n",
    "target_sequence = input_sequence.roll(2)\n",
    "print(\"Input sequence\")\n",
    "print(input_sequence[0])\n",
    "print(\"Target sequence (Translated)\")\n",
    "print(target_sequence[0])\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    encoder_output = encoder(input_sequence, padding_mask)\n",
    "    decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "    loss = criterion(decoder_output.view(-1, vocab_size), target_sequence.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print(\"Batch's output shape: \", decoder_output.shape)\n",
    "print(\"Output sequence:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wG0okHPJRmDf",
   "metadata": {
    "id": "wG0okHPJRmDf"
   },
   "source": [
    "The decoder output is shown below. What is missing from this implementation? Where is the output we desire?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pyKLxDa_RjcO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pyKLxDa_RjcO",
    "outputId": "c6e7300f-3ab0-4c57-b914-35af6dc548c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1518,  1.1863, -1.7123,  0.2617,  1.2484, -0.6863, -1.0119,  0.5562],\n",
      "        [ 1.4876, -0.4550, -1.3039,  1.5357, -0.2532,  0.3253, -1.0567, -0.2778],\n",
      "        [ 0.4609,  0.5789, -2.2267,  1.2172, -0.3164,  0.4919, -0.5744,  0.3647],\n",
      "        [-0.6704, -0.1011,  0.4295,  0.3604, -0.2925,  2.0598, -1.6335, -0.1625],\n",
      "        [-1.4510,  0.3150, -0.6059,  0.8028,  1.6131,  0.3211, -1.3235,  0.3137],\n",
      "        [-1.0165, -0.5673, -1.1851,  1.0282,  0.1646,  1.2266, -0.9146,  1.2520],\n",
      "        [-1.6267,  1.5212,  0.3601,  0.7630, -0.0149,  0.1874, -1.4380,  0.2322],\n",
      "        [ 0.4781, -0.0560, -1.0289,  1.1301, -0.5303,  0.8180, -1.8094,  0.9944]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bCEVpvXKSDIW",
   "metadata": {
    "id": "bCEVpvXKSDIW"
   },
   "source": [
    "Hint: we have missed a final touch for the encoder-decoder transformer architecture, which is the final head on the decoder side. By changing it we can adapt our architecture to many different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sqkCyerGSiWu",
   "metadata": {
    "id": "sqkCyerGSiWu"
   },
   "outputs": [],
   "source": [
    "class TranslatorHead(nn.Module):\n",
    "    def __init__(self, d_model, sequence_length):\n",
    "        super(TranslatorHead, self).__init__()\n",
    "        # Add linear layer for translation\n",
    "        self.fc = nn.Linear(d_model, sequence_length)\n",
    "        self.fc2 = nn.Linear(sequence_length, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MnoIGitySVRO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MnoIGitySVRO",
    "outputId": "0851d56e-bbf8-4dde-ade7-353294ec8b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch's output shape:  torch.Size([4, 8, 8])\n",
      "Input sequence\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "torch.Size([4, 8])\n",
      "Target sequence (Translated)\n",
      "tensor([7, 0, 1, 2, 3, 4, 5, 6])\n",
      "Epoch [1/20], Loss: 15.305901527404785\n",
      "Epoch [2/20], Loss: 8.699307441711426\n",
      "Epoch [3/20], Loss: 5.636639595031738\n",
      "Epoch [4/20], Loss: 5.317410469055176\n",
      "Epoch [5/20], Loss: 7.022207260131836\n",
      "Epoch [6/20], Loss: 7.324475288391113\n",
      "Epoch [7/20], Loss: 6.874665260314941\n",
      "Epoch [8/20], Loss: 5.710813045501709\n",
      "Epoch [9/20], Loss: 5.530500888824463\n",
      "Epoch [10/20], Loss: 5.394636154174805\n",
      "Epoch [11/20], Loss: 5.2456889152526855\n",
      "Epoch [12/20], Loss: 5.221963405609131\n",
      "Epoch [13/20], Loss: 5.68686580657959\n",
      "Epoch [14/20], Loss: 5.580173492431641\n",
      "Epoch [15/20], Loss: 5.707486152648926\n",
      "Epoch [16/20], Loss: 5.3308868408203125\n",
      "Epoch [17/20], Loss: 5.0327606201171875\n",
      "Epoch [18/20], Loss: 5.468961715698242\n",
      "Epoch [19/20], Loss: 5.361144542694092\n",
      "Epoch [20/20], Loss: 5.461234092712402\n",
      "Output shape:  torch.Size([4, 8, 1])\n",
      "Output sequence:\n",
      "tensor([4, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 4, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 4, 3, 4, 3, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8\n",
    "batch_size = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "num_layers = 4\n",
    "d_ff = 16\n",
    "sequence_length = 8\n",
    "dropout = 0.2\n",
    "\n",
    "input_sequence = torch.arange(0, sequence_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "\n",
    "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
    "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
    "\n",
    "# Instantiate the two transformer bodies\n",
    "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
    "translator_head = TranslatorHead(d_model, vocab_size)\n",
    "\n",
    "# Pass the necessary masks as arguments to the encoder and the decoder\n",
    "encoder_output = encoder(input_sequence, padding_mask)\n",
    "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "print(\"Batch's output shape: \", decoder_output.shape)\n",
    "translator_output = translator_head(decoder_output)\n",
    "# Loss and Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1)\n",
    "target_sequence = input_sequence.roll(1)\n",
    "print(\"Input sequence\")\n",
    "print(input_sequence[0])\n",
    "print(input_sequence.shape)\n",
    "print(\"Target sequence (Translated)\")\n",
    "print(target_sequence[0])\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    encoder_output = encoder(input_sequence, padding_mask)\n",
    "    decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
    "    translator_output = translator_head(decoder_output)\n",
    "    loss = criterion(translator_output.view(-1), target_sequence.view(-1).float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "print(\"Output shape: \", translator_output.shape)\n",
    "print(\"Output sequence:\")\n",
    "print(translator_output.view(-1).int())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfGTqC7BLquJ",
   "metadata": {
    "id": "cfGTqC7BLquJ"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this exercise, we have seen the vanilla transformer from the paper Attention Is All You Need by Vaswani et al. and implemented its components from scratch with PyTorch. Experimenting with different hyperparameters that define the deep learning model architecture or training structure is highly recommended as an extra follow-up exercise!\n",
    "\n",
    "We have seen three different implementations of transformers, namely the Encoder-Transformer, Decoder-Transformer and Encoder-Decoder (Vanilla) Transformer. We built them with basic building blocks and tested their input-output pipeline. We have also tested a very brief dummy training example that demonstrates the learning capability to some extent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2h9D82qP3GHp",
   "metadata": {
    "id": "2h9D82qP3GHp"
   },
   "source": [
    "# Questions\n",
    "1. Why do we implement PositionalEncoding in part 1?\n",
    "1. How do we feed Querry, Key, and Value to the multi-head attention blocks?\n",
    "1. What is the purpose of the mask (self_attention_mask) defined in Part 9?\n",
    "1. Why do we use split heads in the attention mechanism?\n",
    "1. What is the difference between self-attention and cross-attention?\n",
    "1. Where exactly is the cross-attention mask applied in the vanilla transformer architecture?\n",
    "1. Which of these (Q, K, and V) is supplied from the Encoder to the cross-attention in the Encoder-Decoder transformer? And which from the decoder's attention?\n",
    "1. Why are we shifting outputs to the right (in the vanilla Transformer architecture)?\n",
    "1. What can we do to improve the training process in Parts 6, 9, and 12? Which mechanisms and architecture alterations can enhance the results?\n",
    "1. What are some possible use-case scenarios for Encoder-only Transformer, Decoder-only Transformer and Encoder-Decoder Transformer?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vQypvqNHYw_x",
   "metadata": {
    "id": "vQypvqNHYw_x"
   },
   "source": [
    "# References\n",
    "References and further tutorials to check out:\n",
    "1. The paper, Attention is all you need, the transformer paper that introduced a whole new way of approaching deep learning: https://arxiv.org/abs/1706.03762\n",
    "1. A solid blog that explains the Transformer architecture and its sub components: https://buomsoo-kim.github.io/attention/2020/04/19/Attention-mechanism-17.md/\n",
    "1. Training a Transformer model, with a real dataset: https://buomsoo-kim.github.io/attention/2020/04/20/Attention-mechanism-18.md/\n",
    "1. Further explanations on top of previous posts https://buomsoo-kim.github.io/attention/2020/04/21/Attention-mechanism-19.md/\n",
    "1. Another Transformer tutorial: https://towardsdatascience.com/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "1. The annotated Transformer, updated for newer version of PyTorch, an excellent guide that uses the original authors sentences and implement everything (just like we did in this tutorial): https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "1. PyTorch's documentation for Transformers and other helper libraries: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "1. Dive into Deep Learning tutorial that uses their version of libraries, a different take on Transformer model implementation: https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
