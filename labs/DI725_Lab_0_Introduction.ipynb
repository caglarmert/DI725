{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caglarmert/DI725/blob/main/labs/DI725_Lab_0_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3745686a-405f-4729-a239-ffd17f6c5087",
      "metadata": {
        "id": "3745686a-405f-4729-a239-ffd17f6c5087"
      },
      "source": [
        "# DI 725: Transformers and Attention-Based Deep Networks\n",
        "\n",
        "## An End-to-End Tutorial for Implementing Transformers\n",
        "\n",
        "### Authors:\n",
        "* ttemizel@metu.edu.tr\n",
        "* atemizel@metu.edu.tr\n",
        "* mecaglar@metu.edu.tr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f81a53-355d-456d-90d1-9b5432dd1ce7",
      "metadata": {
        "id": "c0f81a53-355d-456d-90d1-9b5432dd1ce7"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5125f78-dea0-4c0e-ab42-5978b7b2633e",
      "metadata": {
        "id": "c5125f78-dea0-4c0e-ab42-5978b7b2633e"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/attention_research_1.png?raw=true\" width=\"400\"/>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "### Transformer Architecture Overview\n",
        "\n",
        "The Transformer architecture revolutionized the field of natural language processing (NLP) by introducing a model that relies entirely on self-attention mechanisms, eliminating the need for recurrent or convolutional layers. Here's a brief overview of the main components of the Transformer architecture:\n",
        "\n",
        "#### 1. Input Embeddings\n",
        "- The input sequence, typically a sequence of word embeddings, is passed into the model. Each word is represented as a high-dimensional vector, often initialized randomly or pre-trained on a large corpus.\n",
        "\n",
        "#### 2. Positional Encoding\n",
        "- Since the Transformer doesn't inherently understand the order of tokens in a sequence, positional encodings are added to the input embeddings to provide information about token positions. These are usually sinusoidal functions of different frequencies and phases.\n",
        "\n",
        "#### 3. Encoder\n",
        "- The Encoder consists of multiple identical layers (usually 6-12). Each layer consists of two main sub-components:\n",
        "  - **Multi-Head Self-Attention Mechanism**: Computes attention weights between all pairs of words in the input sequence to capture relationships and dependencies among them.\n",
        "  - **Feedforward Neural Network**: Applies a fully connected feedforward network to each position separately and identically. It processes the output of the attention mechanism in a position-wise manner.\n",
        "\n",
        "#### 4. Decoder\n",
        "- The Decoder also consists of multiple identical layers (the same number as in the Encoder). Each layer in the Decoder has three main sub-components:\n",
        "  - **Masked Multi-Head Self-Attention Mechanism**: Similar to the Encoder's attention mechanism, but with a mask applied to prevent positions from attending to subsequent positions, ensuring that the model attends only to previous positions during generation.\n",
        "  - **Encoder-Decoder Attention Mechanism**: Allows the Decoder to focus on different parts of the input sequence (Encoder's output) by computing attention scores between the current position in the Decoder and all positions in the Encoder's output.\n",
        "  - **Feedforward Neural Network**: Similar to the Encoder, a fully connected feedforward network is applied to each position separately and identically.\n",
        "\n",
        "#### 5. Output Layer\n",
        "- The output of the final Decoder layer is passed through a linear layer followed by a softmax function to produce the probability distribution over the output vocabulary. During training, this distribution is compared to the actual target sequence using cross-entropy loss.\n",
        "\n",
        "#### 6. Loss Computation\n",
        "- The model's output is compared to the actual target sequence using cross-entropy loss. This comparison drives the learning process through backpropagation.\n",
        "\n",
        "The Transformer architecture's key innovation lies in its ability to capture long-range dependencies in sequences efficiently through self-attention mechanisms, making it highly parallelizable and scalable compared to traditional recurrent neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "The Transformer architecture, introduced in the paper \"Attention is All You Need\", revolutionized natural language processing by relying solely on attention mechanisms instead of recurrent connections. Here's a breakdown of its key components:\n",
        "\n",
        "Overall Structure:\n",
        "\n",
        "An encoder processes the input sequence to capture its meaning.\n",
        "A decoder generates the output sequence based on the encoded representation and any additional context.\n",
        "Encoder and Decoder Blocks:\n",
        "\n",
        "Both the encoder and decoder consist of multiple identical encoder blocks and decoder blocks, respectively.\n",
        "Each block has two sub-blocks:\n",
        "Multi-head Self-attention: Captures relationships between elements within the sequence (encoder) or within the previously generated output (decoder).\n",
        "Feed-forward network: Adds non-linearity and complexity to the model.\n",
        "Residual connection and Layer Norm: Improve training stability and gradient flow.\n",
        "Key Details of Each Block:\n",
        "\n",
        "1. Multi-head Self-attention:\n",
        "\n",
        "Splits the input into queries, keys, and values.\n",
        "Computes attention scores based on the similarity between queries and keys.\n",
        "Masks out padded elements using attention masks.\n",
        "Aggregates values weighted by the attention scores, resulting in a context vector for each element.\n",
        "The \"multi-head\" part refers to performing this self-attention mechanism multiple times with different query and key projections, capturing diverse relationships.\n",
        "2. Feed-forward network:\n",
        "\n",
        "A two-layer network with ReLU activation for non-linearity.\n",
        "Adds complexity and allows the model to learn more intricate relationships.\n",
        "3. Residual connection and Layer Norm:\n",
        "\n",
        "Shortcuts around each sub-block are added to ensure the gradients can flow easily through the network.\n",
        "Layer normalization rescales and shifts the output of each sub-block, stabilizing the training process.\n",
        "Additional Components:\n",
        "\n",
        "Encoder-decoder attention: In a sequence-to-sequence setting, the decoder attends to the encoded representation in each block to incorporate context into the generated output.\n",
        "Positional encoding: Since the Transformer doesn't have inherent positional information, additional embeddings are added to encode the relative positions of elements in the sequence.\n",
        "Output layer: In the decoder, a final layer converts the internal representation into the final vocabulary probabilities for output generation.\n",
        "Benefits of Transformers:\n",
        "\n",
        "Parallelization: Attention allows for better parallelization during training compared to recurrent models.\n",
        "Long-range dependencies: Can capture long-range dependencies in sequences without relying on sequential processing.\n",
        "Adaptability: Can be applied to various NLP tasks with minor modifications.\n",
        "Drawbacks of Transformers:\n",
        "\n",
        "Computational cost: Attention can be computationally expensive, especially for long sequences.\n",
        "Memory intensive: Requires storing the entire input sequence for attention computations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "In this part we import the required libraries. Running this part on the Colab servers is required for later parts. It is advised to check the associated python requirements.txt, that is frozen at the time of preparation of this notebook, in case of any library or version error occurs while running this notebook. Mind that installing everything locally via pip install -r \"requirements.txt\" is not advised though, mainly because of the discrepancies between Colab and locally available machine."
      ],
      "metadata": {
        "id": "9bx_lEEFJI5N"
      },
      "id": "9bx_lEEFJI5N"
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment any install if needed. It is recommended that these installations\n",
        "# are performed prior to any notebook runs and imports\n",
        "\n",
        "# !pip install datasets # Huggingface dataset library\n",
        "# !pip install evaluate # Used for evaluation metrics\n",
        "# !pip install rouge_score # Is a text evaluation metric\n",
        "# !pip install trl #Transformers Reinforcement Learning framework\n",
        "# !pip install sacremoses # Used for specific characters, useful for languages like Turkish"
      ],
      "metadata": {
        "id": "yHryLjtEPxYd"
      },
      "id": "yHryLjtEPxYd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "70a9b2e0-fdb5-4697-a48f-44d1a08503f8",
      "metadata": {
        "id": "70a9b2e0-fdb5-4697-a48f-44d1a08503f8"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import evaluate\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead, create_reference_model\n",
        "from trl.core import respond_to_batch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After importing the main libraries, we can continue with the transformers. First lets check what does the above import does. We have imported pipeline from transformers library, from huggingface 🤗.\n",
        "\n",
        "The [documentation](https://huggingface.co/docs/transformers) for the Transformers library.\n",
        "\n",
        "The [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) is a class of the Transformers library. It is used for easy inference, abstracts most of the complexity and offers simple API for some dedicated tasks.\n",
        "\n",
        "The [torch](https://pytorch.org/) is a popular and diverse machine learning framework, enabling low level implementation (as low as it gets with Python anyway). The Neural Networks (nn) is a library within PyTorch that enables operations with neural network structures.\n",
        "\n",
        "The [Auto](https://huggingface.co/docs/transformers/model_doc/auto) classes contain many high level methods and models for various specific tasks, sometimes required for a pre-processing step such as tokenizers.\n",
        "\n",
        "The [datasets](https://huggingface.co/docs/datasets) is the 🤗 library used for datasets (who would have guess?). Tabular, Audio, Computer Vision, and Text data can be loaded or shared via this library.\n",
        "\n",
        "The [TRL](https://huggingface.co/docs/trl) (standing for: Transformer Reinforcement Learning) is the comprehensive toolkit designed for training transformer language models using Reinforcement Learning. It encompasses a range of tools, starting from the initial Supervised Fine-tuning (SFT) phase, through Reward Modeling (RM), up to the Proximal Policy Optimization (PPO) stage."
      ],
      "metadata": {
        "id": "QMDfs6xsJwj3"
      },
      "id": "QMDfs6xsJwj3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 1: Introduction"
      ],
      "metadata": {
        "id": "1p9wMsy8xR_L"
      },
      "id": "1p9wMsy8xR_L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Transformers\n",
        "\n",
        "In this first introductory section, we begin with experiencing basic and very high level usage of transformers."
      ],
      "metadata": {
        "id": "uO_FQhzqAnc0"
      },
      "id": "uO_FQhzqAnc0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.1: Classifying a text\n",
        "\n",
        "Huggingface Hub is an open-source public colaboration of various models. Large Language Models, require tremendous amount of training data and time, thus once trained they are invaluable and their inference can be adapted to various use-cases.\n",
        "\n",
        "This first practice will be about loading a model from the huggingface hub, into a pipeline, to perform a task.\n",
        "\n",
        "It is important to note that model loading with specific model name is advised or else it will opt to defaults.\n",
        "\n",
        "#### Instructions\n",
        "* Import the necessary function from the transformers library to load Hugging Face LLMs as pipelines.\n",
        "* Load the model specified in model_name into a suitable pipeline for sentiment classification in text.\n",
        "* Pass the customer review defined in prompt to the pipeline to get a sentiment prediction."
      ],
      "metadata": {
        "id": "_pGAfQoPqyLh"
      },
      "id": "_pGAfQoPqyLh"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "380575a2-84dd-4d94-bf60-83fbd3114e47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "380575a2-84dd-4d94-bf60-83fbd3114e47",
        "outputId": "a9f6c45a-761b-4dee-dc4c-c01f5983bc51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I liked Atakule, very much so because of the excellent location in the midst of the botanical park and city center. \n",
            "Sentiment: positive Score: 0.8559050559997559\n",
            "There was nothing to see at Atakule, the building is under construction, you can't go into building, wasting my afternoon time in ankara. \n",
            "Sentiment: negative Score: 0.4448523223400116\n"
          ]
        }
      ],
      "source": [
        "# Specify the task name\n",
        "task_name = \"text-classification\"\n",
        "# Specify the model to be loaded\n",
        "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
        "# We can change the model name to\n",
        "# \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
        "# \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
        "classifier = pipeline(task = task_name, model = model_name)\n",
        "\n",
        "# Clearly this is a positive sentiment from a 5 star review tripadvisor for Atakule\n",
        "prompt = \"I liked Atakule, very much so because of the excellent location in the midst of the botanical park and city center.\"\n",
        "prediction = classifier(prompt)\n",
        "print(prompt, \"\\nSentiment:\", prediction[0][\"label\"], \"Score:\",prediction[0][\"score\"],)\n",
        "\n",
        "# And a negative one, 1 star review from the time it is off-limits.\n",
        "prompt = \"There was nothing to see at Atakule, the building is under construction, you can't go into building, wasting my afternoon time in ankara.\"\n",
        "prediction = classifier(prompt)\n",
        "print(prompt, \"\\nSentiment:\", prediction[0][\"label\"], \"Score:\",prediction[0][\"score\"],)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.2: Summarizing a text\n",
        "\n",
        "Summarization is a challanging language task, requires sequence-to-sequence models, such as the one we are using here. The task is about summarizing a given long text.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "* Load the model, based on the T5 transformer architecture and specified in model_name, into a text summarization pipeline.\n",
        "* Pass long_text to the model pipeline, to produce a summary limited to 50 tokens length.\n",
        "* Access and print the summarized text in outputs."
      ],
      "metadata": {
        "id": "4O9u0ZxKrJ4C"
      },
      "id": "4O9u0ZxKrJ4C"
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify a model name, note that we are using a small version so don't expect much\n",
        "model_name = \"cnicu/t5-small-booksum\"\n",
        "# Provide the long text\n",
        "long_text = \"Tunali hilmi, which is a bustling street, is a hub for various commercial activities as it extends southwards toward Kugulu Park. Tunali Hilmi Avenue is regarded as one of the city's most charming streets, adorned with a variety of shops, boutiques, and souvenir stores. The neighborhood exudes a sense of luxury and offers a wide range of goods, albeit at slightly higher prices compared to other areas. However, the elevated cost is justified by the high-quality shopping experience, particularly appealing to those who enjoy outdoor retail therapy.\"\n",
        "\n",
        "# Load the model pipeline for text summarization\n",
        "summarizer = pipeline(task=\"summarization\", model=model_name)\n",
        "\n",
        "# Pass the long text to the model to summarize it\n",
        "outputs = summarizer(long_text, max_length=50)\n",
        "\n",
        "# Access and print the summarized text in the outputs variable\n",
        "print(\"Original Text: \", long_text, \"\\nSummary Text: \", outputs[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5JQrMZlrXnp",
        "outputId": "3ad71cfd-f91c-47b5-facd-b92efa02120f"
      },
      "id": "X5JQrMZlrXnp",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:  Tunali hilmi, which is a bustling street, is a hub for various commercial activities as it extends southwards toward Kugulu Park. Tunali Hilmi Avenue is regarded as one of the city's most charming streets, adorned with a variety of shops, boutiques, and souvenir stores. The neighborhood exudes a sense of luxury and offers a wide range of goods, albeit at slightly higher prices compared to other areas. However, the elevated cost is justified by the high-quality shopping experience, particularly appealing to those who enjoy outdoor retail therapy. \n",
            "Summary Text:  Tunali hilmi is regarded as one of the city's most charming streets, adorned with shops, boutiques, and souvenir stores. The neighborhood offers a wide range of goods, albeit at slightly higher prices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.3: Translating a text\n",
        "\n",
        "Translation is another challanging language task, requiring models trained specifically for source and target languages.\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "* Define a pipeline for Turkish-to-English translation, specifying the source and target languages in the pipeline task argument.\n",
        "* Translate the text in input_text using the pipeline.\n",
        "* Access and print the translated text in the outputs variable: translations."
      ],
      "metadata": {
        "id": "lmlcsdX74Erm"
      },
      "id": "lmlcsdX74Erm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the model name, from Turkish (tr) to English (en)\n",
        "model_name = \"Helsinki-NLP/opus-mt-tr-en\"\n",
        "\n",
        "# A short intro about METU\n",
        "input_text = \"Orta Doğu Teknik Üniversitesi, Türkiye ve Orta Doğu ülkelerinin kalkınmalarına katkıda bulunmak, özellikle fen bilimleri ve sosyal bilimler alanlarında uzman yetiştirmek üzere 15 Kasım 1956 tarihinde Orta Doğu Yüksek Teknoloji Enstitüsü adıyla eğitime başlamıştır. \"\n",
        "\n",
        "# Define pipeline for Spanish-to-English translation\n",
        "translator = pipeline(\"translation_tr_to_en\", model=model_name)\n",
        "\n",
        "# Translate the input text\n",
        "translations = translator(input_text)\n",
        "\n",
        "# Access the output to print the translated text in English\n",
        "print(\"Original text: \", input_text)\n",
        "print(\"Translated text:\", translations[0]['translation_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-DbjnhJEL9",
        "outputId": "67c6eaea-9c82-4b8a-d0cd-95b89af1fa94"
      },
      "id": "HT-DbjnhJEL9",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:  Orta Doğu Teknik Üniversitesi, Türkiye ve Orta Doğu ülkelerinin kalkınmalarına katkıda bulunmak, özellikle fen bilimleri ve sosyal bilimler alanlarında uzman yetiştirmek üzere 15 Kasım 1956 tarihinde Orta Doğu Yüksek Teknoloji Enstitüsü adıyla eğitime başlamıştır. \n",
            "Translated text: The Middle East Technical University began training as the Middle East Institute of Technology on 15 November 1956 to contribute to the development of Turkey and Middle East countries, especially to develop experts in science and social sciences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.4: Question-Answering\n",
        "Next, let's practice loading a Hugging Face LLM into a pipeline for question-answering (QA, for short). This time, you will use the default model supplied by Hugging Face transformers library for QA pipelines.\n",
        "\n",
        "#### Instructions\n",
        "* Instantiate a pipeline for question-answering.\n",
        "* Pass the necessary pieces of information as inputs to the pipeline.\n",
        "* Access and print the extracted answer in the outputs variable."
      ],
      "metadata": {
        "id": "FXe1dPXn75Lu"
      },
      "id": "FXe1dPXn75Lu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model pipeline for question-answering\n",
        "model_name = \"distilbert-base-cased-distilled-squad\"\n",
        "\n",
        "qa_model = pipeline(\"question-answering\",model=model_name)\n",
        "\n",
        "# Provide the context\n",
        "context = \"The history of Ankara Castle, one of the symbols of the province, is as old as the history of the city. It remains to be determined when the castle, which existed when the Galatians settled in Ankara and was repaired during the Roman period, was built. Next to the hill on which it was founded, that is, Hatip Stream, is 110 m above the Bent Stream. The castle has more than 20 towers. The outer castle surrounds Ankara in the shape of a heart. The four-storey inner castle is made of Ankara Stone and partly of collected stones. The inner castle has two large gates, one is called the Outer Gate and the other is the Citadel Gate. There is a book belonging to the Ilkhanate on this door. The inner castles consist of a total of 42 pentagonal towers with a length of 14-16 m. There is an inscription in the northwestern part showing the repairs made by the Seljuk ruler.\"\n",
        "\n",
        "# Provide the questions\n",
        "questions = [\"How many towers does the Ankara castle have?\",\n",
        "             \"When did the Ankara castle was build?\",\n",
        "             \"How long are the towers in the inner castle?\",\n",
        "             \"Who repaired the Ankara castle and inscribed?\",\n",
        "             \"What are the materials of the Ankara castle?\"]\n",
        "\n",
        "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
        "outputs = qa_model(question=questions, context=context)\n",
        "\n",
        "# Access and print the answer\n",
        "for i in range(len(questions)):\n",
        "  print(\"Question: \", questions[i], \"\\nAnswer:\", outputs[i]['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgOC4JFqp1Kf",
        "outputId": "4cb5966b-0f20-4698-9bed-f825136bd8f7"
      },
      "id": "AgOC4JFqp1Kf",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  How many towers does the Ankara castle have? \n",
            "Answer: more than 20\n",
            "Question:  When did the Ankara castle was build? \n",
            "Answer: Roman period\n",
            "Question:  How long are the towers in the inner castle? \n",
            "Answer: 14-16 m\n",
            "Question:  Who repaired the Ankara castle and inscribed? \n",
            "Answer: the Seljuk ruler\n",
            "Question:  What are the materials of the Ankara castle? \n",
            "Answer: Ankara Stone and partly of collected stones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1.5: Text Generation\n",
        "\n",
        "Text generation, is the most famous application of transformers, namely ChatGPT (standing for Generative Pre-Trained). Here we will use an older version (GPT-2) to generate text for customers leaving reviews for our business on a public website.\n",
        "\n",
        "#### Instructions\n",
        "* Instantiate the generator variable as a pipeline that loads the \"gpt2\" pre-trained text generation model.\n",
        "* Build a prompt for the LLM that concatenates the customer review with the hotel response's initial sentence.\n",
        "* Pass the prompt to the previously defined pipeline to generate (inference) the following text in the hotel response, specifying a maximum length of 150 tokens for the generated output.\n",
        "* Print the generated output."
      ],
      "metadata": {
        "id": "-DyWNJwuwyAT"
      },
      "id": "-DyWNJwuwyAT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for text generation using the gpt2 model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "customer_text = \"The Divan is a very comfortable and professionally run hotel in Ankara. The staff are extremely helpful and friendly. Rooms and beds are very comfortable, with all the facilities that you would expect in a four star hotel. The breakfast buffet is very extensive (open 6.30AM to 10.30AM). The only down-side is the hotels location, a ten to fifteen minute taxi ride away from the city centre, embassies and government buildings, but is located within a very quiet residential area.\"\n",
        "\n",
        "response = \"Dear Our Valuable Guest, Thank you for taking the time to leave us a review.\"\n",
        "\n",
        "# Build the prompt for the text generation LLM\n",
        "prompt = f\"Customer review:\\n{customer_text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
        "\n",
        "# Pass the prompt to the model pipeline\n",
        "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id)\n",
        "\n",
        "# Print the augmented sequence generated by the model\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukF1d_xRwxRE",
        "outputId": "750b16ae-5dd2-43dd-b603-91cb7960b1ca"
      },
      "id": "ukF1d_xRwxRE",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer review:\n",
            "The Divan is a very comfortable and professionally run hotel in Ankara. The staff are extremely helpful and friendly. Rooms and beds are very comfortable, with all the facilities that you would expect in a four star hotel. The breakfast buffet is very extensive (open 6.30AM to 10.30AM). The only down-side is the hotels location, a ten to fifteen minute taxi ride away from the city centre, embassies and government buildings, but is located within a very quiet residential area.\n",
            "\n",
            "Hotel reponse to the customer:\n",
            "Dear Our Valuable Guest, Thank you for taking the time to leave us a review. Thank you for your continued commitment to the service of our customers, for being a well-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cce3f17-cf16-4b8e-be4c-769adba52eb2",
      "metadata": {
        "id": "1cce3f17-cf16-4b8e-be4c-769adba52eb2"
      },
      "source": [
        "# Chapter 2: Building a Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Blocks"
      ],
      "metadata": {
        "id": "UXVW1LslAu9y"
      },
      "id": "UXVW1LslAu9y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.1: PyTorch Transformer\n",
        "\n",
        "Transformer class from PyTorch Neural Networks enables building a full transformer architecture with encoder and decoder.\n",
        "\n",
        "The code example below can be used to build a very simple Transformer model. It is required to specifcy the main structural components:\n",
        "* Embedding size\n",
        "* Number of attention heads\n",
        "* Number of encoder layers\n",
        "* Number of decoder layers"
      ],
      "metadata": {
        "id": "SO2kCKMn4s31"
      },
      "id": "SO2kCKMn4s31"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set transformer model hyperparameters\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "\n",
        "# Create the transformer model and assign hyperparameters\n",
        "model = nn.Transformer(\n",
        "    d_model=d_model,\n",
        "    nhead=n_heads,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    num_decoder_layers=num_decoder_layers\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "fz8Wx5zA4wsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4fffc10-3513-4ae4-c244-25ea64377d49"
      },
      "id": "fz8Wx5zA4wsN",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.2: Building positional encoding\n",
        "\n",
        "Building the positional encoding can be observed from the implementation provided below.\n",
        "\n",
        "#### Instructions\n",
        "* Specify the PyTorch class that the positional encoder should subclass from.\n",
        "* Initialize a positional encoding matrix for token positions in sequences up to max_length.\n",
        "* Assign unique position encodings to the matrix pe by alternating the use of sine and cosine functions.\n",
        "* Update the input embeddings tensor x to add position information about the sequence using the positional encodings matrix."
      ],
      "metadata": {
        "id": "cCKhGMTXx-ss"
      },
      "id": "cCKhGMTXx-ss"
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass an appropriate PyTorch class\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_length):\n",
        "        super(PositionalEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize the positional encoding matrix\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        # Calculate and assign position encodings to the matrix\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    # Update the embeddings tensor adding the positional encodings\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "eGiz0SoEp0ng"
      },
      "id": "eGiz0SoEp0ng",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.3: Implementing multi-headed self-attention\n",
        "\n",
        "Building the multi-headed self-attention can be observed from the implementation provided below.\n",
        "\n",
        "#### Instructions\n",
        "* Split the sequence embeddings x across the multiple attention heads.\n",
        "* Compute dot-product based attention scores between the project query and key.\n",
        "* Normalize the attention scores to obtain attention weights.\n",
        "* Multiply the attention weights by the values and linearly transform the concatenated outputs per head."
      ],
      "metadata": {
        "id": "6FZGjkpZ2zHu"
      },
      "id": "6FZGjkpZ2zHu"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b1fb67b6-5884-40ae-9b54-78192b5506d4",
      "metadata": {
        "id": "b1fb67b6-5884-40ae-9b54-78192b5506d4"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Split the sequence embeddings in x across the attention heads\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n",
        "\n",
        "    def compute_attention(self, query, key, mask=None):\n",
        "        # Compute dot-product attention scores\n",
        "        scores = torch.matmul(query, key.permute(1, 2, 0))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        # Normalize attention scores into attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        return attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        query = self.split_heads(self.query_linear(query), batch_size)\n",
        "        key = self.split_heads(self.key_linear(key), batch_size)\n",
        "        value = self.split_heads(self.value_linear(value), batch_size)\n",
        "\n",
        "        attention_weights = self.compute_attention(query, key, mask)\n",
        "\n",
        "        # Multiply attention weights by values and linearly project concatenated outputs\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.output_linear(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.4: Post-attention feed-forward layer\n",
        "\n",
        "Feed-forward sublayer following multi-head self-attention for every encoder layer is built as an example below:\n",
        "\n",
        "#### Instructions\n",
        "* Specify in the __init__() method the sizes of the two linear fully connected layers.\n",
        "* Apply a forward pass through the two linear layers, using the ReLU() activation in between."
      ],
      "metadata": {
        "id": "v0ix1uQi3Gkt"
      },
      "id": "v0ix1uQi3Gkt"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f32a3f9f-f65c-450c-b49d-d849b0cdf617",
      "metadata": {
        "id": "f32a3f9f-f65c-450c-b49d-d849b0cdf617"
      },
      "outputs": [],
      "source": [
        "class FeedForwardSubLayer(nn.Module):\n",
        "    # Specify the two linear layers' input and output sizes\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForwardSubLayer, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    # Apply a forward pass\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder Transformer"
      ],
      "metadata": {
        "id": "xl3xBFBBA3LN"
      },
      "id": "xl3xBFBBA3LN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.5: Encoder layer\n",
        "\n",
        "Assembling a full encoder layer containing:\n",
        "\n",
        "* A multi-headed self-attention mechanism.\n",
        "* A feed-forward sublayer.\n",
        "* A combined layer normalization and dropout to be applied after each of the above two stages."
      ],
      "metadata": {
        "id": "uFbiS9Jg4Ic9"
      },
      "id": "uFbiS9Jg4Ic9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the initialization of elements in the encoder layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        # Multi-head self-attention\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # Feedforward neural network\n",
        "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "      # Multi-head self-attention\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        # Feedforward neural network\n",
        "        ff_output = self.feed_forward(x)\n",
        "        return self.norm2(x + self.dropout(ff_output))"
      ],
      "metadata": {
        "id": "XtH2xwFO4OW3"
      },
      "id": "XtH2xwFO4OW3",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.6: Encoder transformer body and head\n",
        "\n",
        "Implementing the transformer body, that is consisting of a stack of multiple encoder layers and a task specific transformer head that is used to process the encoder's hidden states.\n",
        "\n",
        "#### Instructions\n",
        "* Define a stack of multiple encoder layers in the __init__() method.\n",
        "* Complete the forward() method. Note that the process starts by converting the original sequence tokens in x into embeddings.\n",
        "* Add final linear layer to project encoder results into raw classification outputs.\n",
        "* Apply the necessary function to map raw classification outputs into log class probabilities."
      ],
      "metadata": {
        "id": "_8vDWOqJ4Yfb"
      },
      "id": "_8vDWOqJ4Yfb"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
        "        # Define a stack of multiple encoder layers\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    # Complete the forward pass method\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, d_model, num_classes):\n",
        "        super(ClassifierHead, self).__init__()\n",
        "        # Add linear layer for multiple-class classification\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x[:, 0, :])\n",
        "        # Obtain log class probabilities upon raw outputs\n",
        "        return F.log_softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "c7tjYzQG4PBN"
      },
      "id": "c7tjYzQG4PBN",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.7: Testing the encoder transformer\n",
        "\n",
        "A random and simple sequence will be used as an input to the encoder transformer. Obtaining the output (that is not even human-readable) without any errors is sufficient for this exercise.\n",
        "\n",
        "The following components are adequate to form a full encoder transformer:\n",
        "* PositionalEncoder\n",
        "* MultiHeadAttention\n",
        "* FeedForwardSublayer\n",
        "* EncoderLayer\n",
        "* TransformerEncoder\n",
        "* ClassifierHead\n",
        "\n",
        "Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length.\n",
        "\n",
        "#### Instructions\n",
        "* Instantiate the body and head of the encoder transformer.\n",
        "* Complete the forward pass throughout the entire transformer body and head to obtain and print classification outputs."
      ],
      "metadata": {
        "id": "mdusUqHn4vrs"
      },
      "id": "mdusUqHn4vrs"
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "vocab_size = 10000\n",
        "batch_size = 8\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "sequence_length = 64\n",
        "dropout = 0.1\n",
        "\n",
        "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
        "\n",
        "# Instantiate the encoder transformer's body and head\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
        "classifier = ClassifierHead(d_model, num_classes)\n",
        "\n",
        "# Complete the forward pass\n",
        "output = encoder(input_sequence, mask)\n",
        "classification = classifier(output)\n",
        "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
        "print(classification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGvWfAbYr5Mv",
        "outputId": "b66c3897-a80d-4f15-c59d-3720a694652f"
      },
      "id": "vGvWfAbYr5Mv",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification outputs for a batch of  8 sequences:\n",
            "tensor([[-0.8685, -0.9630, -1.6161],\n",
            "        [-0.4803, -1.2445, -2.3720],\n",
            "        [-1.1427, -1.0917, -1.0631],\n",
            "        [-0.7473, -1.1777, -1.5215],\n",
            "        [-0.8802, -1.1806, -1.2794],\n",
            "        [-0.5990, -1.1460, -2.0196],\n",
            "        [-1.0052, -1.2050, -1.0957],\n",
            "        [-0.8661, -0.8516, -1.8793]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder Transformer"
      ],
      "metadata": {
        "id": "g_yPohnTA6i4"
      },
      "id": "g_yPohnTA6i4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.8: Decoder Layer\n",
        "\n",
        "Encoder layer was built similarly, what is the main difference between these two structures?\n",
        "\n",
        "\n",
        "\n",
        "#### Instructions\n",
        "* A multi-headed self-attention mechanism.\n",
        "* A feed-forward sublayer.\n",
        "* Normalization and dropout to be applied."
      ],
      "metadata": {
        "id": "PgiCPIppxepa"
      },
      "id": "PgiCPIppxepa"
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Multi-head self-attention\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        # Feedforward neural network\n",
        "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, self_mask):\n",
        "        # Multi-head self-attention\n",
        "        attention_output = self.self_attention(x, x, x, self_mask)\n",
        "        x = x + self.dropout(attention_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        # Feedforward neural network\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SHslCMRKxe18"
      },
      "id": "SHslCMRKxe18",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.9: Building a decoder body and head\n",
        "\n",
        "A high-level structure for a decoder only transformer will be implemented in this exercise. Different than the encoder transformer, the model body and head is not seperated in decoder transformer. Instead decoder transformer contains the model head and body. The model body is a stack of decoder layers.\n",
        "\n",
        "#### Instructions\n",
        "* Add the linear layer for the model head inside the TransformerDecoder class.\n",
        "* Apply the last stage of the forward pass, through the model head."
      ],
      "metadata": {
        "id": "mXXBXK9t5yFa"
      },
      "id": "mXXBXK9t5yFa"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # Add a linear layer (head) for next-word prediction\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, self_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, self_mask)\n",
        "\n",
        "        # Apply the forward pass through the model head\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "DGiGIeNm4PG3"
      },
      "id": "DGiGIeNm4PG3",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.10: Testing the decoder transformer\n",
        "\n",
        "A random and simple sequence will be used as an input to the decoder transformer. Obtaining the output without any errors is sufficient for this exercise.\n",
        "\n",
        "The following components are adequate to form a full decoder transformer:\n",
        "* PositionalEncoder\n",
        "* MultiHeadAttention\n",
        "* FeedForwardSublayer\n",
        "* DecoderLayer\n",
        "* TransformerDecoder\n",
        "\n",
        "#### Instructions\n",
        "* Implement the decoder transformer with methods and classes defined before.\n",
        "* Complete the forward pass throughout the entire transformer body and head to obtain and print  outputs."
      ],
      "metadata": {
        "id": "ahPqhTJ06BIP"
      },
      "id": "ahPqhTJ06BIP"
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "vocab_size = 10000\n",
        "batch_size = 8\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "sequence_length = 64\n",
        "dropout = 0.1\n",
        "\n",
        "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "\n",
        "# Create a triangular attention mask for causal attention\n",
        "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n",
        "\n",
        "# Instantiate the decoder transformer\n",
        "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
        "\n",
        "output = decoder(input_sequence, self_attention_mask)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFMAatSbvg5e",
        "outputId": "3b4d6ce1-6d45-43aa-b10d-677a8ac96bd4"
      },
      "id": "HFMAatSbvg5e",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 64, 10000])\n",
            "tensor([[[ -9.3008,  -9.1400,  -9.4382,  ...,  -8.8393,  -9.6336,  -9.6300],\n",
            "         [ -9.7394,  -9.7596,  -9.5366,  ...,  -8.3186,  -9.2694,  -9.9648],\n",
            "         [ -9.5096,  -9.3017,  -9.0819,  ...,  -8.9034, -10.0289, -10.2051],\n",
            "         ...,\n",
            "         [ -8.9449,  -8.9809,  -8.6654,  ...,  -9.6186,  -8.3171,  -9.1322],\n",
            "         [ -9.3038,  -9.8143,  -9.2632,  ...,  -9.1180,  -9.4488, -10.2792],\n",
            "         [ -9.0742, -10.3100,  -9.3993,  ...,  -8.5381,  -8.8241,  -9.0917]],\n",
            "\n",
            "        [[-10.1083,  -8.5358,  -9.3812,  ...,  -9.6989,  -8.7557,  -9.4067],\n",
            "         [ -8.7439,  -9.4612,  -8.8393,  ...,  -9.6481,  -9.4200,  -9.5525],\n",
            "         [ -9.9176,  -9.2855,  -8.4389,  ...,  -9.7773,  -9.1805, -10.3609],\n",
            "         ...,\n",
            "         [ -9.2394,  -9.5725,  -8.5591,  ...,  -9.6167,  -9.7242, -10.3278],\n",
            "         [ -9.7585, -10.7210,  -9.3563,  ...,  -9.4965,  -8.4365,  -9.2414],\n",
            "         [ -9.5250,  -9.6970,  -9.4847,  ..., -10.0146,  -7.7003,  -9.1269]],\n",
            "\n",
            "        [[ -9.5469, -10.4773, -10.0193,  ...,  -8.8341,  -8.7656, -10.5739],\n",
            "         [ -9.9963, -10.2402,  -9.5817,  ...,  -8.2354,  -9.4978, -10.3891],\n",
            "         [ -9.0986, -10.1831, -10.6402,  ...,  -9.0015,  -9.6256, -10.8951],\n",
            "         ...,\n",
            "         [ -9.0914, -10.0758,  -9.4753,  ...,  -9.9635,  -9.2190,  -8.9659],\n",
            "         [-10.1920, -10.8027,  -9.3526,  ..., -10.0356,  -8.5957,  -9.3501],\n",
            "         [-10.0099,  -9.6501,  -9.2587,  ...,  -9.0963,  -7.9032,  -9.3201]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ -9.2971,  -9.4481, -10.1672,  ...,  -9.4362,  -7.9162, -10.0254],\n",
            "         [ -8.8964,  -8.8730,  -9.3868,  ...,  -9.0587,  -8.8212, -10.2453],\n",
            "         [ -9.6706,  -8.8670,  -9.7920,  ...,  -9.9624,  -9.4962, -10.6268],\n",
            "         ...,\n",
            "         [ -9.5081,  -9.2283, -10.2061,  ...,  -8.6651,  -9.0010, -10.0453],\n",
            "         [ -8.9252, -10.2389,  -9.6283,  ..., -10.2819,  -8.6127,  -9.2376],\n",
            "         [-10.3320, -10.3228,  -9.7149,  ...,  -9.1582,  -9.3425,  -9.6812]],\n",
            "\n",
            "        [[ -9.3780,  -9.4655,  -9.6502,  ...,  -9.4584,  -9.3785,  -9.5005],\n",
            "         [ -8.2058,  -9.9498,  -9.8161,  ...,  -9.1591,  -9.7986,  -8.9989],\n",
            "         [-10.1087,  -9.6254,  -9.6416,  ...,  -9.2127,  -8.7431,  -9.5399],\n",
            "         ...,\n",
            "         [ -9.4282,  -9.5182,  -9.1175,  ...,  -9.6627,  -9.0146, -10.1141],\n",
            "         [ -9.0802, -10.3381, -10.1075,  ..., -10.4229,  -9.5926, -10.1805],\n",
            "         [ -9.0995,  -9.0464,  -9.5435,  ...,  -9.8049,  -8.7027,  -9.8232]],\n",
            "\n",
            "        [[ -9.5062,  -8.8557, -10.2526,  ...,  -9.7602,  -9.3402,  -9.6885],\n",
            "         [ -9.1638, -10.2814, -10.1237,  ...,  -9.6661,  -9.8325, -10.4656],\n",
            "         [ -9.4190,  -8.7751, -10.7012,  ..., -10.1388,  -9.2857, -10.6123],\n",
            "         ...,\n",
            "         [ -9.2043, -10.2287,  -9.6772,  ...,  -9.2191,  -9.1140,  -9.4540],\n",
            "         [ -8.7031,  -9.9848,  -9.0278,  ...,  -9.6887,  -8.8516,  -9.3813],\n",
            "         [ -9.1541, -10.5435,  -8.9090,  ...,  -9.7214,  -8.2138,  -9.3156]]],\n",
            "       grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder-Decoder Transformer"
      ],
      "metadata": {
        "id": "Z-M0a32yA8yR"
      },
      "id": "Z-M0a32yA8yR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.11: Incorporating cross-attention in a decoder\n",
        "\n",
        "In an encoder-decoder transformer, decoder layers incorporate two attention mechanisms: the causal attention inherent to any transformer decoder, plus a cross-attention that integrates source sequence information processed by the encoder with the target sequence information being processed through the decoder.\n",
        "\n",
        "Modify the DecoderLayer class to incorporate this twofold attention scheme.\n",
        "\n",
        "#### Instructions\n",
        "* Initialize the two attention mechanisms used in an encoder-decoder transformers' decoder layer: causal (masked) self-attention and cross-attention.\n",
        "* Pass the necessary input arguments (query, key, values, and mask) to the two attention stages in the forward pass."
      ],
      "metadata": {
        "id": "ZBaL5H4lKLlY"
      },
      "id": "ZBaL5H4lKLlY"
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        # Initialize the causal (masked) self-attention and cross-attention\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
        "        # Pass the necessary arguments to the causal self-attention and cross-attention\n",
        "        self_attn_output = self.self_attn(x, x, x, causal_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn_output))\n",
        "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_mask)\n",
        "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "7drUCHLr6Kb5"
      },
      "id": "7drUCHLr6Kb5",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.12: Updating Decoder Transformer"
      ],
      "metadata": {
        "id": "l_m2lLe35zyd"
      },
      "id": "l_m2lLe35zyd"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, causal_mask, encoder_output, cross_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, causal_mask, encoder_output, cross_mask)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xJgCmGfv5t1h"
      },
      "id": "xJgCmGfv5t1h",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2.13: Trying out an encoder-decoder transformer\n",
        "Your next task is complete the following piece of code to define and forward-pass an example batch of randomly generated input sequences through an encoder-decoder transformer.\n",
        "\n",
        "Remember that we are only testing a yet-to-be-trained transformer architecture, hence the use of random input sequences.\n",
        "\n",
        "The following components are required to form a full encoder-decoder transformer:\n",
        "* MultiHeadAttention\n",
        "* FeedForwardSubLayer\n",
        "* PositionalEncoding\n",
        "* EncoderLayer\n",
        "* DecoderLayer\n",
        "* TransformerEncoder\n",
        "* TransformerDecoder\n",
        "* ClassifierHead\n",
        "\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "* Create a batch of random input sequences of size batch_size X sequence_length.\n",
        "* Instantiate the two transformer bodies using the appropriate class names.\n",
        "* Pass the necessary masks as arguments to the encoder and the decoder for their underlying attention mechanisms; each mask argument should be added in the same order they are utilized inside the encoder or decoder layer."
      ],
      "metadata": {
        "id": "XEr8387SKV65"
      },
      "id": "XEr8387SKV65"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "batch_size = 16\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "sequence_length = 128\n",
        "dropout = 0.1\n",
        "\n",
        "\n",
        "# Create a batch of random input sequences\n",
        "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "padding_mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
        "causal_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)\n",
        "\n",
        "# Instantiate the two transformer bodies\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
        "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
        "\n",
        "# Pass the necessary masks as arguments to the encoder and the decoder\n",
        "encoder_output = encoder(input_sequence, padding_mask)\n",
        "decoder_output = decoder(input_sequence, causal_mask, encoder_output, padding_mask)\n",
        "print(\"Batch's output shape: \", decoder_output.shape)"
      ],
      "metadata": {
        "id": "PF-GaVkHKAQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ecdcc2-d6c8-4267-ba40-2bb7ef852935"
      },
      "id": "PF-GaVkHKAQK",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch's output shape:  torch.Size([16, 128, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 3: Pre-trained Transformers"
      ],
      "metadata": {
        "id": "dVrx11sAIshd"
      },
      "id": "dVrx11sAIshd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.1: Classifying two movie opinions\n",
        "\n",
        "Previously we have built basic transformers and tested with sample sequences. For this exercise, we will suppy a pre-trained transformer (distilbert) with two opposing reviews.\n",
        "\n",
        "\n",
        "We have seen how to pass one example sequence to a pre-trained text classification LLM for inference. In this exercise you will practice passing two example sequences simultaneously, describing two rather opposite opinions of a movie.\n",
        "\n",
        "All the necessary imports have been made for you, including the auto classes specific to using pre-trained classification LLMs. The variable model_name has been also set with the name of the BERT-based model to use: \"textattack/distilbert-base-uncased-SST-2\".\n",
        "\n",
        "#### Instructions\n",
        "* Use the necessary task-specific classes and methods to load the tokenizer and pre-trained model.\n",
        "* Tokenize the inputs and pass them to the LLM to perform classification inference."
      ],
      "metadata": {
        "id": "jm2t6GepIx6Z"
      },
      "id": "jm2t6GepIx6Z"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"textattack/distilbert-base-uncased-SST-2\"\n",
        "\n",
        "# Load the tokenizer and pre-trained model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "  model_name, num_labels=2)\n",
        "\n",
        "text = [\"The best movie I've ever watched!\", \"What an awful movie. I regret watching it.\"]\n",
        "\n",
        "# Tokenize inputs and pass them to the model for inference\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "outputs = model(**inputs)\n",
        "logits = outputs.logits\n",
        "\n",
        "predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
        "for idx, predicted_class in enumerate(predicted_classes):\n",
        "    print(f\"Predicted class for \\\"{text[idx]}\\\": {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BL5TZbAIyCy",
        "outputId": "94d22458-5660-455c-e06f-6f5492b44bf1"
      },
      "id": "-BL5TZbAIyCy",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class for \"The best movie I've ever watched!\": 1\n",
            "Predicted class for \"What an awful movie. I regret watching it.\": 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarizing a product opinion\n",
        "In this text summarization exercise, we will examine different aspects of the \"opinosis\" dataset containing product reviews and summaries, as well as showing an example input sequence and its generated summarization.\n",
        "\n",
        "The necessary imports have been made for you, including the AutoTokenizer class and the specific auto class for handling sequence-to-sequence models: AutoModelForSeq2SeqLM.\n",
        "\n",
        "#### Instructions\n",
        "* Display the names of the features in the data, by accessing the downloaded 'train' fold.\n",
        "* Use the necessary variables and methods to encode the input example, pass it to the model to generate a summary, and decode the summary."
      ],
      "metadata": {
        "id": "BwrdvnxuIyIv"
      },
      "id": "BwrdvnxuIyIv"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"opinosis\")\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Number of instances: {len(dataset['train'])}\")\n",
        "\n",
        "# Show the names of features in the training fold of the dataset\n",
        "print(f\"Feature names: {dataset['train'].column_names}\")\n",
        "\n",
        "# Encode the input example, obtain the summary, and decode it\n",
        "example = dataset['train'][-2]['review_sents']\n",
        "input_ids = tokenizer.encode(\"summarize: \" + example, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "summary_ids = model.generate(input_ids, max_length=150)\n",
        "summary = tokenizer.decode(\n",
        "  summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\nOriginal Text (first 400 characters): \\n\", example[:400])\n",
        "print(\"\\nGenerated Summary: \\n\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeSEkFv9IyPA",
        "outputId": "38ecc165-227f-4e97-a076-518ce60e61c5"
      },
      "id": "FeSEkFv9IyPA",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of instances: 51\n",
            "Feature names: ['review_sents', 'summaries']\n",
            "\n",
            "Original Text (first 400 characters): \n",
            " I bought the 8, gig Ipod Nano that has the built, in video camera .\n",
            "  Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod .\n",
            "I have lots of music cd's and dvd's, so currently I'm just interested in storing some of my music and videos on the ipod so I can enjoy them on my vacation, and while at work .\n",
            "There's a right way and wrong wa\n",
            "\n",
            "Generated Summary: \n",
            " I bought the 8, gig Ipod Nano that has the built, in video camera. Itunes has an on, line store, where you may purchase and download music and videos which will install onto the ipod.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Spanish phrasebook mission\n",
        "You are a content writer at a reputable travel guide publisher. The next title to be published is a Spain travel guide for English speakers, but due to high demand and limited human resources, they assigned you the urgent task of drafting a \"Spanish phrasebook\" page, covering some essential survival Spanish words and phrases.\n",
        "\n",
        "Luckily, LLMs are here to help! In this exercise, you'll try using a pre-trained LLM for English-to-Spanish translation, and start this important mission by translating the first five common English phrases into Spanish.\n",
        "\n",
        "#### Instructions\n",
        "* Use the appropriate task-specific classes and methods to load the tokenizer and the model (the classes needed have been already imported for you, as usual!).\n",
        "* Complete the instructions to encode the input sequences, generate translations, and decode them. For encodings, use an extra argument to return them as PyTorch tensors."
      ],
      "metadata": {
        "id": "MHTnaomaIyU2"
      },
      "id": "MHTnaomaIyU2"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "\n",
        "# Load the tokenizer and the model checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "english_inputs = [\"Hello\", \"Thank you\", \"How are you?\", \"Sorry\", \"Goodbye\"]\n",
        "\n",
        "# Encode the inputs, generate translations, decode, and print them\n",
        "for english_input in english_inputs:\n",
        "    input_ids = tokenizer.encode(english_input, return_tensors=\"pt\")\n",
        "    translated_ids = model.generate(input_ids)\n",
        "    translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "    print(f\"English: {english_input} | Spanish: {translated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twHbK6TSN3WT",
        "outputId": "38733a41-b8ea-424f-df5c-dcbf03e8385f"
      },
      "id": "twHbK6TSN3WT",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English: Hello | Spanish: Hola.\n",
            "English: Thank you | Spanish: Gracias.\n",
            "English: How are you? | Spanish: ¿Cómo estás?\n",
            "English: Sorry | Spanish: Lo siento.\n",
            "English: Goodbye | Spanish: Adiós.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and inspect a QA dataset\n",
        "In this exercise, you will load a dataset for extractive QA, inspect some data, and tokenize a question-context example into a suitable format for feeding it to an LLM for QA.\n",
        "\n",
        "The necessary libraries, classes, and functions have been imported for you.\n",
        "\n",
        "#### Instructions\n",
        "* Load the dataset \"xtreme\" and subset \"MLQA.en.en\" using the variables already defined.\n",
        "* Initialize tokenizer using the \"deepset/minilm-uncased-squad2\" model checkpoint.\n",
        "Tokenize the example question and context retrieved, ensuring the results are returned as PyTorch tensors."
      ],
      "metadata": {
        "id": "QnWZ-1dKOOtx"
      },
      "id": "QnWZ-1dKOOtx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a specific subset of the dataset\n",
        "mlqa = load_dataset(\"xtreme\", name=\"MLQA.en.en\")\n",
        "\n",
        "question = mlqa[\"test\"][\"question\"][0]\n",
        "context = mlqa[\"test\"][\"context\"][0]\n",
        "print(\"Question: \", question)\n",
        "print(\"Context: \", context)\n",
        "\n",
        "# Initialize the tokenizer using the model checkpoint\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepset/minilm-uncased-squad2\")\n",
        "\n",
        "# Tokenize the inputs returning the result as tensors\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "print(\"First five encoded tokens: \", inputs[\"input_ids\"][0][:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVVjosPwN6JX",
        "outputId": "ec82d936-6861-441e-ab23-1d8ba60a9113"
      },
      "id": "IVVjosPwN6JX",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  Who analyzed the biopsies?\n",
            "Context:  In 1994, five unnamed civilian contractors and the widows of contractors Walter Kasza and Robert Frost sued the USAF and the United States Environmental Protection Agency. Their suit, in which they were represented by George Washington University law professor Jonathan Turley, alleged they had been present when large quantities of unknown chemicals had been burned in open pits and trenches at Groom. Biopsies taken from the complainants were analyzed by Rutgers University biochemists, who found high levels of dioxin, dibenzofuran, and trichloroethylene in their body fat. The complainants alleged they had sustained skin, liver, and respiratory injuries due to their work at Groom, and that this had contributed to the deaths of Frost and Kasza. The suit sought compensation for the injuries they had sustained, claiming the USAF had illegally handled toxic materials, and that the EPA had failed in its duty to enforce the Resource Conservation and Recovery Act (which governs handling of dangerous materials). They also sought detailed information about the chemicals to which they were allegedly exposed, hoping this would facilitate the medical treatment of survivors. Congressman Lee H. Hamilton, former chairman of the House Intelligence Committee, told 60 Minutes reporter Lesley Stahl, \"The Air Force is classifying all information about Area 51 in order to protect themselves from a lawsuit.\"\n",
            "First five encoded tokens:  tensor([  101,  2040, 16578,  1996, 16012])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating accuracy\n",
        "In this exercise you will use a sentiment classification pipeline to classify four short reviews with known labels, and then calculate the accuracy of predictions using the evaluate library.\n",
        "\n",
        "The necessary imports have been made for you. The test_examples variable contains the text reviews and their ground-truth labels:\n",
        "\n",
        "\n",
        "\n",
        "#### Instructions\n",
        "* Pass a list containing the four input reviews to the sentiment classification pipeline.\n",
        "* Load the accuracy score metric from the evaluate library"
      ],
      "metadata": {
        "id": "VsvBYrFUPMFo"
      },
      "id": "VsvBYrFUPMFo"
    },
    {
      "cell_type": "code",
      "source": [
        "test_examples = [\n",
        "    {\"text\": \"I am making a good use of this product!\", \"label\": 1},\n",
        "    {\"text\": \"The service was disappointing.\", \"label\": 0},\n",
        "    {\"text\": \"I learned a lot from this book.\", \"label\": 1},\n",
        "    {\"text\": \"The book cover broke after two days of use.\", \"label\": 0},\n",
        "]\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Pass the four input texts (without labels) to the pipeline\n",
        "predictions = sentiment_analysis([example[\"text\"] for example in test_examples])\n",
        "\n",
        "true_labels = [example[\"label\"] for example in test_examples]\n",
        "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
        "\n",
        "# Load the accuracy metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "result = accuracy.compute(references=true_labels, predictions=predicted_labels)\n",
        "print(result)\n",
        "\n",
        "\n",
        "# Load the accuracy, precision, recall and F1 score .metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# Obtain a description of each metric\n",
        "print(accuracy.description)\n",
        "print(precision.description)\n",
        "print(recall.description)\n",
        "print(f1.description)\n",
        "\n",
        "test_examples = [\n",
        "    \"Fantastic hotel, exceeded expectations!\",\n",
        "    \"Quiet despite central location, great stay.\",\n",
        "    \"Friendly staff, welcoming atmosphere.\",\n",
        "    \"Spacious, comfy room—a perfect retreat.\",\n",
        "    \"Cleanliness could improve, overall decent stay.\",\n",
        "      \"Disappointing stay, noisy and unclean room.\",\n",
        "    \"Terrible service, unfriendly staff, won't return.\"\n",
        "]\n",
        "test_labels = [1, 1, 1, 1, 0, 0, 0]\n",
        "\n",
        "# Pass the examples to the pipeline, and obtain a list of predicted labels\n",
        "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
        "predictions = sentiment_analysis([example for example in test_examples])\n",
        "predicted_labels = [1 if pred[\"label\"] == \"POSITIVE\" else 0 for pred in predictions]\n",
        "\n",
        "# Compute the metrics by comparing real and predicted labels\n",
        "print(precision.compute(references=test_labels, predictions=predicted_labels))\n",
        "print(recall.compute(references=test_labels, predictions=predicted_labels))\n",
        "print(f1.compute(references=test_labels, predictions=predicted_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_svnEfGOa5R",
        "outputId": "5af15e49-5b9a-4c58-96e4-ecdebdaf471f"
      },
      "id": "o_svnEfGOa5R",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'accuracy': 1.0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\n",
            "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
            " Where:\n",
            "TP: True positive\n",
            "TN: True negative\n",
            "FP: False positive\n",
            "FN: False negative\n",
            "\n",
            "\n",
            "Precision is the fraction of correctly labeled positive examples out of all of the examples that were labeled as positive. It is computed via the equation:\n",
            "Precision = TP / (TP + FP)\n",
            "where TP is the True positives (i.e. the examples correctly labeled as positive) and FP is the False positive examples (i.e. the examples incorrectly labeled as positive).\n",
            "\n",
            "\n",
            "Recall is the fraction of the positive examples that were correctly labeled by the model as positive. It can be computed with the equation:\n",
            "Recall = TP / (TP + FN)\n",
            "Where TP is the true positives and FN is the false negatives.\n",
            "\n",
            "\n",
            "The F1 score is the harmonic mean of the precision and recall. It can be computed with the equation:\n",
            "F1 = 2 * (precision * recall) / (precision + recall)\n",
            "\n",
            "{'precision': 0.8}\n",
            "{'recall': 1.0}\n",
            "{'f1': 0.888888888888889}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perplexed about 2030\n",
        "This exercise gives you the chance to generate some text and calculate its perplexity, based on the following prompt:\n",
        "\n",
        "#### Instructions\n",
        "Encode the text prompt, pass it to the GPT2 model for text generation, and decode the generated text.\n",
        "Load and compute the mean perplexity score on the generated text."
      ],
      "metadata": {
        "id": "Ib7PMsGJQ0pX"
      },
      "id": "Ib7PMsGJQ0pX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model name\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Initialize the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "prompt = \"Current trends show that by 2030 \"\n",
        "\n",
        "# Encode the prompt, generate text and decode it\n",
        "prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(prompt_ids, max_length=20)\n",
        "generated_text = tokenizer.decode(\n",
        "  output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Text: \", generated_text)\n",
        "\n",
        "# Load and compute the perplexity score\n",
        "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
        "results = perplexity.compute(model_id='gpt2',\n",
        "                             predictions=generated_text)\n",
        "print(\"Perplexity: \", results['mean_perplexity'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "4114ef16b0e442fda91720870b27ad36",
            "cd955340a79542e591bfb2a411c3da10",
            "639719cd55454d3390b9441d07db3b19",
            "a284511789d24973a2054a6743983e23",
            "aa0f5a74e3b54d11a3e1e97acb995311",
            "185cf84d817b48ebb35506a9a06aa73b",
            "620cf9573d97451aac16760b55c6315f",
            "571bf742887546b79ec40e348b1bb0f6",
            "14ee747c54ad473db4c2c7c1df374886",
            "f461304efc7547298fb8ec858aab4c18",
            "d0e312a4f04248938df8a7970cdaf179"
          ]
        },
        "id": "hkoUW2z0Qt5g",
        "outputId": "3b4eba16-138f-4921-a47e-aac7f27d4bcd"
      },
      "id": "hkoUW2z0Qt5g",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:  Current trends show that by 2030  the number of people living in poverty will be at its lowest\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4114ef16b0e442fda91720870b27ad36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity:  3514.5176167589552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4: Evaluation\n"
      ],
      "metadata": {
        "id": "c6JCGIy-RgAv"
      },
      "id": "c6JCGIy-RgAv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Metrics"
      ],
      "metadata": {
        "id": "n8Ks3rxaCADZ"
      },
      "id": "n8Ks3rxaCADZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4.1: Rouge, Meteor and Exact Match (EM)\n",
        "\n",
        "1. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**:\n",
        "   - ROUGE is a set of metrics used for evaluating automatic summarization and machine translation tasks.\n",
        "   - It measures the overlap between the model-generated summary (or translation) and the reference summaries (or translations).\n",
        "   - ROUGE includes various variants like ROUGE-N, ROUGE-L, and ROUGE-W. ROUGE-N measures n-gram overlap, ROUGE-L measures the longest common subsequence, and ROUGE-W measures weighted LCS-based statistics.\n",
        "   - ROUGE typically reports precision, recall, and F1-score for the overlap between the model output and the reference.\n",
        "\n",
        "2. **METEOR (Metric for Evaluation of Translation with Explicit ORdering)**:\n",
        "   - METEOR is another metric used in machine translation and automatic summarization tasks.\n",
        "   - It evaluates the quality of machine translation by considering not only exact word matches but also synonyms and paraphrases.\n",
        "   - METEOR computes a score based on precision, recall, and alignment between words in the reference and system output. It also considers the WordNet synonymy and stem overlap.\n",
        "   - METEOR has been shown to correlate well with human judgments of translation quality.\n",
        "\n",
        "3. **Exact Match (EM)**:\n",
        "   - EM is a metric commonly used in question answering tasks to evaluate the accuracy of the model's responses.\n",
        "   - It measures whether the model's output exactly matches the reference answer. If the generated answer matches the reference answer exactly, it gets a score of 1; otherwise, it gets a score of 0.\n",
        "   - EM is a binary metric, indicating whether the model's output is an exact match to the ground truth answer.\n",
        "\n",
        "Each of these metrics provides different perspectives on the quality and performance of NLP models. While ROUGE and METEOR are often used in text generation tasks like summarization and translation, EM is more commonly used in question answering and dialogue systems where exact answers are expected. Choosing the appropriate metric depends on the specific task and the desired evaluation criteria."
      ],
      "metadata": {
        "id": "P3Gku8mOCQkG"
      },
      "id": "P3Gku8mOCQkG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the rouge metric\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "predictions = [\"\"\"Pluto is a dwarf planet in our solar system, located in the Kuiper Belt beyond Neptune, and was formerly considered the ninth planet until its reclassification in 2006.\"\"\"]\n",
        "references = [\"\"\"Pluto is a dwarf planet in the solar system, located in the Kuiper Belt beyond Neptune, and was previously deemed as a planet until it was reclassified in 2006.\"\"\"]\n",
        "\n",
        "# Calculate the rouge scores between the predicted and reference summaries\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "print(\"ROUGE results: \", results)\n",
        "\n",
        "meteor = evaluate.load(\"meteor\")\n",
        "\n",
        "predictions = [\"He thought it right and necessary to become a knight-errant, roaming the world in armor, seeking adventures and practicing the deeds he had read about in chivalric tales.\"]\n",
        "references = [\"He believed it was proper and essential to transform into a knight-errant, traveling the world in armor, pursuing adventures, and enacting the heroic deeds he had encountered in tales of chivalry.\"]\n",
        "\n",
        "# Compute and print the METEOR score\n",
        "results = meteor.compute(predictions=predictions, references=references)\n",
        "print(\"Meteor: \", results['meteor'])\n",
        "\n",
        "\n",
        "exact_match = evaluate.load(\"exact_match\")\n",
        "\n",
        "predictions = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
        "references = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
        "\n",
        "# Compute the exact match and print the results\n",
        "results = exact_match.compute(predictions=predictions, references=references)\n",
        "print(\"EM results: \", results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQkECVw5PRc-",
        "outputId": "67fafeae-2b22-429c-9682-30fc0a85946e"
      },
      "id": "OQkECVw5PRc-",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE results:  {'rouge1': 0.7719298245614034, 'rouge2': 0.6181818181818182, 'rougeL': 0.736842105263158, 'rougeLsum': 0.736842105263158}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meteor:  0.5350702240481536\n",
            "EM results:  {'exact_match': 0.3333333333333333}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4.2: BLEU Score\n",
        "\n",
        "BLEU (BiLingual Evaluation Understudy) is a metric for automatically evaluating machine-translated text. The BLEU score is a number between zero and one that measures the similarity of the machine-translated text to a set of high quality reference translations.\n",
        "\n",
        "A pipeline based on the Helsinki-NLP Turkish-English translation model and the BLEU metric has been loaded, use evaluate.load(\"bleu\") from the evaluate library.\n",
        "\n",
        "#### Instructions\n",
        "Pass the input sentence in input_sentence_1 to the translator, then calculate the BLEU metric using reference_1."
      ],
      "metadata": {
        "id": "sSiVddBWETEE"
      },
      "id": "sSiVddBWETEE"
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "input_sentence_1 = \"Hola, ¿cómo estás?\"\n",
        "\n",
        "reference_1 = [\n",
        "     [\"Hello, how are you?\", \"Hi, how are you?\"]\n",
        "     ]\n",
        "\n",
        "input_sentences_2 = [\"Hola, ¿cómo estás?\", \"Estoy genial, gracias.\"]\n",
        "\n",
        "references_2 = [\n",
        "     [\"Hello, how are you?\", \"Hi, how are you?\"],\n",
        "     [\"I'm great, thanks.\", \"I'm great, thank you.\"]\n",
        "     ]\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "# Translate the first input sentence\n",
        "translated_output = translator(input_sentence_1)\n",
        "\n",
        "translated_sentence = translated_output[0]['translation_text']\n",
        "\n",
        "print(\"Translated:\", translated_sentence)\n",
        "\n",
        "# Calculate BLEU metric\n",
        "results = bleu.compute(predictions=[translated_sentence], references=reference_1)\n",
        "print(results)\n",
        "\n",
        "\n",
        "# Translate the input sentences, extract the translated text, and compute BLEU score\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-es-en\")\n",
        "\n",
        "translated_outputs = translator(input_sentences_2)\n",
        "\n",
        "predictions = [translated_output['translation_text'] for translated_output in translated_outputs]\n",
        "print(predictions)\n",
        "\n",
        "results = bleu.compute(predictions=predictions, references=references_2)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "2mwu43u2Rgty",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f206301-a822-4cd5-d3a9-c54aba5a4b4a"
      },
      "id": "2mwu43u2Rgty",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translated: Hey, how are you?\n",
            "{'bleu': 0.7598356856515925, 'precisions': [0.8333333333333334, 0.8, 0.75, 0.6666666666666666], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 6, 'reference_length': 6}\n",
            "['Hey, how are you?', \"I'm great, thanks.\"]\n",
            "{'bleu': 0.8627788640890415, 'precisions': [0.9090909090909091, 0.8888888888888888, 0.8571428571428571, 0.8], 'brevity_penalty': 1.0, 'length_ratio': 1.0, 'translation_length': 11, 'reference_length': 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reinforcement Learning"
      ],
      "metadata": {
        "id": "8n9HTqZxGQQ8"
      },
      "id": "8n9HTqZxGQQ8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up an RLHF loop\n",
        "The Proximal Policy Optimization (PPO) algorithm is popularly used in Reinforcement Learning from Human Feedback (RLHF) loops to fine-tune an LLM. The algorithm facilitates the iterative updating of model parameters based on a reward model derived from human feedback, ensuring the model's behavior is adapted predicated on human preferences.\n",
        "\n",
        "In this example, you will set up a simple RLHF loop based on PPO and a \"dummy\" reward model.\n",
        "\n",
        "#### Instructions\n",
        "* Instantiate a reference LLM to be used in the optimization process.\n",
        "* Initialize a trainer configuration object assigning it to ppo_config.\n",
        "* Create a PPOTrainer instance, assigning it the required arguments.\n",
        "* Train the LLM for one step using the PPO instance."
      ],
      "metadata": {
        "id": "tq7U1pVwFgqi"
      },
      "id": "tq7U1pVwFgqi"
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLMWithValueHead.from_pretrained('sshleifer/tiny-gpt2')\n",
        "\n",
        "# Instantiate a reference model\n",
        "model_ref = create_reference_model(model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('sshleifer/tiny-gpt2')\n",
        "\n",
        "if tokenizer._pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Initialize trainer configuration\n",
        "ppo_config = PPOConfig(mini_batch_size = 1, batch_size=1)\n",
        "\n",
        "prompt = \"Next year, I \"\n",
        "input = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "response  = respond_to_batch(model, input)\n",
        "\n",
        "# Create a PPOTrainer instance\n",
        "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
        "reward = [torch.tensor(1.0)]\n",
        "\n",
        "# Train LLM for one step with PPO\n",
        "train_stats = ppo_trainer.step([input[0]], [response[0]], reward)\n",
        "\n",
        "print(train_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBF7fjlXEs_6",
        "outputId": "d808a17a-2024-4971-b615-afa31676d536"
      },
      "id": "LBF7fjlXEs_6",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_trainer.py:257: UserWarning: No dataset is provided. Make sure to set config.batch_size to the correct value before training.\n",
            "  warnings.warn(\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'objective/kl': 0.0, 'objective/kl_dist': 0.0, 'objective/logprobs': array([[-10.803115, -10.834713, -10.795107, -10.808006, -10.812501,\n",
            "        -10.837952, -10.845894, -10.81187 , -10.855097, -10.814019,\n",
            "        -10.826392, -10.858207, -10.863373, -10.840664, -10.855502,\n",
            "        -10.841264, -10.843487, -10.8325  , -10.834808, -10.852801,\n",
            "        -10.824698, -10.796897, -10.909509, -10.800005]], dtype=float32), 'objective/ref_logprobs': array([[-10.803115, -10.834713, -10.795107, -10.808006, -10.812501,\n",
            "        -10.837952, -10.845894, -10.81187 , -10.855097, -10.814019,\n",
            "        -10.826392, -10.858207, -10.863373, -10.840664, -10.855502,\n",
            "        -10.841264, -10.843487, -10.8325  , -10.834808, -10.852801,\n",
            "        -10.824698, -10.796897, -10.909509, -10.800005]], dtype=float32), 'objective/kl_coef': 0.2, 'objective/entropy': 216.75743103027344, 'ppo/mean_non_score_reward': 0.0, 'ppo/mean_scores': 1.0, 'ppo/std_scores': nan, 'tokens/queries_len_mean': 5.0, 'tokens/queries_len_std': nan, 'tokens/queries_dist': 5.0, 'tokens/responses_len_mean': 20.0, 'tokens/responses_len_std': nan, 'tokens/responses_dist': 20.0, 'ppo/loss/policy': -7.050633576000109e-05, 'ppo/loss/value': 0.2960798740386963, 'ppo/loss/total': 0.02953748032450676, 'ppo/policy/entropy': 10.824555397033691, 'ppo/policy/approxkl': 1.2485349643043264e-08, 'ppo/policy/policykl': 1.3399123417912051e-05, 'ppo/policy/clipfrac': 0.0, 'ppo/policy/advantages': array([-2.3218036 , -2.268544  , -2.2124813 , -2.153468  , -1.5457101 ,\n",
            "       -1.0616994 , -1.4177942 , -0.8394928 , -0.70536184, -1.0378492 ,\n",
            "       -0.44726515, -0.29867703, -0.60997224,  0.00980062, -0.300618  ,\n",
            "        0.34540877,  0.09423296,  0.7125927 ,  0.45201775,  1.1308036 ,\n",
            "        0.99585235,  1.1073786 ,  1.341118  ,  2.0752363 , -2.3218036 ,\n",
            "       -2.268544  , -2.2124813 , -2.153468  , -1.5457101 , -1.0616994 ,\n",
            "       -1.4177942 , -0.8394928 , -0.70536184, -1.0378492 , -0.44726515,\n",
            "       -0.29867703, -0.60997224,  0.00980062, -0.300618  ,  0.34540877,\n",
            "        0.09423296,  0.7125927 ,  0.45201775,  1.1308036 ,  0.99585235,\n",
            "        1.1073786 ,  1.341118  ,  2.0752363 , -2.3218036 , -2.268544  ,\n",
            "       -2.2124813 , -2.153468  , -1.5457101 , -1.0616994 , -1.4177942 ,\n",
            "       -0.8394928 , -0.70536184, -1.0378492 , -0.44726515, -0.29867703,\n",
            "       -0.60997224,  0.00980062, -0.300618  ,  0.34540877,  0.09423296,\n",
            "        0.7125927 ,  0.45201775,  1.1308036 ,  0.99585235,  1.1073786 ,\n",
            "        1.341118  ,  2.0752363 , -2.3218036 , -2.268544  , -2.2124813 ,\n",
            "       -2.153468  , -1.5457101 , -1.0616994 , -1.4177942 , -0.8394928 ,\n",
            "       -0.70536184, -1.0378492 , -0.44726515, -0.29867703, -0.60997224,\n",
            "        0.00980062, -0.300618  ,  0.34540877,  0.09423296,  0.7125927 ,\n",
            "        0.45201775,  1.1308036 ,  0.99585235,  1.1073786 ,  1.341118  ,\n",
            "        2.0752363 ], dtype=float32), 'ppo/policy/advantages_mean': 8.940696716308594e-08, 'ppo/policy/ratio': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
            "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
            "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
            "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
            "       1.        , 1.        , 1.        , 1.        , 1.000001  ,\n",
            "       1.0000544 , 0.9999361 , 0.99994373, 0.9996796 , 0.99999714,\n",
            "       0.99994946, 1.0000019 , 0.99999714, 0.99993706, 0.99999905,\n",
            "       0.9999933 , 0.999959  , 1.0000572 , 0.9999466 , 1.0000553 ,\n",
            "       1.0000877 , 1.0000515 , 1.0000048 , 1.0000553 , 0.99999905,\n",
            "       1.000001  , 1.0000019 , 1.0000563 , 1.000001  , 1.0001097 ,\n",
            "       0.99986744, 0.99988747, 0.9993479 , 0.9999943 , 0.99989796,\n",
            "       1.0000029 , 0.9999943 , 0.9998741 , 0.99999905, 0.9999876 ,\n",
            "       0.9999199 , 1.0001135 , 0.9998913 , 1.0001097 , 1.0001764 ,\n",
            "       1.000104  , 1.0000114 , 1.0001097 , 0.9999962 , 1.        ,\n",
            "       1.0000048 , 1.0001125 , 1.0000029 , 1.000165  , 0.9997988 ,\n",
            "       0.9998312 , 0.99900484, 0.99999046, 0.9998474 , 1.0000048 ,\n",
            "       0.9999924 , 0.9998112 , 0.99999714, 0.999979  , 0.99987984,\n",
            "       1.0001688 , 0.9998379 , 1.000164  , 1.000268  , 1.0001554 ,\n",
            "       1.0000153 , 1.000164  , 0.9999943 , 1.        , 1.0000067 ,\n",
            "       1.0001698 ], dtype=float32), 'ppo/returns/mean': 0.5889323353767395, 'ppo/returns/var': 0.04953465983271599, 'ppo/val/vpred': -0.1496870517730713, 'ppo/val/error': 0.5921597480773926, 'ppo/val/clipfrac': 0.0, 'ppo/val/mean': -0.14959938824176788, 'ppo/val/var': 0.002457443857565522, 'ppo/val/var_explained': -10.954452514648438, 'ppo/learning_rate': 1.41e-05, 'time/ppo/forward_pass': 0.017885446548461914, 'time/ppo/compute_rewards': 0.00028061866760253906, 'time/ppo/compute_advantages': 0.0013582706451416016, 'time/ppo/optimize_step': 0.17944669723510742, 'time/ppo/calc_stats': 0.002587556838989258, 'time/ppo/total': 0.20175409317016602}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use-Case"
      ],
      "metadata": {
        "id": "RaKQ22CcGVIL"
      },
      "id": "RaKQ22CcGVIL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Toxic employee reviews?\n",
        "You have just joined a new company as a team lead. Two of your team members send thorough employee reviews on each other. To have a first, quick glimpse, you ask a pre-trained summarization LLM for help to get some concise points about each employee, as shown below:\n",
        "\n",
        "Your task is to carefully assess the toxicity level of these suggested responses.\n",
        "\n",
        "#### Instructions\n",
        "* Calculate the individual toxicity of each sequence, the maximum toxicity and toxicity ratio per employee."
      ],
      "metadata": {
        "id": "dkaculBPMG0w"
      },
      "id": "dkaculBPMG0w"
    },
    {
      "cell_type": "code",
      "source": [
        "emp_1 = [\"Everyone in the team adores him\",\n",
        "           \"He is a true genius, pure talent\"]\n",
        "emp_2 = [\"Nobody in the team likes him\",\n",
        "           \"He is a useless 'good-for-nothing'\"]\n",
        "\n",
        "toxicity_metric = evaluate.load(\"toxicity\")\n",
        "\n",
        "# Calculate the individual toxicities, maximum toxicities, and toxicity ratios\n",
        "toxicity_1 = toxicity_metric.compute(predictions=emp_1)\n",
        "toxicity_2 = toxicity_metric.compute(predictions=emp_2)\n",
        "print(\"Toxicities (emp. 1):\", toxicity_1['toxicity'])\n",
        "print(\"Toxicities (emp. 2): \", toxicity_2['toxicity'])\n",
        "\n",
        "toxicity_1_max = toxicity_metric.compute(predictions=emp_1, aggregation=\"maximum\")\n",
        "toxicity_2_max = toxicity_metric.compute(predictions=emp_2, aggregation=\"maximum\")\n",
        "print(\"Maximum toxicity (emp. 1):\", toxicity_1_max['max_toxicity'])\n",
        "print(\"Maximum toxicity (emp. 2): \", toxicity_2_max['max_toxicity'])\n",
        "\n",
        "toxicity_1_ratio = toxicity_metric.compute(predictions=emp_1, aggregation=\"ratio\")\n",
        "toxicity_2_ratio = toxicity_metric.compute(predictions=emp_2, aggregation=\"ratio\")\n",
        "print(\"Toxicity ratio (emp. 1):\", toxicity_1_ratio['toxicity_ratio'])\n",
        "print(\"Toxicity ratio (emp. 2): \", toxicity_2_ratio['toxicity_ratio'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RnVdbBUGP4z",
        "outputId": "006b7c3b-af15-404a-e347-15be76276568"
      },
      "id": "8RnVdbBUGP4z",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:evaluate_modules.metrics.evaluate-measurement--toxicity.2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b.toxicity:Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toxicities (emp. 1): [0.0001386617950629443, 0.00013368591316975653]\n",
            "Toxicities (emp. 2):  [0.00014245195779949427, 0.010071253404021263]\n",
            "Maximum toxicity (emp. 1): 0.0001386617950629443\n",
            "Maximum toxicity (emp. 2):  0.010071253404021263\n",
            "Toxicity ratio (emp. 1): 0.0\n",
            "Toxicity ratio (emp. 2):  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Best \"regard\"!\n",
        "In this wrap-up exercise, you will use the regard metric to assess polarity in two lists of sample texts associated to two groups of employees in two branches of a company. Each branch is located in a different country and geographical region of the planet.\n",
        "\n",
        "(It was reported a priori that due to naturally occurring cultural differences, both groups exhibit differences in the way they perform their work.)\n",
        "\n",
        "#### Instructions\n",
        "* Load the regard and regard-comparison metrics.\n",
        "* Compute the regard (polarity scores) for each group separately.\n",
        "* Compute the relative regard between the two groups for comparing polarities."
      ],
      "metadata": {
        "id": "6uzTEzfHM4Uf"
      },
      "id": "6uzTEzfHM4Uf"
    },
    {
      "cell_type": "code",
      "source": [
        "group1 = ['abc are described as loyal employees',\n",
        "          'abc are ambitious in their career expectations']\n",
        "group2 = ['abc are known for causing lots of team conflicts',\n",
        "          'abc are verbally violent']\n",
        "\n",
        "# Load the regard and regard-comparison metrics\n",
        "regard = evaluate.load(\"regard\")\n",
        "regard_comp = evaluate.load(\"regard\", \"compare\")\n",
        "\n",
        "# Compute the regard (polarities) of each group separately\n",
        "polarity_results_1 = regard.compute(data=group1)\n",
        "print(\"Polarity in group 1:\\n\", polarity_results_1)\n",
        "polarity_results_2 = regard.compute(data=group2)\n",
        "print(\"Polarity in group 2:\\n\", polarity_results_2)\n",
        "\n",
        "# Compute the relative regard between the two groups for comparison\n",
        "polarity_results_comp = regard_comp.compute(data=group1, references=group2)\n",
        "print(\"Polarity comparison between groups:\\n\", polarity_results_comp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-DHd0GJM4bf",
        "outputId": "3e0072f9-e655-42b4-aa72-33968c85567a"
      },
      "id": "Q-DHd0GJM4bf",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity in group 1:\n",
            " {'regard': [[{'label': 'positive', 'score': 0.9098386764526367}, {'label': 'neutral', 'score': 0.059396952390670776}, {'label': 'other', 'score': 0.026468101888895035}, {'label': 'negative', 'score': 0.004296252969652414}], [{'label': 'positive', 'score': 0.7809812426567078}, {'label': 'neutral', 'score': 0.18085983395576477}, {'label': 'other', 'score': 0.030492952093482018}, {'label': 'negative', 'score': 0.007666013203561306}]]}\n",
            "Polarity in group 2:\n",
            " {'regard': [[{'label': 'negative', 'score': 0.9658734202384949}, {'label': 'other', 'score': 0.021555885672569275}, {'label': 'neutral', 'score': 0.012026479467749596}, {'label': 'positive', 'score': 0.0005441228277049959}], [{'label': 'negative', 'score': 0.9774736166000366}, {'label': 'other', 'score': 0.012994581833481789}, {'label': 'neutral', 'score': 0.008945506066083908}, {'label': 'positive', 'score': 0.0005862844991497695}]]}\n",
            "Polarity comparison between groups:\n",
            " {'regard_difference': {'positive': 0.8448447558912449, 'neutral': 0.10964240040630102, 'other': 0.011205293238162994, 'negative': -0.9656923853326589}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dNf8B5rhNAXl"
      },
      "id": "dNf8B5rhNAXl",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformers"
      ],
      "metadata": {
        "id": "3ucTFhs0fnUt"
      },
      "id": "3ucTFhs0fnUt"
    },
    {
      "cell_type": "code",
      "source": [
        "a = a\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "def patchify(images, n_patches):\n",
        "    n, c, h, w = images.shape\n",
        "\n",
        "    assert h == w, \"Patchify method is implemented for square images only\"\n",
        "\n",
        "    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches**2)\n",
        "    patch_size = h // n_patches\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        for i in range(n_patches):\n",
        "            for j in range(n_patches):\n",
        "                patch = image[\n",
        "                    :,\n",
        "                    i * patch_size : (i + 1) * patch_size,\n",
        "                    j * patch_size : (j + 1) * patch_size,\n",
        "                ]\n",
        "                patches[idx, i * n_patches + j] = patch.flatten()\n",
        "    return patches\n",
        "\n",
        "\n",
        "class MyMSA(nn.Module):\n",
        "    def __init__(self, d, n_heads=2):\n",
        "        super(MyMSA, self).__init__()\n",
        "        self.d = d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
        "\n",
        "        d_head = int(d / n_heads)\n",
        "        self.q_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.k_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.v_mappings = nn.ModuleList(\n",
        "            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
        "        )\n",
        "        self.d_head = d_head\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        # Sequences has shape (N, seq_length, token_dim)\n",
        "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
        "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
        "        result = []\n",
        "        for sequence in sequences:\n",
        "            seq_result = []\n",
        "            for head in range(self.n_heads):\n",
        "                q_mapping = self.q_mappings[head]\n",
        "                k_mapping = self.k_mappings[head]\n",
        "                v_mapping = self.v_mappings[head]\n",
        "\n",
        "                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
        "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
        "\n",
        "                attention = self.softmax(q @ k.T / (self.d_head**0.5))\n",
        "                seq_result.append(attention @ v)\n",
        "            result.append(torch.hstack(seq_result))\n",
        "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
        "\n",
        "\n",
        "class MyViTBlock(nn.Module):\n",
        "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "        super(MyViTBlock, self).__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_d)\n",
        "        self.mhsa = MyMSA(hidden_d, n_heads)\n",
        "        self.norm2 = nn.LayerNorm(hidden_d)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.mhsa(self.norm1(x))\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class MyViT(nn.Module):\n",
        "    def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
        "        # Super constructor\n",
        "        super(MyViT, self).__init__()\n",
        "\n",
        "        # Attributes\n",
        "        self.chw = chw  # ( C , H , W )\n",
        "        self.n_patches = n_patches\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_d = hidden_d\n",
        "\n",
        "        # Input and patches sizes\n",
        "        assert (\n",
        "            chw[1] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by number of patches\"\n",
        "        assert (\n",
        "            chw[2] % n_patches == 0\n",
        "        ), \"Input shape not entirely divisible by number of patches\"\n",
        "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
        "\n",
        "        # 1) Linear mapper\n",
        "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
        "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "        # 2) Learnable classification token\n",
        "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
        "\n",
        "        # 3) Positional embedding\n",
        "        self.register_buffer(\n",
        "            \"positional_embeddings\",\n",
        "            get_positional_embeddings(n_patches**2 + 1, hidden_d),\n",
        "            persistent=False,\n",
        "        )\n",
        "\n",
        "        # 4) Transformer encoder blocks\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)]\n",
        "        )\n",
        "\n",
        "        # 5) Classification MLPk\n",
        "        self.mlp = nn.Sequential(nn.Linear(self.hidden_d, out_d), nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, images):\n",
        "        # Dividing images into patches\n",
        "        n, c, h, w = images.shape\n",
        "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
        "\n",
        "        # Running linear layer tokenization\n",
        "        # Map the vector corresponding to each patch to the hidden size dimension\n",
        "        tokens = self.linear_mapper(patches)\n",
        "\n",
        "        # Adding classification token to the tokens\n",
        "        tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
        "\n",
        "        # Adding positional embedding\n",
        "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
        "\n",
        "        # Transformer Blocks\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        # Getting the classification token only\n",
        "        out = out[:, 0]\n",
        "\n",
        "        return self.mlp(out)  # Map to output dimension, output category distribution\n",
        "\n",
        "\n",
        "def get_positional_embeddings(sequence_length, d):\n",
        "    result = torch.ones(sequence_length, d)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(d):\n",
        "            result[i][j] = (\n",
        "                np.sin(i / (10000 ** (j / d)))\n",
        "                if j % 2 == 0\n",
        "                else np.cos(i / (10000 ** ((j - 1) / d)))\n",
        "            )\n",
        "    return result\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Loading data\n",
        "    transform = ToTensor()\n",
        "\n",
        "    train_set = MNIST(\n",
        "        root=\"./../datasets\", train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_set = MNIST(\n",
        "        root=\"./../datasets\", train=False, download=True, transform=transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
        "    test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
        "\n",
        "    # Defining model and training options\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\n",
        "        \"Using device: \",\n",
        "        device,\n",
        "        f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\",\n",
        "    )\n",
        "    model = MyViT(\n",
        "        (1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10\n",
        "    ).to(device)\n",
        "    N_EPOCHS = 5\n",
        "    LR = 0.005\n",
        "\n",
        "    # Training loop\n",
        "    optimizer = Adam(model.parameters(), lr=LR)\n",
        "    criterion = CrossEntropyLoss()\n",
        "    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(\n",
        "            train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False\n",
        "        ):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "\n",
        "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
        "\n",
        "    # Test loop\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        test_loss = 0.0\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "            total += len(x)\n",
        "        print(f\"Test loss: {test_loss:.2f}\")\n",
        "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "JBhKQy2Ofonq",
        "outputId": "76cea31b-1cbe-4b28-c557-f19755b15375"
      },
      "id": "JBhKQy2Ofonq",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'a' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-37656fb7f001>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5x57qO-3lI3w"
      },
      "id": "5x57qO-3lI3w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eEEAK0LtlJPK"
      },
      "id": "eEEAK0LtlJPK"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RiUnTnsqlJdw"
      },
      "id": "RiUnTnsqlJdw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4114ef16b0e442fda91720870b27ad36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd955340a79542e591bfb2a411c3da10",
              "IPY_MODEL_639719cd55454d3390b9441d07db3b19",
              "IPY_MODEL_a284511789d24973a2054a6743983e23"
            ],
            "layout": "IPY_MODEL_aa0f5a74e3b54d11a3e1e97acb995311"
          }
        },
        "cd955340a79542e591bfb2a411c3da10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_185cf84d817b48ebb35506a9a06aa73b",
            "placeholder": "​",
            "style": "IPY_MODEL_620cf9573d97451aac16760b55c6315f",
            "value": "100%"
          }
        },
        "639719cd55454d3390b9441d07db3b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_571bf742887546b79ec40e348b1bb0f6",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14ee747c54ad473db4c2c7c1df374886",
            "value": 6
          }
        },
        "a284511789d24973a2054a6743983e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f461304efc7547298fb8ec858aab4c18",
            "placeholder": "​",
            "style": "IPY_MODEL_d0e312a4f04248938df8a7970cdaf179",
            "value": " 6/6 [00:01&lt;00:00,  3.82it/s]"
          }
        },
        "aa0f5a74e3b54d11a3e1e97acb995311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "185cf84d817b48ebb35506a9a06aa73b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "620cf9573d97451aac16760b55c6315f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "571bf742887546b79ec40e348b1bb0f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14ee747c54ad473db4c2c7c1df374886": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f461304efc7547298fb8ec858aab4c18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e312a4f04248938df8a7970cdaf179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}