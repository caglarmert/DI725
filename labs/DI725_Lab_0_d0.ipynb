{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3745686a-405f-4729-a239-ffd17f6c5087",
      "metadata": {
        "id": "3745686a-405f-4729-a239-ffd17f6c5087"
      },
      "source": [
        "# DI 725: Transformers and Attention-Based Deep Networks\n",
        "\n",
        "## An End-to-End Tutorial for Implementing Transformers\n",
        "\n",
        "### Authors: ttemizel@metu.edu.tr atemizel@metu.edu.tr mecaglar@metu.edu.tr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main resources: https://huggingface.co/"
      ],
      "metadata": {
        "id": "BtBS9rfBHIQC"
      },
      "id": "BtBS9rfBHIQC"
    },
    {
      "cell_type": "markdown",
      "id": "c0f81a53-355d-456d-90d1-9b5432dd1ce7",
      "metadata": {
        "id": "c0f81a53-355d-456d-90d1-9b5432dd1ce7"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5125f78-dea0-4c0e-ab42-5978b7b2633e",
      "metadata": {
        "id": "c5125f78-dea0-4c0e-ab42-5978b7b2633e"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/caglarmert/DI725/blob/main/src/attention_research_1.png?raw=true\" width=\"400\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "In this part we import the required libraries. Running this code on the Colab servers is recommended. It is advised to check the associated python requirements.txt, that is frozen at the time of preparation of this notebook, in case of any library or version error occurs while running this notebook. Mind that installing everything locally via pip install -r \"requirements.txt\" is not advised though, mainly because of the discrepancies between Colab and local machine."
      ],
      "metadata": {
        "id": "9bx_lEEFJI5N"
      },
      "id": "9bx_lEEFJI5N"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "70a9b2e0-fdb5-4697-a48f-44d1a08503f8",
      "metadata": {
        "id": "70a9b2e0-fdb5-4697-a48f-44d1a08503f8"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After importing the main libraries, we can continue with the transformers. First lets check what does the above import does. We have imported pipeline from transformers library, from huggingface ðŸ¤—.\n",
        "\n",
        "The [documentation](https://huggingface.co/docs/transformers) for the Transformers library.\n",
        "\n",
        "The [pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) is a class of the Transformers library. It is used for easy inference, abstracts most of the complexity and offers simple API for some dedicated tasks.\n",
        "\n",
        "Lets try some tasks that we can do with pre-trained models."
      ],
      "metadata": {
        "id": "QMDfs6xsJwj3"
      },
      "id": "QMDfs6xsJwj3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "1p9wMsy8xR_L"
      },
      "id": "1p9wMsy8xR_L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classifying a restaurant's customer review\n",
        "\n",
        "Let's practice loading an LLM from the Hugging Face hub into a pipeline to perform sentiment classification of customer restaurant reviews.\n",
        "\n",
        "Specifying the target language task when calling the pipeline() function is enough often to load a \"default\" model from Hugging Face. Nonetheless, it is usually a good practice to specify the name of the model we want to use. This is done by adding the model argument to the pipeline() function.\n",
        "\n",
        "The model_name variable, has been already instantiated for you with the name of a BERT-based LLM particularly suited for classifying reviews in a 1-to-5 star rating scale.\n",
        "\n",
        "#### Instructions\n",
        "* Import the necessary function from the transformers library to load Hugging Face LLMs as pipelines.\n",
        "* Load the model specified in model_name into a suitable pipeline for sentiment classification in text.\n",
        "* Pass the customer review defined in prompt to the pipeline to get a sentiment prediction."
      ],
      "metadata": {
        "id": "_pGAfQoPqyLh"
      },
      "id": "_pGAfQoPqyLh"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "380575a2-84dd-4d94-bf60-83fbd3114e47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "380575a2-84dd-4d94-bf60-83fbd3114e47",
        "outputId": "8535bd9e-1836-4ce7-f936-c2ba30667617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'label': 'positive', 'score': 0.8415129780769348}]\n"
          ]
        }
      ],
      "source": [
        "task_name = \"text-classification\"\n",
        "model_name = \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\"\n",
        "# We can change the model name to \"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\"\n",
        "\n",
        "classifier = pipeline(task = task_name, model = model_name)\n",
        "\n",
        "prompt = \"The food was good, but service at the restaurant was a bit slow\"\n",
        "\n",
        "prediction = classifier(prompt)\n",
        "print(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a pipeline for summarization\n",
        "In this exercise, you'll practice loading a Hugging Face LLM into a pipeline for text summarization. This is a remarkable but challenging language task that requires sequence-to-sequence LLMs -such as T5 models- to output a summarized sequence given an original text sequence.\n",
        "\n",
        "The pipeline import has been made for you. The text to be summarized has also been defined in the long_text variable. The beginning of the text looks like this:\n",
        "\n",
        "The tower is 324 meters (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side â€¦\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "* Load the model, based on the T5 transformer architecture and specified in model_name, into a text summarization pipeline.\n",
        "* Pass long_text to the model pipeline, to produce a summary limited to 50 tokens length.\n",
        "* Access and print the summarized text in outputs."
      ],
      "metadata": {
        "id": "4O9u0ZxKrJ4C"
      },
      "id": "4O9u0ZxKrJ4C"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"cnicu/t5-small-booksum\"\n",
        "\n",
        "long_text = \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n",
        "\n",
        "# Load the model pipeline for text summarization\n",
        "summarizer = pipeline(task=\"summarization\", model=model_name)\n",
        "\n",
        "# Pass the long text to the model to summarize it\n",
        "outputs = summarizer(long_text, max_length=50)\n",
        "\n",
        "# Access and print the summarized text in the outputs variable\n",
        "print(outputs[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5JQrMZlrXnp",
        "outputId": "4bc275c3-f104-4431-be0a-6cdfa023b6b3"
      },
      "id": "X5JQrMZlrXnp",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the Eiffel Tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Nlfcdn7YrnjR"
      },
      "id": "Nlfcdn7YrnjR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set transformer model hyperparameters\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "num_encoder_layers = 6\n",
        "num_decoder_layers = 6\n",
        "\n",
        "# Create the transformer model and assign hyperparameters\n",
        "model = nn.Transformer(\n",
        "    d_model=d_model,\n",
        "    nhead=n_heads,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    num_decoder_layers=num_decoder_layers\n",
        ")\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU6NkFHfJEJN",
        "outputId": "9238e955-8dd6-4027-accc-720052717663"
      },
      "id": "CU6NkFHfJEJN",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzM1VSVeqcix"
      },
      "id": "pzM1VSVeqcix",
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Helsinki-NLP/opus-mt-es-en\"\n",
        "\n",
        "input_text = \"Este curso sobre LLMs se estÃ¡ poniendo muy interesante\"\n",
        "\n",
        "# Define pipeline for Spanish-to-English translation\n",
        "translator = pipeline(\"translation_es_to_en\", model=model_name)\n",
        "\n",
        "# Translate the input text\n",
        "translations = translator(input_text)\n",
        "\n",
        "# Access the output to print the translated text in English\n",
        "print(translations[0]['translation_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT-DbjnhJEL9",
        "outputId": "7616fd5d-4b7a-404a-d8ce-8bd0473de47c"
      },
      "id": "HT-DbjnhJEL9",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This course on LLMs is getting very interesting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7wSNXFop1HX"
      },
      "id": "r7wSNXFop1HX",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model pipeline for question-answering\n",
        "context = \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\"\n",
        "\n",
        "qa_model = pipeline(\"question-answering\")\n",
        "question = \"For how long was the Eiffel Tower the tallest man-made structure in the world?\"\n",
        "\n",
        "# Pass the necessary inputs to the LLM pipeline for question-answering\n",
        "outputs = qa_model(question=question, context=context)\n",
        "\n",
        "# Access and print the answer\n",
        "print(outputs['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgOC4JFqp1Kf",
        "outputId": "bd3d9f60-3c0f-4a90-86f1-907c0fd4880f"
      },
      "id": "AgOC4JFqp1Kf",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41 years\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ai3V5hyBp1NJ"
      },
      "id": "Ai3V5hyBp1NJ",
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating replies to customer reviews\n",
        "In this exercise, you'll practice using an LLM pipeline for text generation.\n",
        "\n",
        "A text variable has been defined, containing a customer review for Riverview Hotel:\n",
        "\n",
        "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had\n",
        "\n",
        "The language task consists in generating a hotel reply to the customer review. The initial sentence for the reply is defined in the response variable so that the LLM gets it prompted along with the customer review to continue generating the reply.\n",
        "\n",
        "Note: the pad_token_id=generator.tokenizer.eos_token_id argument sets the tokenizer padding token ID as the EOS (End Of Speech) token ID\n",
        "#### Instructions\n",
        "Instantiate the generator variable as a pipeline that loads the \"gpt2\" pre-trained text generation model.\n",
        "Build a prompt for the LLM that concatenates the customer review with the hotel response's initial sentence.\n",
        "Pass the prompt to the previously defined pipeline to generate (inference) the following text in the hotel response, specifying a maximum length of 150 tokens for the generated output.\n",
        "Print the generated output."
      ],
      "metadata": {
        "id": "-DyWNJwuwyAT"
      },
      "id": "-DyWNJwuwyAT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline for text generation using the gpt2 model\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "text = \"I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\"\n",
        "\n",
        "response = \"Dear valued customer, I am glad to hear you had a good stay with us.\"\n",
        "\n",
        "# Build the prompt for the text generation LLM\n",
        "prompt = f\"Customer review:\\n{text}\\n\\nHotel reponse to the customer:\\n{response}\"\n",
        "\n",
        "# Pass the prompt to the model pipeline\n",
        "outputs = generator(prompt, max_length=150, pad_token_id=generator.tokenizer.eos_token_id)\n",
        "\n",
        "# Print the augmented sequence generated by the model\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukF1d_xRwxRE",
        "outputId": "dd330fcd-6040-43a7-dd22-fc409258c316"
      },
      "id": "ukF1d_xRwxRE",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer review:\n",
            "I had a wonderful stay at the Riverview Hotel! The staff were incredibly attentive and the amenities were top-notch. The only hiccup was a slight delay in room service, but that didn't overshadow the fantastic experience I had.\n",
            "\n",
            "Hotel reponse to the customer:\n",
            "Dear valued customer, I am glad to hear you had a good stay with us. We had a great time having your business. We went out to dinner and it felt great to have a wonderful relaxed dining experience and to discover a world of amazing food and service. I am very glad you are here to keep us going at Riverview. I would recommend your experience to others. We enjoy traveling, dining out and traveling with our\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cce3f17-cf16-4b8e-be4c-769adba52eb2",
      "metadata": {
        "id": "1cce3f17-cf16-4b8e-be4c-769adba52eb2"
      },
      "source": [
        "## Second Chapter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hands-on positional encoding\n",
        "In this exercise you'll complete the class implementation for a positional encoding mechanism.\n",
        "\n",
        "The necessary imports have been done for you, namely import torch.nn as nn.\n",
        "\n",
        "#### Instructions\n",
        "* Specify the PyTorch class that the positional encoder should subclass from.\n",
        "* Initialize a positional encoding matrix for token positions in sequences up to max_length.\n",
        "* Assign unique position encodings to the matrix pe by alternating the use of sine and cosine functions.\n",
        "* Update the input embeddings tensor x to add position information about the sequence using the positional encodings matrix."
      ],
      "metadata": {
        "id": "cCKhGMTXx-ss"
      },
      "id": "cCKhGMTXx-ss"
    },
    {
      "cell_type": "code",
      "source": [
        "# Subclass an appropriate PyTorch class\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_length):\n",
        "        super(PositionalEncoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize the positional encoding matrix\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        # Calculate and assign position encodings to the matrix\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    # Update the embeddings tensor adding the positional encodings\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "eGiz0SoEp0ng"
      },
      "id": "eGiz0SoEp0ng",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing multi-headed self-attention\n",
        "Now it's the turn of the multi-headed self-attention mechanism implementation.\n",
        "\n",
        "Besides the necessary imports, including this time torch.nn.functional as F, the __init__() method is also provided.\n",
        "#### Instructions\n",
        "* Split the sequence embeddings x across the multiple attention heads.\n",
        "* Compute dot-product based attention scores between the project query and key.\n",
        "* Normalize the attention scores to obtain attention weights.\n",
        "* Multiply the attention weights by the values and linearly transform the concatenated outputs per head."
      ],
      "metadata": {
        "id": "6FZGjkpZ2zHu"
      },
      "id": "6FZGjkpZ2zHu"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "b1fb67b6-5884-40ae-9b54-78192b5506d4",
      "metadata": {
        "id": "b1fb67b6-5884-40ae-9b54-78192b5506d4"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "    def split_heads(self, x, batch_size):\n",
        "        # Split the sequence embeddings in x across the attention heads\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.head_dim)\n",
        "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.num_heads, -1, self.head_dim)\n",
        "\n",
        "    def compute_attention(self, query, key, mask=None):\n",
        "        # Compute dot-product attention scores\n",
        "        scores = torch.matmul(query, key.permute(1, 2, 0))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        # Normalize attention scores into attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        return attention_weights\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        query = self.split_heads(self.query_linear(query), batch_size)\n",
        "        key = self.split_heads(self.key_linear(key), batch_size)\n",
        "        value = self.split_heads(self.value_linear(value), batch_size)\n",
        "\n",
        "        attention_weights = self.compute_attention(query, key, mask)\n",
        "\n",
        "        # Multiply attention weights by values and linearly project concatenated outputs\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "        output = output.view(batch_size, self.num_heads, -1, self.head_dim).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.output_linear(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-attention feed-forward layer\n",
        "Let's assemble some of the pieces of an encoder transformer, starting with the feed-forward sublayer that follows multi-headed self-attention in every encoder layer.\n",
        "\n",
        "#### Instructions\n",
        "* Specify in the __init__() method the sizes of the two linear fully connected layers.\n",
        "* Apply a forward pass through the two linear layers, using the ReLU() activation in between."
      ],
      "metadata": {
        "id": "v0ix1uQi3Gkt"
      },
      "id": "v0ix1uQi3Gkt"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "f32a3f9f-f65c-450c-b49d-d849b0cdf617",
      "metadata": {
        "id": "f32a3f9f-f65c-450c-b49d-d849b0cdf617"
      },
      "outputs": [],
      "source": [
        "class FeedForwardSubLayer(nn.Module):\n",
        "    # Specify the two linear layers' input and output sizes\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForwardSubLayer, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    # Apply a forward pass\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### encoder layer\n",
        "You've made it quite far in building your own skeleton transformer architecture! Now you are ready to assemble a full encoder layer containing:\n",
        "\n",
        "* A multi-headed self-attention mechanism.\n",
        "* A feed-forward sublayer.\n",
        "* A combined layer normalization and dropout to be applied after each of the above two stages.\n",
        "* Complete the implementation of the EncoderLayer class to initialize all its inner elements one by one."
      ],
      "metadata": {
        "id": "uFbiS9Jg4Ic9"
      },
      "id": "uFbiS9Jg4Ic9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the initialization of elements in the encoder layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForwardSubLayer(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        return self.norm2(x + self.dropout(ff_output))"
      ],
      "metadata": {
        "id": "XtH2xwFO4OW3"
      },
      "id": "XtH2xwFO4OW3",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder transformer body and head\n",
        "Almost there! Now that the encoder layer implementation has been completed, all that remains is:\n",
        "\n",
        "Implementing the transformer body, namely a stack of multiple encoder layers.\n",
        "Appending a task-specific transformer head to process the encoder's resulting hidden states and produce the final outputs for the language task at hand!\n",
        "#### Instructions\n",
        "* Define a stack of multiple encoder layers in the __init__() method.\n",
        "* Complete the forward() method. Note that the process starts by converting the original sequence tokens in x into embeddings.\n",
        "* Add final linear layer to project encoder results into raw classification outputs.\n",
        "* Apply the necessary function to map raw classification outputs into log class probabilities."
      ],
      "metadata": {
        "id": "_8vDWOqJ4Yfb"
      },
      "id": "_8vDWOqJ4Yfb"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoder(d_model, max_sequence_length)\n",
        "        # Define a stack of multiple encoder layers\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "    # Complete the forward pass method\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n",
        "\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, d_model, num_classes):\n",
        "        super(ClassifierHead, self).__init__()\n",
        "        # Add linear layer for multiple-class classification\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.fc(x[:, 0, :])\n",
        "        # Obtain log class probabilities upon raw outputs\n",
        "        return F.log_softmax(logits, dim=-1)"
      ],
      "metadata": {
        "id": "c7tjYzQG4PBN"
      },
      "id": "c7tjYzQG4PBN",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the encoder transformer\n",
        "In this exercise, you'll practice creating some instructions to pass an example random sequence throughout the encoder transformer you just defined to obtain and print the classification output. The following variables and model hyperparameters are defined for you:\n",
        "```\n",
        "num_classes = 3\n",
        "vocab_size = 10000\n",
        "batch_size = 8\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "sequence_length = 256\n",
        "dropout = 0.1\n",
        "```\n",
        "The PositionalEncoder, MultiHeadAttention, FeedForwardSublayer,EncoderLayer, TransformerEncoder, and ClassifierHead classes are also implemented.\n",
        "\n",
        "Note: although a random input sequence and mask are being used here, in practice, the mask should correspond to the actual location of padding tokens in the input sequences to ensure all of them are the same length.\n",
        "\n",
        "#### Instructions\n",
        "* Instantiate the body and head of the encoder transformer.\n",
        "* Complete the forward pass throughout the entire transformer body and head to obtain and print classification outputs."
      ],
      "metadata": {
        "id": "mdusUqHn4vrs"
      },
      "id": "mdusUqHn4vrs"
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "vocab_size = 10000\n",
        "batch_size = 8\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "sequence_length = 64\n",
        "dropout = 0.1\n",
        "\n",
        "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "mask = torch.randint(0, 2, (sequence_length, sequence_length))\n",
        "\n",
        "# Instantiate the encoder transformer's body and head\n",
        "encoder = TransformerEncoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
        "classifier = ClassifierHead(d_model, num_classes)\n",
        "\n",
        "# Complete the forward pass\n",
        "output = encoder(input_sequence, mask)\n",
        "classification = classifier(output)\n",
        "print(\"Classification outputs for a batch of \", batch_size, \"sequences:\")\n",
        "print(classification)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncVFx8um4PDv",
        "outputId": "37b59e99-c894-482d-f821-7ce99ab8ec0a"
      },
      "id": "ncVFx8um4PDv",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification outputs for a batch of  8 sequences:\n",
            "tensor([[-1.2418, -1.3817, -0.7765],\n",
            "        [-1.1628, -0.9391, -1.2161],\n",
            "        [-0.4997, -2.4064, -1.1936],\n",
            "        [-1.2461, -1.9649, -0.5583],\n",
            "        [-0.9535, -1.8489, -0.7826],\n",
            "        [-0.8685, -1.0750, -1.4307],\n",
            "        [-1.0314, -1.6768, -0.7841],\n",
            "        [-0.9121, -2.1297, -0.7351]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a decoder body and head\n",
        "Time to design a high-level architecture for a decoder-only transformer! On this occasion, instead of building the model body and the model head in two separate classes, the model head will be incorporated as part of the model body class that contains the stack of decoder layers.\n",
        "\n",
        "As usual, the necessary imports for this exercise have been done for you.\n",
        "\n",
        "#### Instructions\n",
        "* Add the linear layer for the model head inside the TransformerDecoder class.\n",
        "* Apply the last stage of the forward pass, through the model head."
      ],
      "metadata": {
        "id": "mXXBXK9t5yFa"
      },
      "id": "mXXBXK9t5yFa"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # Add a linear layer (head) for next-word prediction\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, self_mask):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, self_mask)\n",
        "\n",
        "        # Apply the forward pass through the model head\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "DGiGIeNm4PG3"
      },
      "id": "DGiGIeNm4PG3",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the decoder transformer\n",
        "In this exercise, you'll practice creating some instructions to pass an example random sequence throughout a decoder transformer architecture to obtain outputs in the form of next-token probabilities across the vocabulary.\n",
        "\n",
        "The following variables and model hyperparameters are defined for you:\n",
        "\n",
        "\n",
        "The PositionalEncoder, MultiHeadAttention, PositionWiseFeedForward,DecoderLayer, and TransformerDecoder classes are also implemented, the last of which integrates the model body and head.\n",
        "\n",
        "#### Instructions\n",
        "Create a triangular mask for enabling causal attention so that every token in the sequence only attends to the previous ones on its left-hand side.\n",
        "Instantiate the decoder transformer model."
      ],
      "metadata": {
        "id": "ahPqhTJ06BIP"
      },
      "id": "ahPqhTJ06BIP"
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "vocab_size = 10000\n",
        "batch_size = 8\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "sequence_length = 256\n",
        "dropout = 0.1\n",
        "\n",
        "input_sequence = torch.randint(0, vocab_size, (batch_size, sequence_length))\n",
        "\n",
        "# Create a triangular attention mask for causal attention\n",
        "self_attention_mask = (1 - torch.triu(torch.ones(1, sequence_length, sequence_length), diagonal=1)).bool()\n",
        "\n",
        "# Instantiate the decoder transformer\n",
        "decoder = TransformerDecoder(vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length=sequence_length)\n",
        "\n",
        "output = decoder(input_sequence, self_attention_mask)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "aSTY8Odk6CA2",
        "outputId": "a1ef0b51-80d9-4838-9db6-adb631faad7e"
      },
      "id": "aSTY8Odk6CA2",
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PositionalEncoding' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-97e9048fb3c5>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Instantiate the decoder transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-47c0164af402>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_sequence_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransformerDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionalEncoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDecoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PositionalEncoding' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7drUCHLr6Kb5"
      },
      "id": "7drUCHLr6Kb5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}